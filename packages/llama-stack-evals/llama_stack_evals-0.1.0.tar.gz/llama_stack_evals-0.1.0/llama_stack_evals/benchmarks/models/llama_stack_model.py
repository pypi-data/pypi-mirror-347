# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the terms described in the LICENSE file in
# the root directory of this source tree.

import json
import os
import time

from llama_stack_client import LlamaStackClient

from ..models.interface import Dialog, IModel


class LlamaStackModel(IModel):
    """
    Wrapper for the Llama Stack endpoint using Llama Stack Client
    """

    def __init__(
        self,
        model_id: str,
        system_message: str | None = None,
        base_url: str = "http://localhost:8321",
        api_key: str | None = None,
    ):
        self.model_id = model_id
        self.system_message = system_message
        print(f"Initializing LlamaStackModel with model_id: {model_id}, using base_url: {base_url}")
        self.client = LlamaStackClient(
            base_url=base_url,
            api_key=api_key,
            provider_data={
                "fireworks_api_key": os.environ.get("FIREWORKS_API_KEY", ""),
                "together_api_key": os.environ.get("TOGETHER_API_KEY", ""),
                "sambanova_api_key": os.environ.get("SAMBANOVA_API_KEY", ""),
                "openai_api_key": os.environ.get("OPENAI_API_KEY", ""),
            },
        )
        self.error_file = None
        if os.environ.get("ERROR_LOG_FILE"):
            self.error_file = open(os.environ["ERROR_LOG_FILE"], "a")

    def minibatch_size(self) -> int:
        return int(os.getenv("MINIBATCH_SIZE", 4))

    def generate(
        self,
        dialog_minibatch: list[Dialog],
        *,
        temperature: float,
        max_tokens: int,
        top_p: float | None,
    ) -> list[str]:
        """Generate a response from the model."""
        if self.system_message:
            dialog_minibatch = [
                [
                    {"role": "system", "content": self.system_message},
                ]
                + dialog
                for dialog in dialog_minibatch
            ]

        trial = 0
        while True:
            if temperature > 0.0:
                strategy = {
                    "type": "top_p",
                    "temperature": temperature,
                    "top_p": top_p,
                }
            else:
                strategy = {
                    "type": "greedy",
                }

            params = {
                "model_id": self.model_id,
                "messages_batch": dialog_minibatch,
                "sampling_params": {
                    "strategy": strategy,
                    "max_tokens": max_tokens,
                },
            }
            try:
                response_batch = self.client.inference.batch_chat_completion(**params)
                result_batch = []
                for response in response_batch.batch:
                    content = response.completion_message.content
                    result_batch.append(content if isinstance(content, str) else content.text)
                return result_batch

            except Exception as e:
                sleep_time = 4 + 1.5**trial
                print(f"Error generating response: {e}. Sleep for {sleep_time} seconds")
                time.sleep(sleep_time)
                trial += 1
                if trial > int(os.getenv("MAX_RETRIES", 6)):
                    if self.error_file:
                        d = json.dumps(
                            {
                                "error": str(e),
                                "params": params,
                            }
                        )
                        self.error_file.write(d + "\n")
                    # use an easy to grep error message
                    return [f"INFERENCE_ERROR: {e}"]

    def __str__(self) -> str:
        return f"{self.__class__.__name__}({self.model_id})"
