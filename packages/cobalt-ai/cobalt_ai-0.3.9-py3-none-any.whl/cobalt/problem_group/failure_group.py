# Copyright (C) 2023-2024 BlueLightAI, Inc. All Rights Reserved.
#
# Any use, distribution, or modification of this software is subject to the
# terms of the BlueLightAI Software License Agreement.

"""Algorithms for finding groups of data points with high model error."""

from math import ceil
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import mapper
import numpy as np
import pandas as pd
import scipy.stats
from mapper import cluster_tree, clustering_scores, flat_cluster
from mapper.graph import CSRGraph

from cobalt.graph_utils import (
    get_graph_edge_list,
    graph_superlevel_components,
    node_ids_to_data_point_ids,
    select_graph_degree,
)
from cobalt.problem_group import merge_tree
from cobalt.problem_group.autogroup import autogroup_modularity
from cobalt.schema import CobaltDataSubset, Embedding
from cobalt.schema.evaluation_metric import EvaluationMetric
from cobalt.schema.model_metadata import ModelMetadata

# Failure mode detection algorithms

# The goal of these algorithms is to find coherent subsets of data points for
# which some measure of model error is higher than expected. These may indicate
# systematic problems with the model.

# The algorithm currently implemented here is based on Mapper graphs, where each
# node represents a group of similar data points and there are edges between
# similar groups. The idea is to assign to each node in the graph an overall
# model error score for the data points in that node. We then look at the
# subgraph generated by the nodes with error scores higher than some threshold.
# This graph may have (probably has) more than one connected component, so we
# find each component. Since these components are connected, they should consist
# of similar data points, and they were selected to have high error rates.

# This is a promising algorithm, but does have some reliance on choosing the
# right resolution for the graph (how many data points per node?), as well as
# the right level of connectivity (how similar do two groups of data points need
# to be in order to be linked by an edge?). Choosing the right threshold may
# also be difficult, although we have a simple technique that seems to work all
# right.


def get_density_local_minima(
    f: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    """Finds minima in a kernel density estimate for f."""
    # The rule-of-thumb bandwidth choice tends to oversmooth, so may not detect
    # multimodal distributions. This is probably ok, since undersmoothing is
    # likely to produce lots of spurious local minima.

    # TODO: handle distributions with distant outliers
    density_eval_pts = np.linspace(f.min(), f.max(), 100)
    kde = scipy.stats.gaussian_kde(f)
    density_vals = kde.evaluate(density_eval_pts)

    d_density = np.diff(density_vals)
    change_points = []
    for i in range(len(d_density) - 1):
        if d_density[i] <= 0 and d_density[i + 1] > 0:
            change_points.append(i + 1)

    candidate_thresholds = density_eval_pts[change_points]
    return candidate_thresholds, density_vals


def choose_failure_value_threshold(
    failure_values: np.ndarray,
    minimum_quantile: float = 0.5,
    maximum_quantile: float = 0.99,
    default_quantile: float = 0.9,
    node_sizes: Optional[np.ndarray] = None,
) -> float:
    """Choose a cutoff threshold for failure values based on their distribution.

    The algorithm first estimates the distribution of the failure values and
    tries to find a local minimum in the density function. If it finds at least
    one, it uses the location of the rightmost minimum, as long as its quantile
    is above `minimum_quantile`, and below `maximum_quantile`. Otherwise, it
    uses the `default_quantile` quantile of the provided values.
    """
    if node_sizes is not None:
        # use the node sizes to spread function values across total number of
        # data points rather than across nodes

        # may be helpful if nodes have large differences in size
        # eg if all the high-error nodes are small and the low-error nodes are
        # large, this could avoid overestimating the error prevalence and make
        # sure the threshold is not set too high.

        failure_values = np.concatenate(
            [[val] * n_points for val, n_points in zip(failure_values, node_sizes)]
        )
    threshold_candidates, _ = get_density_local_minima(failure_values)
    max_quantile_val = np.quantile(failure_values, maximum_quantile)
    min_quantile_val = np.quantile(failure_values, minimum_quantile)

    # choose the rightmost minimum of the density that satisfies the constraints, if it exists
    valid_thresholds = [
        t
        for t in threshold_candidates
        if min_quantile_val < t <= max_quantile_val and t < np.max(failure_values)
    ]
    if valid_thresholds:
        threshold = valid_thresholds[-1]
    else:
        threshold = np.quantile(failure_values, default_quantile)

    if threshold == np.max(failure_values):
        threshold = np.nextafter(threshold, -np.inf)

    return threshold


def find_failure_groups_superlevel(
    data: CobaltDataSubset,
    node_values: np.ndarray,
    graph: Union[mapper.DisjointPartitionGraph, mapper.DataGraph],
    n_edges: int,
    threshold: float,
    max_edge_rank: Optional[int] = None,
) -> List[CobaltDataSubset]:
    """Find connected sets of nodes in a Mapper graph with high average levels of a failure metric.

    Args:
        data: A CobaltDataSubset containing the data to be analyzed
        node_values: a numpy array with the value of the failure metric for each node
        graph: A `mapper.DisjointPartitionGraph` built on `data`.
        n_edges: The number of edges from `graph` to use for computing connected components.
        threshold: The minimum value for the failure metric on a node to include it
            in a failure group.
        max_edge_rank: The maximum rank of an edge to include in the graph for
            computing connected components.
    """
    node_groups = graph_superlevel_components(
        graph, node_values, threshold, n_edges, max_edge_rank=max_edge_rank
    )

    group_idxs = [
        node_ids_to_data_point_ids(graph, node_group) for node_group in node_groups
    ]

    return [data.subset(idxs) for idxs in group_idxs]


def find_failure_groups_merge_tree(
    data: CobaltDataSubset,
    node_values: np.ndarray,
    graph: Union[mapper.DisjointPartitionGraph, mapper.DataGraph],
    n_edges: int,
    max_edge_rank: Optional[int] = None,
    threshold: float = -np.inf,
    stability_threshold: float = 0,  # TODO: default?
    filter_local_thresholds: bool = False,
) -> List[CobaltDataSubset]:

    node_sizes = np.array([len(u) for u in graph.nodes])
    # may want to consider other normalizations
    normalized_node_value = node_values - node_values.min()

    # multiplying by the normalized node value encourages clusters with high
    # values of the failure metric. this could be scaled in many different ways.
    node_weights = node_sizes * normalized_node_value
    mean_node_value = np.sum(node_sizes * node_values) / np.sum(node_sizes)
    min_node_value = max(mean_node_value, threshold)

    edge_list, n_nodes = get_graph_edge_list(graph, n_edges, max_rank=max_edge_rank)
    csr_graph = CSRGraph.from_edge_list(edge_list, n_nodes)
    nodes = merge_tree.graph_select_merge_tree_components(
        csr_graph,
        node_values=node_values,
        node_weights=node_weights,
        min_node_value=min_node_value,
        stability_threshold=stability_threshold,
    )

    # Within larger groups, choose a local threshold to weed out any outlier failure values
    # as this stands, it can result in disconnected groups
    if filter_local_thresholds:
        for i, u in enumerate(nodes):
            if len(u) > 10:
                local_threshold = choose_failure_value_threshold(
                    node_values[u],
                    minimum_quantile=0,
                    maximum_quantile=0.8,
                    default_quantile=0,
                )
                nodes[i] = [j for j in u if node_values[j] >= local_threshold]
    data_points = [node_ids_to_data_point_ids(graph, node_idxs) for node_idxs in nodes]
    subsets = [data.subset(indices) for indices in data_points]
    return subsets


# TODO: split superlevel sets into clusters based on connectivity/modularity

# TODO: try a more sophisticated superlevel set algorithm that incorporates weights


def build_graph_for_failure_groups(
    data: CobaltDataSubset,
    embedding: Embedding,
    min_mean_points_per_node: float = 5,
    max_nodes: int = 10000,
) -> Tuple[mapper.DataGraph, int, mapper.HierarchicalDataGraph, int]:
    """Choose a graph to use with the superlevel set failure mode detection algorithm.

    Uses the provided embedding, and chooses the finest resolution scale where
    the average number of data points per node is greater than
    `min_mean_points_per_node`.

    Returns the graph and the number of edges to use.
    """
    # TODO: replace with call to more specialized function
    multires_graph = mapper.quick_graph(
        embedding.get(data), metric=embedding.default_distance_metric
    )

    # choose the first resolution scale where nodes have enough data points on average
    # TODO: other criteria for choosing coarseness?
    N = len(data)
    graph_level_index = multires_graph.n_levels - 1
    for i, graph_level in enumerate(multires_graph.levels):
        n_nodes = len(graph_level.nodes)
        points_per_node = N / n_nodes
        if points_per_node >= min_mean_points_per_node and n_nodes < max_nodes:
            graph_level_index = i
            break

    # TODO: choose connectivity more carefully. some sort of scoring function?
    degree = select_graph_degree(graph_level)
    n_edges = ceil(len(graph_level.nodes) * degree / 2)

    return graph_level, n_edges, multires_graph, graph_level_index


# TODO: option to smooth function values?


def build_failure_value_array(
    data: CobaltDataSubset,
    graph: Union[mapper.DisjointPartitionGraph, mapper.DataGraph],
    failure_values: Union[pd.Series, np.ndarray, str],
) -> np.ndarray:
    """Calculate a failure value for each node in a graph.

    If the `failure_values` parameter is an array, takes the mean value of this
    array on each node. If it is a string, will first check if it is the name of
    a registered model performance metric; if so, it will calculate that metric
    on each node. Otherwise, it will assume the string is the name of a column
    in the dataset and average it over each node.
    """
    if isinstance(failure_values, str):
        if failure_values in data.model.performance_metric_keys:
            return np.array(
                [
                    data.subset(u).overall_model_performance_score(failure_values, 0)
                    for u in graph.nodes
                ]
            )
        else:
            failure_values = data.select_col(failure_values).to_numpy()
    return np.array([failure_values[u].mean() for u in graph.nodes])


def split_group(
    source_data: CobaltDataSubset,
    group: CobaltDataSubset,
    graph: Union[mapper.HierarchicalPartitionGraph, mapper.HierarchicalDataGraph],
    max_subgroup_size: int,
    graph_level: int,
) -> List[CobaltDataSubset]:

    subset_mask = group.as_mask_on(source_data)
    subset_graph_indices = np.flatnonzero(subset_mask)

    usable_levels = filter(
        lambda x: all(len(u) <= max_subgroup_size for u in x[1].nodes),
        reversed(list(enumerate(graph.levels))),
    )
    start_level = min(next(usable_levels)[0], graph_level)

    clt_data = cluster_tree._make_cluster_tree_local(
        graph, start_level=start_level, subset=subset_graph_indices
    )
    clt = cluster_tree.ClusterTree(clt_data, start_level=graph_level)

    modularity_score = clustering_scores.make_masked_negative_modularity_score_fn(
        graph.neighbor_graph, subset_mask, r=0
    )
    modularity_clusters = flat_cluster.optimize_cluster_score_top_down(
        clt,
        local_score_fns=[modularity_score],
        min_cluster_size=1,
        max_cluster_size=max_subgroup_size,
    )

    subgroups = [source_data.subset(indices) for indices in modularity_clusters]
    return subgroups


def split_group_new_graph(
    group: CobaltDataSubset, embedding: Embedding, max_subgroup_size: int
) -> List[CobaltDataSubset]:
    if Embedding is None:
        embedding = group.get_embedding(0)
    X = embedding.get(group)
    metric = embedding.get_available_distance_metrics()[0]
    g = mapper.quick_graph(X, metric=metric)
    subgroups, _ = autogroup_modularity(group, g, max_group_size=max_subgroup_size)
    return subgroups


def filter_groups_metric_threshold(
    groups: List[CobaltDataSubset], metric: EvaluationMetric, threshold: float
) -> List[CobaltDataSubset]:
    """Returns only the groups from groups where the metric exceeds the threshold."""
    filtered_groups = []
    for gp in groups:
        score = metric.overall_score(gp)[metric.get_key()]
        if not metric.lower_values_are_better:
            score = -score
        if score >= threshold:
            filtered_groups.append(gp)

    return filtered_groups


def split_groups_max_size(
    groups: List[CobaltDataSubset],
    max_size: int,
    split_group_fn: Callable[[CobaltDataSubset], List[CobaltDataSubset]],
):
    """Applies split_group_fn to any groups larger than max_size, and concatenates the results."""
    split_groups = []
    for gp in groups:
        if len(gp) > max_size:
            subgroups = split_group_fn(gp)
            split_groups.extend(subgroups)
        else:
            split_groups.append(gp)
    return split_groups


def filter_groups_min_errors(
    groups: List[CobaltDataSubset], model_metadata: ModelMetadata, min_errors: int
):
    if "error" in model_metadata.performance_metric_keys():
        metric = model_metadata.get_performance_metric_for("error")
        return [
            gp for gp in groups if metric.calculate(gp)["error"].sum() >= min_errors
        ]
    else:
        return groups


def find_failure_groups_superlevel_auto(
    data: CobaltDataSubset,
    failure_values: Union[pd.Series, np.ndarray, str],
    embedding: Optional[Embedding] = None,
    graph: Optional[Union[mapper.DisjointPartitionGraph, mapper.DataGraph]] = None,
    n_edges: Optional[int] = None,
    threshold: Optional[float] = None,
    use_node_sizes_for_threshold: bool = True,
    min_mean_points_per_node: float = 5,
    use_merge_tree: bool = True,
    max_edge_rank: Optional[int] = None,
) -> Tuple[List[CobaltDataSubset], Dict[str, Any]]:
    """Find connected sets of nodes in a Mapper graph with high average levels of a failure metric.

    Args:
        data: A CobaltDataSubset containing the data to be analyzed
        failure_values: a numpy array or Pandas Series of the same length as `data`
            containing the value of a failure metric for each data point. Or, the name
            of a failure metric or column in `data` to use.
        embedding: If no graph is provided, this embedding will be used to create a graph.
        graph: A `mapper.DisjointPartitionGraph` built on `data`. If none is
            provided, will create one from the provided embedding.
        n_edges: The number of edges from `graph` to use for computing connected
            components. If not provided, all edges will be used.
        threshold: The minimum value for the failure metric on a node to include it
            in a failure group. If not specified, will choose a threshold automatically.
        use_node_sizes_for_threshold: If the threshold is chosen automatically,
            whether to consider the distribution of node sizes in choosing the
            threshold.
        min_mean_points_per_node: The minimum average number of points per node,
            used to select a coarseness level for the generated graph if one is not
            provided.
        use_merge_tree: Whether to use the experimental merge tree-based algorithm for
            extracting groups instead of a fixed threshold.
        max_edge_rank: Maximum rank of edges to use in the graph for the merge tree algorithm.

    Returns:
        A list of the discovered failure groups and a dictionary containing
        values of parameters that were used in the process.
    """
    if graph is None:
        if embedding is None:
            embedding = data.embedding_metadata[0]
        graph, n_edges, full_graph, graph_level_index = build_graph_for_failure_groups(
            data, embedding, min_mean_points_per_node=min_mean_points_per_node
        )
    else:
        full_graph = None

    if n_edges is None:
        n_edges = graph.n_edges

    node_values = build_failure_value_array(data, graph, failure_values)

    all_equal = np.all(node_values == node_values[0])
    if all_equal:
        # no way to distinguish failures
        params = {
            "data": data,
            "graph": graph,
            "failure_values": failure_values,
            "node_values": node_values,
            "n_edges": n_edges,
            "threshold": threshold if threshold is not None else 0,
        }
        return [], params

    node_sizes = (
        np.array([len(u) for u in graph.nodes])
        if use_node_sizes_for_threshold
        else None
    )

    if use_merge_tree:
        if threshold is None:
            threshold = choose_failure_value_threshold(
                node_values,
                maximum_quantile=0.95,
                default_quantile=0.75,
                node_sizes=node_sizes,
            )
        failure_groups = find_failure_groups_merge_tree(
            data,
            node_values,
            graph,
            n_edges,
            max_edge_rank=max_edge_rank,
            threshold=threshold,
        )
    else:
        if threshold is None:
            threshold = choose_failure_value_threshold(
                node_values,
                node_sizes=node_sizes,
            )
        failure_groups = find_failure_groups_superlevel(
            data, node_values, graph, n_edges, threshold, max_edge_rank=max_edge_rank
        )

    # TODO: method to tweak parameters if certain criteria for results are not
    # met (e.g. need at least one group, need relatively balanced group sizes)

    params = {
        "data": data,
        "graph": graph,
        "failure_values": failure_values,
        "node_values": node_values,
        "n_edges": n_edges,
        "threshold": threshold,
    }

    if full_graph:
        params["full_graph"] = full_graph
        params["graph_level"] = graph_level_index

    return failure_groups, params
