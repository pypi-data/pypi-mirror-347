{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Welcome to the Dell AI SDK Walkthrough\n",
                "\n",
                "In this interactive guide, we'll explore the capabilities of the Dell AI SDK, helping you understand how to harness Dell's enterprise-grade AI infrastructure for your projects. By the end of this notebook, you'll have hands-on experience with:\n",
                "\n",
                "- Setting up and authenticating your Dell AI client\n",
                "- Discovering and exploring available AI models\n",
                "- Understanding Dell's AI hardware platforms\n",
                "- Generating deployment configurations for production environments\n",
                "\n",
                "Let's begin our journey into enterprise AI deployment!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setting Up Your Environment\n",
                "\n",
                "First, let's import the necessary libraries. The Dell AI SDK provides a simple client interface that handles all communication with Dell's AI infrastructure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dell_ai import DellAIClient"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Connecting to Dell AI\n",
                "\n",
                "Let's initialize our connection to Dell AI services. The client acts as your gateway to all Dell AI functionality.\n",
                "\n",
                "üí° **Authentication Tip:** The client will automatically try to use your Hugging Face token from the cache. If you haven't authenticated with Hugging Face before, you can pass your token directly as shown in the commented example below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the client (uses HF token from cache if available)\n",
                "client = DellAIClient()\n",
                "\n",
                "# If you need to provide a token directly:\n",
                "# client = DellAIClient(token=\"your_huggingface_token\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Verifying Your Connection\n",
                "\n",
                "Before proceeding, let's make sure we're properly connected and authenticated with the Dell AI platform. This step is crucial as it verifies your access to Dell's enterprise AI services.\n",
                "\n",
                "Let's check your authentication status and retrieve your user information:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's verify our connection and see your account details\n",
                "is_auth = client.is_authenticated()\n",
                "print(f\"‚úÖ Authentication status: {'Successful' if is_auth else 'Failed'}\")\n",
                "\n",
                "if is_auth:\n",
                "    user_info = client.get_user_info()\n",
                "    print(\"\\nüìã Your Hugging Face User Information:\")\n",
                "    for key, value in user_info.items():\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "    print(\"\\nYou're all set to explore Dell's AI capabilities!\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Please check your authentication token and try again.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploring Available AI Models\n",
                "\n",
                "Now that we're connected, let's discover the AI models available through Dell's platform. Dell AI provides access to a curated selection of high-performance models optimized for enterprise use cases.\n",
                "\n",
                "These models range from large language models (LLMs) to specialized AI models for various tasks, all optimized to run efficiently on Dell's hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìö Found 30 models on the Dell AI platform\n",
                        "\n",
                        "Sample of available models:\n",
                        "  ‚Ä¢ meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "  ‚Ä¢ meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
                        "  ‚Ä¢ google/gemma-3-27b-it\n",
                        "  ‚Ä¢ google/gemma-3-12b-it\n",
                        "  ‚Ä¢ google/gemma-3-4b-it\n",
                        "\n",
                        "üîç Spotlight on: meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "  Description: The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
                        "  License: llama4\n",
                        "  Is Multimodal: True\n",
                        "\n",
                        "Tip: To explore a different model, use: client.get_model('model_name')\n"
                    ]
                }
            ],
            "source": [
                "# Let's explore the available models\n",
                "models = client.list_models()\n",
                "print(f\"üìö Found {len(models)} models on the Dell AI platform\")\n",
                "\n",
                "# Display a few examples\n",
                "print(\"\\nSample of available models:\")\n",
                "for model in models[:5]:  # Show first 5 models\n",
                "    print(f\"  ‚Ä¢ {model}\")\n",
                "\n",
                "# Show more details about one model\n",
                "if models:\n",
                "    example_model = models[0]  # Let's look at the first model in detail\n",
                "    model_details = client.get_model(example_model)\n",
                "\n",
                "    print(f\"\\nüîç Spotlight on: {example_model}\")\n",
                "    print(f\"  Description: {model_details.description}\")\n",
                "    print(f\"  License: {model_details.license}\")\n",
                "    print(f\"  Is Multimodal: {model_details.is_multimodal}\")\n",
                "\n",
                "    print(\"\\nTip: To explore a different model, use: client.get_model('model_name')\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Understanding Dell's AI Hardware Platforms\n",
                "\n",
                "One of Dell's key strengths is its range of optimized hardware platforms for AI workloads. These platforms are designed to deliver maximum performance for different types of AI models and use cases.\n",
                "\n",
                "Let's explore the available hardware platforms:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üñ•Ô∏è Found 7 AI-optimized hardware platforms\n",
                        "\n",
                        "Available platforms:\n",
                        "  ‚Ä¢ xe9680-nvidia-h200\n",
                        "  ‚Ä¢ xe9680-nvidia-h100\n",
                        "  ‚Ä¢ xe9680-amd-mi300x\n",
                        "  ‚Ä¢ xe9680-intel-gaudi3\n",
                        "  ‚Ä¢ xe8640-nvidia-h100\n",
                        "  ‚Ä¢ r760xa-nvidia-h100\n",
                        "  ‚Ä¢ r760xa-nvidia-l40s\n",
                        "\n",
                        "üîç Platform Details: XE9680 Nvidia H200\n",
                        "  Server: xe9680\n",
                        "  GPU Information:\n",
                        "    - Vendor: Nvidia\n",
                        "    - Type: H200\n",
                        "    - Memory per GPU: 141G\n",
                        "    - Total GPUs: 8\n"
                    ]
                }
            ],
            "source": [
                "# Discover Dell AI hardware platforms\n",
                "platforms = client.list_platforms()\n",
                "print(f\"üñ•Ô∏è Found {len(platforms)} AI-optimized hardware platforms\")\n",
                "\n",
                "# List all available platforms\n",
                "print(\"\\nAvailable platforms:\")\n",
                "for platform in platforms:\n",
                "    print(f\"  ‚Ä¢ {platform}\")\n",
                "\n",
                "# Deep dive into one platform\n",
                "if platforms:\n",
                "    example_platform = platforms[0]\n",
                "    platform_details = client.get_platform(example_platform)\n",
                "\n",
                "    print(f\"\\nüîç Platform Details: {platform_details.name}\")\n",
                "    print(f\"  Server: {platform_details.server}\")\n",
                "    print(\"  GPU Information:\")\n",
                "    print(f\"    - Vendor: {platform_details.vendor}\")\n",
                "    print(f\"    - Type: {platform_details.gputype}\")\n",
                "    print(f\"    - Memory per GPU: {platform_details.gpuram}\")\n",
                "    print(f\"    - Total GPUs: {platform_details.totalgpucount}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model-Platform Compatibility\n",
                "\n",
                "Not all models can run efficiently on all hardware. Dell AI provides detailed compatibility information to help you choose the right hardware for your AI workloads.\n",
                "\n",
                "Let's check which platforms support our example model and what configurations are available:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Platform Support for: meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
                        "==================================================\n",
                        "This model can be deployed on 2 platform(s):\n",
                        "\n",
                        "üñ•Ô∏è xe9680-amd-mi300x\n",
                        "  Configuration Option 1:\n",
                        "    ‚Ä¢ Max Batch Prefill Tokens: 16484\n",
                        "    ‚Ä¢ Max Input Tokens: 16383\n",
                        "    ‚Ä¢ Max Total Tokens: 16384\n",
                        "    ‚Ä¢ Num Gpus: 8\n",
                        "\n",
                        "üñ•Ô∏è xe9680-nvidia-h200\n",
                        "  Configuration Option 1:\n",
                        "    ‚Ä¢ Max Batch Prefill Tokens: 8484\n",
                        "    ‚Ä¢ Max Input Tokens: 8383\n",
                        "    ‚Ä¢ Max Total Tokens: 8384\n",
                        "    ‚Ä¢ Num Gpus: 8\n",
                        "    ‚Ä¢ Max Concurrent Requests: 500\n"
                    ]
                }
            ],
            "source": [
                "# Check which platforms support our model\n",
                "\n",
                "if models:\n",
                "    model_id = models[0]  # Using our previous example model\n",
                "    model_details = client.get_model(model_id)\n",
                "\n",
                "    print(f\"üìä Platform Support for: {model_id}\")\n",
                "    print(\"=\" * 50)\n",
                "\n",
                "    if not model_details.configs_deploy:\n",
                "        print(\"‚ö†Ô∏è No deployment configurations available for this model.\")\n",
                "    else:\n",
                "        print(\n",
                "            f\"This model can be deployed on {len(model_details.configs_deploy)} platform(s):\"\n",
                "        )\n",
                "\n",
                "        for platform_id, configs in model_details.configs_deploy.items():\n",
                "            print(f\"\\nüñ•Ô∏è {platform_id}\")\n",
                "\n",
                "            for idx, config in enumerate(configs, 1):\n",
                "                print(f\"  Configuration Option {idx}:\")\n",
                "\n",
                "                # Get all attributes dynamically including any extra fields\n",
                "                config_dict = config.model_dump()\n",
                "                for key, value in config_dict.items():\n",
                "                    # Format the key to be more readable\n",
                "                    formatted_key = key.replace(\"_\", \" \").title()\n",
                "                    print(f\"    ‚Ä¢ {formatted_key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 List Compatible Platforms for Each Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for model in models:\n",
                "    if models:\n",
                "        model_id = model  # Using our previous example model\n",
                "        model_details = client.get_model(model_id)\n",
                "\n",
                "        print(\"=\" * 100)\n",
                "        print(f\"\\nüìä Platform Support for: {model_id}\")\n",
                "\n",
                "        if not model_details.configs_deploy:\n",
                "            print(\"‚ö†Ô∏è No deployment configurations available for this model.\")\n",
                "        else:\n",
                "            print(\n",
                "                f\"   This model can be deployed on {len(model_details.configs_deploy)} platform(s):\"\n",
                "            )\n",
                "\n",
                "            for platform_id, configs in model_details.configs_deploy.items():\n",
                "                print(f\"\\nüñ•Ô∏è {platform_id}\")\n",
                "\n",
                "                for idx, config in enumerate(configs, 0):\n",
                "                    print(f\"  Configuration Option {idx+1}:\")\n",
                "\n",
                "                    # Get all attributes dynamically including any extra fields\n",
                "                    config_dict = config.model_dump()\n",
                "                    for key, value in config_dict.items():\n",
                "                        # Format the key to be more readable\n",
                "                        formatted_key = key.replace(\"_\", \" \").title()\n",
                "                        print(f\"    ‚Ä¢ {formatted_key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 List Compatible Platforms for Specified Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check which platforms support our model\n",
                "user_input = input(\"Enter model id:üëâ \")\n",
                "\n",
                "if models:\n",
                "    # model_id = models[0]  # Using our previous example model\n",
                "    model_details = client.get_model(user_input)\n",
                "\n",
                "    print(f\"üìä Platform Support for: {user_input}\")\n",
                "    print(\"=\" * 50)\n",
                "\n",
                "    if not model_details.configs_deploy:\n",
                "        print(\"‚ö†Ô∏è No deployment configurations available for this model.\")\n",
                "    else:\n",
                "        print(\n",
                "            f\"This model can be deployed on {len(model_details.configs_deploy)} platform(s):\"\n",
                "        )\n",
                "\n",
                "        for platform_id, configs in model_details.configs_deploy.items():\n",
                "            print(f\"\\nüñ•Ô∏è {platform_id}\")\n",
                "\n",
                "            for idx, config in enumerate(configs, 1):\n",
                "                print(f\"  Configuration Option {idx}:\")\n",
                "\n",
                "                # Get all attributes dynamically including any extra fields\n",
                "                config_dict = config.model_dump()\n",
                "                for key, value in config_dict.items():\n",
                "                    # Format the key to be more readable\n",
                "                    formatted_key = key.replace(\"_\", \" \").title()\n",
                "                    print(f\"    ‚Ä¢ {formatted_key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Generating Deployment Configurations\n",
                "\n",
                "Now let's see how to deploy your chosen AI model! Dell AI simplifies deployment by generating ready-to-use configuration snippets for Docker and Kubernetes.\n",
                "\n",
                "Let's generate deployment snippets for our example model on a compatible platform:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Preparing Deployment for deepseek-ai/deepseek-r1-distill-llama-70b\n",
                        "On Platform: xe9680-amd-mi300x\n",
                        "\n",
                        "Deployment Configuration:\n",
                        "  ‚Ä¢ Max Batch Prefill Tokens: 49152\n",
                        "  ‚Ä¢ Max Input Tokens: 16384\n",
                        "  ‚Ä¢ Max Total Tokens: 18432\n",
                        "  ‚Ä¢ Num Gpus: 4\n",
                        "  ‚Ä¢ Max Concurrent Requests: 500\n",
                        "\n",
                        "üì¶ Docker Deployment Command:\n",
                        "Copy this command to deploy with Docker:\n",
                        "```bash\n",
                        "docker run \\\n",
                        "    -it \\\n",
                        "    -p 80:80 \\\n",
                        "    --security-opt seccomp=unconfined \\\n",
                        "    --device=/dev/kfd \\\n",
                        "    --device=/dev/dri \\\n",
                        "    --group-add video \\\n",
                        "    --ipc=host \\\n",
                        "    --shm-size 256g \\\n",
                        "    -e NUM_SHARD=4 \\\n",
                        "    -e MAX_BATCH_PREFILL_TOKENS=49152 \\\n",
                        "    -e MAX_TOTAL_TOKENS=18432 \\\n",
                        "    -e MAX_INPUT_TOKENS=16384 \\\n",
                        "    -e MAX_CONCURRENT_REQUESTS=500 \\\n",
                        "    registry.dell.huggingface.co/enterprise-dell-inference-deepseek-ai-deepseek-r1-distill-llama-70b-amd\n",
                        "```\n",
                        "\n",
                        "‚ò∏Ô∏è Kubernetes Deployment Manifest:\n",
                        "```yaml\n",
                        "# Write the Kubernetes manifest below in a deployment.yaml file,\n",
                        "# and then run the following kubectl command on the Kubernetes Cluster:\n",
                        "# kubectl apply -f deployment.yaml\n",
                        "\n",
                        "apiVersion: apps/v1\n",
                        "kind: Deployment\n",
                        "metadata:\n",
                        "  name: tgi-deployment\n",
                        "spec:\n",
                        "  replicas: 1\n",
                        "  selector:\n",
                        "    matchLabels:\n",
                        "      app: tgi-server\n",
                        "  template:\n",
                        "    metadata:\n",
                        "      labels:\n",
                        "        app: tgi-server\n",
                        "        hf.co/model: deepseek-ai--deepseek-r1-distill-llama-70b\n",
                        "        hf.co/task: text-generation\n",
                        "    spec:\n",
                        "      containers:\n",
                        "        - name: tgi-container\n",
                        "          image: registry.dell.huggingface.co/enterprise-dell-inference-deepseek-ai-deepseek-r1-distill-llama-70b-amd\n",
                        "          securityContext:\n",
                        "            seccompProfile:\n",
                        "              type: Unconfined\n",
                        "          resources:\n",
                        "            limits:\n",
                        "              amd.com/gpu: 4\n",
                        "          env: \n",
                        "            - name: NUM_SHARD\n",
                        "              value: \"4\"\n",
                        "            - name: MAX_BATCH_PREFILL_TOKENS\n",
                        "              value: \"49152\"\n",
                        "            - name: MAX_TOTAL_TOKENS\n",
                        "              value: \"18432\"\n",
                        "            - name: MAX_INPUT_TOKENS\n",
                        "              value: \"16384\"\n",
                        "            - name: MAX_CONCURRENT_REQUESTS\n",
                        "              value: \"500\"\n",
                        "          volumeMounts:\n",
                        "            - mountPath: /dev/shm\n",
                        "              name: dshm\n",
                        "            - name: dev-kfd\n",
                        "              mountPath: /dev/kfd\n",
                        "            - name: dev-dri\n",
                        "              mountPath: /dev/dri\n",
                        "      volumes:\n",
                        "        - name: dshm\n",
                        "          emptyDir:\n",
                        "            medium: Memory\n",
                        "            sizeLimit: 512Gi\n",
                        "        - name: dev-kfd\n",
                        "          hostPath:\n",
                        "            path: /dev/kfd\n",
                        "        - name: dev-dri\n",
                        "          hostPath:\n",
                        "            path: /dev/dri\n",
                        "---\n",
                        "apiVersion: v1\n",
                        "kind: Service\n",
                        "metadata:\n",
                        "  name: tgi-service\n",
                        "spec:\n",
                        "  type: LoadBalancer\n",
                        "  ports:\n",
                        "    - protocol: TCP\n",
                        "      port: 80\n",
                        "      targetPort: 80\n",
                        "  selector:\n",
                        "    app: tgi-server\n",
                        "---\n",
                        "apiVersion: networking.k8s.io/v1\n",
                        "kind: Ingress\n",
                        "metadata:\n",
                        "  name: tgi-ingress\n",
                        "  annotations:\n",
                        "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
                        "spec:\n",
                        "  ingressClassName: nginx-ingress\n",
                        "  rules:\n",
                        "    - http:\n",
                        "        paths:\n",
                        "          - path: /\n",
                        "            pathType: Prefix\n",
                        "            backend:\n",
                        "              service:\n",
                        "                name: tgi-service\n",
                        "                port:\n",
                        "                  number: 80\n",
                        "\n",
                        "```\n"
                    ]
                }
            ],
            "source": [
                "# Generate deployment snippets\n",
                "\n",
                "if models and model_details.configs_deploy:\n",
                "\n",
                "    model_id = model_details.repo_name\n",
                "\n",
                "    # Get the first platform ID from the available platforms\n",
                "    platform_ids = list(model_details.configs_deploy.keys())\n",
                "\n",
                "    if platform_ids:\n",
                "        platform_id = platform_ids[0]  # Take the first platform\n",
                "        config = model_details.configs_deploy[platform_id][\n",
                "            0\n",
                "        ]  # Take the first config of that platform\n",
                "\n",
                "        print(f\"üöÄ Preparing Deployment for {model_id}\")\n",
                "        print(f\"On Platform: {platform_id}\")\n",
                "        print(\"\\nDeployment Configuration:\")\n",
                "\n",
                "        # Display all config properties dynamically\n",
                "        config_dict = config.model_dump()\n",
                "        for key, value in config_dict.items():\n",
                "            # Format the key to be more readable\n",
                "            formatted_key = key.replace(\"_\", \" \").title()\n",
                "            print(f\"  ‚Ä¢ {formatted_key}: {value}\")\n",
                "\n",
                "    # Generate Docker deployment snippet\n",
                "    docker_snippet = client.get_deployment_snippet(\n",
                "        model_id=model_id,\n",
                "        platform_id=platform_id,\n",
                "        engine=\"docker\",\n",
                "        num_gpus=config.num_gpus,\n",
                "        num_replicas=1,\n",
                "    )\n",
                "\n",
                "    print(\"\\nüì¶ Docker Deployment Command:\")\n",
                "    print(\"Copy this command to deploy with Docker:\")\n",
                "    print(\"```bash\")\n",
                "    print(docker_snippet)\n",
                "    print(\"```\")\n",
                "\n",
                "    # Generate Kubernetes deployment snippet\n",
                "    k8s_snippet = client.get_deployment_snippet(\n",
                "        model_id=model_id,\n",
                "        platform_id=platform_id,\n",
                "        engine=\"kubernetes\",\n",
                "        num_gpus=config.num_gpus,\n",
                "        num_replicas=1,\n",
                "    )\n",
                "\n",
                "    print(\"\\n‚ò∏Ô∏è Kubernetes Deployment Manifest:\")\n",
                "    print(\"```yaml\")\n",
                "    print(k8s_snippet)\n",
                "    print(\"```\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No deployment configurations available. Try with a different model.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "Congratulations! You've successfully completed the Dell AI SDK walkthrough. You now have the knowledge to:\n",
                "\n",
                "1. **Connect** to Dell's enterprise AI platform\n",
                "2. **Explore** available AI models and hardware platforms\n",
                "3. **Check** compatibility between models and platforms\n",
                "4. **Generate** deployment configurations for your chosen model\n",
                "\n",
                "### Where to go from here:\n",
                "\n",
                "- Try deploying one of these models on your Dell hardware\n",
                "- Explore additional models and their capabilities\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
