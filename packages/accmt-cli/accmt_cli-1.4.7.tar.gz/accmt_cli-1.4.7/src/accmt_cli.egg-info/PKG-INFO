Metadata-Version: 2.4
Name: accmt-cli
Version: 1.4.7
Summary: CLI for AcceleratorModule library (accmt).
Home-page: https://github.com/ghanvert/accmt-cli
Author: ghanvert
Author-email: martin.pizarro@cenia.cl
Project-URL: Bug Tracker, https://github.com/ghanvert/accmt-cli/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: accelerate
Requires-Dist: PyYAML

# AcceleratorModule CLI
CLI for AcceleratorModule library ([accmt](https://github.com/ghanvert/AcceleratorModule)).

This is a command-line tool wrapper around Accelerate's command-line utilities ('accelerate').

## Installation
**accmt-cli** is automatically installed when installing **accmt** library. You also install it via pip:
```bash
pip install accmt-cli
```

## Launch
You can launch any distributed training process with the following command:
```bash
accmt launch [-N][-n, --gpus][-O1][--strat] <your_python_script> [...]
```

Where:
- **-N** (*optional*): Corresponds to the number of processes, or a Python-like slice to take GPUs from a certain index (e.g. '-N=2:', to take GPUs from index 2).
- **-n** or **--gpus** (*optional*): Corresponds to a list of CUDA devices (e.g. '-n=1,3,5,6', to take GPUs indices 1, 3, 5 and 6).
- **-O1** (*optional*): Corresponds to the optimization of type 1, which calculates the efficient number for 'OMP_NUM_THREADS', depending on how many processes you will run you training script.
- **--strat** (*optional*): Corresponds to the specific strategy to implement, or a configuration file path from Accelerate ('accelerate config --config_file=your-config.yaml'). See 'accmt strats' for specific strategies.
**...** (*optional*): You can add here any additional arguments that your Python script might have.

## Get model from checkpoint
You can get a model from any checkpoint using the following command:
```bash
accmt get <checkpoint> --out=<output-model-directory> [--dtype]
```

Where:
- **--out** or **-O** (*REQUIRED*): Output model directory name where to save the model.
- **--dtype** (*Optional*): PyTorch data type of model parameters. Default is 'float32'.

## Strats
You can check the specific strats included with the following command:
```bash
accmt strats [--ddp][--fsdp][--deepspeed]
```

Where:
- **--ddp**: To only filter for DDP strategies.
- **--fsdp**: To only filter for FSDP strategies.
- **--deepspeed**: To only filter for DeepSpeed strategies.

## Example
Generate an example HPS file config with the following command:
```bash
accmt example
```

This will generate a file on your current directory called 'hps_example.yaml'.

## Debug
Enable debug mode with:
```bash
accmt debug [--level] ...
```

Where `--level` flag is an integer number, which indicates the level of debugging. Available levels are:
**LEVEL 1**:
- Disables logging (MLFlow, Tensorboard, etc).

**LEVEL 2**:
- Disables model and teacher compilation.

**LEVEL 3**:
- Disables model saving, checkpointing and resuming (no folders will be created).

**LEVEL 4** (default):
- Force `eval_when_start` (in Trainer) to False.

**LEVEL 5**:
- Disables any evaluation.
