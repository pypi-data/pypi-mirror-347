#!/usr/bin/env python3
"""
Test Runner for Desktop Automation Recorder

This script runs multiple test scripts generated by the Desktop Automation Recorder
and produces a combined report of the results.

Usage:
    python test_runner.py [test_scripts...]
    
If no test scripts are provided, all Python files in the current directory will be considered.
"""

import os
import sys
import json
import glob
import time
import argparse
import datetime
import importlib.util
import traceback

def load_test_module(script_path):
    """Load a test script as a Python module"""
    try:
        # Get absolute path
        script_path = os.path.abspath(script_path)
        
        # Generate a module name based on the filename
        module_name = os.path.basename(script_path).replace('.py', '')
        
        # Load the module
        spec = importlib.util.spec_from_file_location(module_name, script_path)
        if not spec:
            raise ImportError(f"Failed to load spec for {script_path}")
        
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        
        # Verify it has the required run_test function
        if not hasattr(module, 'run_test'):
            raise AttributeError(f"Module {module_name} does not have a run_test function")
        
        return module
    except Exception as e:
        print(f"Error loading test module {script_path}: {str(e)}")
        traceback.print_exc()
        return None

def run_tests(test_scripts=None, headless=False):
    """Run multiple test scripts and collect results"""
    start_time = datetime.datetime.now()
    
    # If no scripts provided, find all Python files in the current directory
    if not test_scripts:
        test_scripts = glob.glob("*.py")
        # Exclude this runner script
        if os.path.basename(__file__) in test_scripts:
            test_scripts.remove(os.path.basename(__file__))
    
    # Initialize combined results
    combined_results = {
        "start_time": start_time.isoformat(),
        "end_time": "",
        "duration_seconds": 0,
        "tests": [],
        "total_tests": len(test_scripts),
        "passed_tests": 0,
        "failed_tests": 0,
        "error_tests": 0,
        "total_checks": 0,
        "passed_checks": 0,
        "failed_checks": 0
    }
    
    # Run each test
    for script_path in test_scripts:
        print(f"\n{'='*80}")
        print(f"Running test: {script_path}")
        print(f"{'='*80}")
        
        try:
            # Load the test module
            test_module = load_test_module(script_path)
            if not test_module:
                # Failed to load module
                test_result = {
                    "test_name": os.path.basename(script_path),
                    "status": "ERROR",
                    "error": "Failed to load test module",
                    "start_time": datetime.datetime.now().isoformat(),
                    "end_time": datetime.datetime.now().isoformat(),
                    "duration_seconds": 0,
                    "visual_checks": [],
                    "errors": [{
                        "type": "LoadError",
                        "message": "Failed to load test module",
                        "timestamp": datetime.datetime.now().isoformat()
                    }],
                    "checks_passed": 0,
                    "checks_failed": 0
                }
                combined_results["error_tests"] += 1
            else:
                # Set headless mode if required
                if headless and hasattr(test_module, 'HEADLESS_MODE'):
                    test_module.HEADLESS_MODE = True
                
                # Run the test
                test_result = test_module.run_test()
                
                # Update counters based on test result
                if test_result["status"] == "PASSED":
                    combined_results["passed_tests"] += 1
                elif test_result["status"] == "FAILED":
                    combined_results["failed_tests"] += 1
                elif test_result["status"] == "ERROR":
                    combined_results["error_tests"] += 1
                
                # Add check counts
                combined_results["total_checks"] += test_result["checks_passed"] + test_result["checks_failed"]
                combined_results["passed_checks"] += test_result["checks_passed"]
                combined_results["failed_checks"] += test_result["checks_failed"]
            
            # Add test result to combined results
            combined_results["tests"].append(test_result)
            
        except Exception as e:
            print(f"Unhandled exception running test {script_path}: {str(e)}")
            traceback.print_exc()
            
            # Create an error result
            error_result = {
                "test_name": os.path.basename(script_path),
                "status": "ERROR",
                "error": str(e),
                "start_time": datetime.datetime.now().isoformat(),
                "end_time": datetime.datetime.now().isoformat(),
                "duration_seconds": 0,
                "visual_checks": [],
                "errors": [{
                    "type": "RunnerException",
                    "message": str(e),
                    "timestamp": datetime.datetime.now().isoformat(),
                    "traceback": traceback.format_exc()
                }],
                "checks_passed": 0,
                "checks_failed": 0
            }
            combined_results["tests"].append(error_result)
            combined_results["error_tests"] += 1
    
    # Update final timing
    end_time = datetime.datetime.now()
    combined_results["end_time"] = end_time.isoformat()
    combined_results["duration_seconds"] = (end_time - start_time).total_seconds()
    
    return combined_results

def save_combined_report(results):
    """Save combined test results to a JSON file"""
    # Create reports directory if it doesn't exist
    reports_dir = os.path.join(os.path.dirname(__file__), "reports")
    os.makedirs(reports_dir, exist_ok=True)
    
    # Generate report filename
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(reports_dir, f"combined_report_{timestamp}.json")
    
    # Write report to file
    with open(report_path, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nCombined report saved to: {report_path}")
    return report_path

def print_summary(results):
    """Print a summary of test results"""
    print(f"\n{'='*80}")
    print(f"TEST RUN SUMMARY")
    print(f"{'='*80}")
    print(f"Total tests:    {results['total_tests']}")
    print(f"Passed tests:   {results['passed_tests']}")
    print(f"Failed tests:   {results['failed_tests']}")
    print(f"Error tests:    {results['error_tests']}")
    print(f"Total checks:   {results['total_checks']}")
    print(f"Passed checks:  {results['passed_checks']}")
    print(f"Failed checks:  {results['failed_checks']}")
    print(f"Duration:       {results['duration_seconds']:.2f} seconds")
    print(f"{'='*80}")
    
    # Print individual test results
    print("\nTest Results:")
    for i, test in enumerate(results["tests"]):
        status_color = "\033[92m" if test["status"] == "PASSED" else "\033[91m"  # Green for pass, red for fail
        reset_color = "\033[0m"
        print(f"{i+1}. {test['test_name']}: {status_color}{test['status']}{reset_color} - Checks: {test['checks_passed']}/{test['checks_passed'] + test['checks_failed']}")
    
    # Print final status
    overall_status = "PASSED" if results["failed_tests"] == 0 and results["error_tests"] == 0 else "FAILED"
    status_color = "\033[92m" if overall_status == "PASSED" else "\033[91m"
    print(f"\nOverall status: {status_color}{overall_status}{reset_color}")

def main():
    """Main function for test runner"""
    parser = argparse.ArgumentParser(description="Run multiple test scripts and generate a combined report")
    parser.add_argument("scripts", nargs="*", help="List of test scripts to run (Python files)")
    parser.add_argument("--headless", action="store_true", help="Run in headless mode (no user interaction)")
    args = parser.parse_args()
    
    print("Desktop Automation Recorder - Test Runner")
    print(f"Running in {'headless' if args.headless else 'interactive'} mode")
    
    # Run tests
    results = run_tests(args.scripts, args.headless)
    
    # Save combined report
    save_combined_report(results)
    
    # Print summary
    print_summary(results)
    
    # Return exit code based on test results
    return 0 if results["failed_tests"] == 0 and results["error_tests"] == 0 else 1

if __name__ == "__main__":
    sys.exit(main()) 