{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cacb624-02ad-4318-9ca8-84a23fe94777",
   "metadata": {},
   "source": [
    "This notebook was originally posted [here](https://github.com/guillaumeeb/supaero-otsu-course/blob/27de0375d3aa60a4c6278b123aa5c80fc6c3d650/.github/ndsi_sol.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af70523-7851-4e5f-8248-87e0cfb79d55",
   "metadata": {},
   "source": [
    "# OTSU Big Data Cloud BE\n",
    "\n",
    "Welcome to this training of ISAE Supaero OTSU Cloud, Big Data and Machine Learning module. \n",
    "\n",
    "The end goal of this exercise is to build temporal statistics from several Seninel-2 L2A products accessed from an object storage. In order to do this, we'll read the products using (rio)Xarray with a Dask backend, directly from Google Cloud Storage, compute a NDSI over ten dates and plot snow cover evolution accross the period.\n",
    "\n",
    "By doing this, we will learn the folowing things:\n",
    "\n",
    "- How to access a Cloud storage and browse its objects,\n",
    "- How to use rioxarray to access and load Satellite imagery,\n",
    "- How to use Xarray Dataset class to build a complete timeseries dataset,\n",
    "- How to chunk our data and use Dask to perform bigger than memory or distributed analysis over our entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff6544-8687-4441-8dec-05c80f3a31a2",
   "metadata": {},
   "source": [
    "## Imports and settings\n",
    "\n",
    "As always, this begins with imports. We sen set some environment variable to easily acces Google Cloud Storage anonymously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d61d7-42b2-43cf-b357-28cb680b533a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gcsfs dask distributed matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c259a5-f645-48f9-ba76-e83bce896159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from jupytergis.tiler import GISDocument\n",
    "import gcsfs\n",
    "import rioxarray\n",
    "import rasterio\n",
    "import os\n",
    "import numpy as np\n",
    "from distributed import Client\n",
    "import xarray as xr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872af06f-3098-474c-8d25-950aa2281b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env():\n",
    "    os.environ[\"GS_NO_SIGN_REQUEST\"] = \"YES\"\n",
    "\n",
    "set_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450ef8b-1836-43e6-9884-25e0420b5819",
   "metadata": {},
   "source": [
    "## Check you have access to GCS bucket\n",
    "\n",
    "We can use gcsfs package, which mimics a file system usage on an object storage.\n",
    "\n",
    "It is pretty easy on a public bucket with anonymous access authorized, like the one we use. You can see that an object store with fsspec can behave a lot like a standard fil system. Just don't forget it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77180241-340b-4da2-af6f-79a21e6ba230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem(bucket_name=\"supaero\", token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a03a66d-f5c1-46ae-bc49-30f78932d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls('supaero/31TCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1b94a-0ef8-4699-b426-d4214f1eed7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs.ls('supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b4529-571e-4965-9a16-16072fbbe322",
   "metadata": {},
   "source": [
    "### The products\n",
    "\n",
    "As you can see, we'll use Sentinel-2 L2A products, which have been create using Maja algorithm from CNES.\n",
    "Do not hesitate to check Maja or [Theia website](https://www.theia-land.fr/en/product/sentinel-2-surface-reflectance/) to know more about these.\n",
    "\n",
    "For example :\n",
    "\n",
    "- How many bands a product has?\n",
    "- What correction to the L1C product does MAJA makes?\n",
    "- Have every band the same resolution (this is important for the following)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a711fc6-bdd6-401a-b0c1-8678c4155485",
   "metadata": {},
   "source": [
    "## Reading a product band using rioxarray and rasterio\n",
    "\n",
    "Rasterio is the Pythonic interface to GDAL, the go to tool when dealing with satelite products. Rioxarray is a library that makes it easy to read rasterio compatible products as Xarray DataArray or Datasets.\n",
    "\n",
    "At first, we'll read only a subsection of a product, using classical Numpy slices. This way, only selected pixels will be loaded into memory (important when using Dask afterwards, and to not blow up your computer or server memory).\n",
    "\n",
    "Just use the rioxarray.open_rasterio function to open a random tif file from above, then slice it in order to keep x and y dimension from pixels 4000 to 5000.\n",
    "Wee'll have use an URL like \"gs://bucket/path\".\n",
    "\n",
    "Opening the file should be pretty fast. Then plot the dataset in another cell, data should really be accessed at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db657988-8180-4b97-b64c-9df16c4546e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "swir = rioxarray.open_rasterio(\"gs://supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2_FRE_B11.tif\")\n",
    "swir = swir[:,4000:5000,4000:5000]\n",
    "swir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30d211-0a44-439c-bccc-18929ee99141",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "swir.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfb25f-c983-4894-b9c8-0b5a42e2ed56",
   "metadata": {},
   "source": [
    "Even with the swir band, we can see we are in mountainous area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90af8e-609d-4512-bce8-5b4518ee9cbe",
   "metadata": {},
   "source": [
    "## Building a single date Dataset\n",
    "\n",
    "First, without using Dask, will build an Xarray Dataset from two DataArray that share the same dimensions and coordinates.\n",
    "\n",
    "As we'll read two bands that to not share the same resolution, we'll have to resample the green band (using a simple mean).\n",
    "\n",
    "Then, we'll use the Dataset to compute two more variable, NDSI, and snow mask.\n",
    "\n",
    "### Build DataArrays\n",
    "\n",
    "So first, we'll read the swir band as from above, but this time, we'll need to make sure that we handle the nodata value correctly (this will be important in the next part of this exercise). To do so, as these L2A products are note really standard, we need to manually remove the no data values using a filter. Then write it in rioxarray metadata.\n",
    "\n",
    "So open the swir band as above, slice it the same way, but then user xarray.where to remove nodata values (-10_000), and use rioxarray write_no_data with encoded and inplace kwargs to set it correctly inside the data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24789b-2872-45ec-b8e6-a9294aa7caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "swir = rioxarray.open_rasterio(\"gs://supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2_FRE_B11.tif\")\n",
    "swir = swir[:,4000:5000,4000:5000]\n",
    "#No data handling\n",
    "swir = swir.where(swir != -10000)\n",
    "swir.rio.write_nodata(-10000, encoded=True, inplace=True)\n",
    "swir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb94d0-42ae-4094-8361-b46f101ae303",
   "metadata": {},
   "outputs": [],
   "source": [
    "swir.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467fa4e-3c99-49f6-bf23-59b76bcf4aff",
   "metadata": {},
   "source": [
    "Note that Xarray with numpy backend is already a bit lazy:\n",
    "- If you only open a file, only its metadata are read, nothin is loaded until an operation like plot or load is performed.\n",
    "- When applying a metadata filter, we transform and load the data, how can you see it in the HTML representation of the DataArray in the notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4c09a-d28b-4293-8734-1384d65240cf",
   "metadata": {},
   "source": [
    "Now, we need to open the green band. Note that its resolution is two times better on spatial dimension than the SWIR band.\n",
    "\n",
    "In order to build a dataset or perform operations, we need to resample the data to the same resolution as the SWIR band.\n",
    "\n",
    "There are several ways to do this, including reproject from rioxarray, but since here we only want to divide resolution by exactly two, we'll use a simple mean through the coarsen function of Xarray.\n",
    "\n",
    "So to sum up, in the following cell you'll need to:\n",
    "- Open the green band of the same product as above,\n",
    "- Slice the data before reading it, we only want to load only the data of interest. Be carefull, we've got twice as many pixels at the beginning, so be sure to take the correct amount of pixels, and at the correct indices.\n",
    "- Apply coarsen to the result, with x=2, y=2, boundary='pad', and take the mean.\n",
    "- As above, filter nodata values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28dc29-828d-4cdf-999c-e0cba00afd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "green = rioxarray.open_rasterio(\"gs://supaero/31TCH/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2/SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2_FRE_B3.tif\")\n",
    "green = green[:,8000:10000,8000:10000]\n",
    "# Rééchantillonage à 20m, diviser résolution par 2\n",
    "green = green.coarsen(x=2, y=2, boundary='pad').mean()\n",
    "#No data\n",
    "green = green.where(green != -10000)\n",
    "green.rio.write_nodata(-10000, encoded=True, inplace=True)\n",
    "green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5594cc-3967-4d52-9784-51b4184b47c7",
   "metadata": {},
   "source": [
    "Then plot the DataArray you just created.\n",
    "\n",
    "You should observe the same mountains, but this time, with the green band, you should see some snow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856133b-fd0d-4297-b3d5-d5e24d340c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "green.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd414ca-ad69-4230-8203-4d916aff9f5f",
   "metadata": {},
   "source": [
    "### Compute the NDSI using DataArrays\n",
    "\n",
    "Since the two DataArrays are of the same dimension, you can already perform operations with both of them.\n",
    "\n",
    "Use them two compute the [Normalised Difference Snow Index (NDSI)](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/ndsi/).\n",
    "\n",
    "This should be as straigthforward as with Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bf750-7cea-47d4-8aa5-dda6b5e8b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndsi = (green - swir) / (green + swir)\n",
    "ndsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622754f-28b0-433c-8cae-500d34f7b6f3",
   "metadata": {},
   "source": [
    "Did you tried to plot it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf254c8-c0ad-450f-ad50-e8f34b37f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndsi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62c9f2-1dd6-41c2-805f-392d77e177c8",
   "metadata": {},
   "source": [
    "It has been established by research that a level of 0.4 in NDSI usually means there is snow.\n",
    "\n",
    "Just use this threshold to compute a boolean DataArray representing the snow cover over this particular area, and plot it. There should be plenty of snow on your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514108d-5b43-405c-bf0f-45b6351f6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ndsi > 0.4).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0712d3b-8833-4abd-92b7-8f721e9c6686",
   "metadata": {},
   "source": [
    "### Xarray Datasets\n",
    "\n",
    "Now, we'll just use what we've done above to build a Simple Dataset made of several variables, because this will be handy in the next part of the exercise.\n",
    "\n",
    "Building a Dataset is as simple as using its constructor (xr.Dataset()) and giving it a dict with a string name associated to a DataArray.\n",
    "\n",
    "Build a dataset with a green and swir variable pointing to the DataArray we built above, then display it to see its structure. You can also play with the Database buttons or metadata to have more informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a82bd0-d2aa-48fb-b915-d2b72ed17ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ds = xr.Dataset({\"green\": green, \"swir\": swir})\n",
    "sub_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c5a4f-b5c9-4060-a42f-eeb9fdfc0f21",
   "metadata": {},
   "source": [
    "A nice feature about Dataset is to be able to build and store new variables. We'll see also in the next part that we can add Temporal index as dimensions.\n",
    "\n",
    "For now, we'll just create two new variable in this Dataset:\n",
    "- a ndsi variable, that is linked to the NDSI computation over the two other variables,\n",
    "- a snow variable, boolean array as above.\n",
    "\n",
    "Adding variable is a simple as affecting a new Column in pandas, e.g. dataset[\"ndsi\"] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b969e5-6607-4fbd-a858-05edff3bde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ds[\"ndsi\"] = (sub_ds.green - sub_ds.swir) / (sub_ds.green + sub_ds.swir)\n",
    "sub_ds[\"snow\"] = sub_ds.ndsi > 0.4\n",
    "sub_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fd86e-e244-416a-8caf-57a904f13e1e",
   "metadata": {},
   "source": [
    "Then plot the snow variable, just to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f01a4d-4c37-482b-854d-286fdacc8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ds.snow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35229463-03f5-40b7-bc2f-23020bd1865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = int(sub_ds.ndsi.min().compute()), int(sub_ds.ndsi.max().compute())\n",
    "vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25378e81-bc99-4ffc-8a2b-11cf3528bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = GISDocument(longitude=1.7092461496028497, latitude=42.530360648396055, zoom=11.38992197518327)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a42f41-ad5d-4a44-a052-a219a53a8c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await doc.add_tiler_layer(\n",
    "    name=\"NDSI Layer\",\n",
    "    data_array=sub_ds.ndsi,\n",
    "    colormap_name=\"viridis\",\n",
    "    rescale=(vmin, vmax),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2366bc14-6e59-445d-aafe-b55100358e06",
   "metadata": {},
   "source": [
    "### Compute snow percentage over the area\n",
    "\n",
    "The end goalg below is to plot the evolution of snow cover accross a time serie. But how to plot a snow cover percentage over one product?\n",
    "\n",
    "You'll need to count pixels having snow, and divide this number by the number of pixels wich are not nodata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6759a9-bf78-4a1b-be38-699803553172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ds.snow.sum() / sub_ds.snow.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba935e-5633-4566-b74b-0edf70c63284",
   "metadata": {},
   "source": [
    "On this particular patch, you should get over 77% of snow cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d5067a-6467-4030-ae12-fcc92661f029",
   "metadata": {},
   "source": [
    "## Compute a time series analysis\n",
    "\n",
    "Now that we know how to create a single timestamp dataset, we'll build a dataset with a time dimension.\n",
    "The idea is to stack single temporal datasets into a single one using a new time dimension.\n",
    "\n",
    "Up to now, we've only built datasets that easily fit in memory, by taking only part of one observation. \n",
    "In order to be able to work on full images and on ten products, we'll need to use Dask.\n",
    "\n",
    "### Start a Dask cluster\n",
    "\n",
    "Configure it according to your computing power. Then we'll need to set environment variable on every worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840c368-e1b5-46e3-856b-c507fcff3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='1GiB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b486824-5a73-4fb3-9869-aec6f0c29baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(set_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb353a-95b2-47db-8763-ec8e1a488967",
   "metadata": {},
   "source": [
    "It's really interesting when using Dask to start a real Distributed cluster (even on one machine like we've done above).\n",
    "\n",
    "This gives access to a nice Dashboard, just click on the link displayed above (ending with 8787/status). If you are using binder, you should replace the URL with something like:\n",
    "\n",
    "https://hub.2i2c.mybinder.org/user/guillaumeeb-supaero-otsu-course-4kfqus3j/proxy/8787/status\n",
    "\n",
    "First part of the URL should be copied from you Jupyterlab URL on your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a17ae2-dc0b-4d28-a726-9ba41a6cff0d",
   "metadata": {},
   "source": [
    "### Define some functions to build Datasets\n",
    "\n",
    "OK, in order to simplify the building of our time serie, we'll define some functions for the following:\n",
    "- Reading one band of a product. It should also handle an optional resampling. This time, we also want to use a Dask backend.\n",
    "- Creating a single time Dataset with green and swir band.\n",
    "\n",
    "So first, we'll define a read_one_band(product, band, coarsen=1) function:\n",
    "- product is the name of the product, i.e. the \"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\" part, which is repeated in a folder, and in the file name.\n",
    "- band is the band identifier, i.e. B3 or B11 in our case. We always access \"FRE\" bands in our case.\n",
    "- coarsen is the level of subsambling, it should be 1 by default (no subsampling) for the SWIR band, set it to 2 for GREEN to perform a resampling.\n",
    "- When opening the file with rioxarray, we'll use two new arguments: chunks, and lock=False. chunks indicate that we want chunked array as a backend, so Dask Arrays. You use (-1, 1024 * coarsen, 1024*coarsen) as a value before opening the file.\n",
    "- We also want to remove the band dimension that is useless, in order to do so, just apply .squeeze('band', drop=True) right after the opening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d021c-14f5-4512-abf6-80049959ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_band(product, band, coarsen=1):\n",
    "    chunks=(-1, 1024*coarsen, 1024*coarsen)\n",
    "    band = rioxarray.open_rasterio(f\"gs://supaero/31TCH/{product}/{product}_FRE_{band}.tif\", \n",
    "                                chunks=chunks,\n",
    "                                lock=False).squeeze('band', drop=True)\n",
    "    band = band.where(band != -10000)\n",
    "    band.rio.write_nodata(-10000, encoded=True, inplace=True)\n",
    "    if coarsen > 1:\n",
    "        band = band.coarsen(x=coarsen, y=coarsen, boundary='pad').mean()\n",
    "    return band"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aebc25-9e13-43a5-a88f-04aa4f63d833",
   "metadata": {},
   "source": [
    "Try this function on the green band of \"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\", so with a coarsen=2 value, and whatch the HTML repr. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9766be-5cd7-415b-9462-808c999174ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_one_band(\"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\", \"B3\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcfc33-e2a3-43dd-a919-72342b6b98f6",
   "metadata": {},
   "source": [
    "Now, we'll define a create_dataset(product) function.\n",
    "\n",
    "Product argument is the same as above, and it will create a 2 dimension dataset with two DataArray variables: green and swir, at the same resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82417c7-aa06-49cd-b459-690da9939e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(product):\n",
    "    ds = xr.Dataset({\"green\": read_one_band(product, \"B3\", 2), \"swir\": read_one_band(product, \"B11\")})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d5b12-9a6f-4d27-b0d6-eb1b1bdc3b07",
   "metadata": {},
   "source": [
    "Try it on the above product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c427ab7-efb0-4a50-bba0-4ece007d1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(\"SENTINEL2B_20191224-104910-788_L2A_T31TCH_C_V2-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34c381-ab46-4e2d-8c53-af0dd637aef4",
   "metadata": {},
   "source": [
    "### Build our time serie\n",
    "\n",
    "We are now ready to build a time series of our ten products.\n",
    "\n",
    "In order to do that, we'll need:\n",
    "- our product list (which is built in the below cell)\n",
    "- A list of containing all of our datasets\n",
    "- A Pandas Datetime index corresponding to our product list (code also given)\n",
    "- Apply xarray concat on our dataset list associated with our new Index as a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8faa01-2ba7-4691-8f09-0cd299300a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = [path.split(\"/\")[-1] for path in fs.ls('supaero/31TCH')]\n",
    "product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba060c-f99f-4438-8d9d-a6c63dd87a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time index\n",
    "dates = [product.split(\"_\")[1] for product in product_list]\n",
    "dt_index = pd.to_datetime(dates, format=\"%Y%m%d-%H%M%S-%f\")\n",
    "dt_index.name = \"time\"\n",
    "dt_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63dbaa7-a52b-48a5-bd98-c105bbb84c66",
   "metadata": {},
   "source": [
    "So now, create a list by creating a dataset from all of the products above.\n",
    "\n",
    "Then, use xr.concat to create a new overall dataset with a new time dimension and all of our single time step datasets in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb6d81-300d-4da7-b145-898188e31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for product in product_list:\n",
    "    datasets.append(create_dataset(product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e20ab-36e8-4502-8b19-4b3938455b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ds = xr.concat(datasets, dt_index)\n",
    "complete_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae2100-1d94-4582-8244-08726d9981f8",
   "metadata": {},
   "source": [
    "### Add NDSI and snow mask\n",
    "\n",
    "Just as with a single time step Dataset, you can easily add two new variables to this complete Dataset as above.\n",
    "\n",
    "So create the new ndsi and snow variables, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dc1dd-48a2-494a-b2d6-6b930d4dad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ds[\"ndsi\"] = (complete_ds.green - complete_ds.swir) / (complete_ds.green + complete_ds.swir)\n",
    "complete_ds[\"snow\"] = complete_ds.ndsi > 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661404a7-2d87-48a0-81b7-1ebdb70506b9",
   "metadata": {},
   "source": [
    "### Compute snow percentage time series over the whole image\n",
    "\n",
    "Did you notice?\n",
    "\n",
    "All the operation above where pretty fast, no? That's because we didn't really computed anything. By using Dask backend and Xarray, we only worked using product metadata, all the real values are still waiting to be computed. This is the lazy power of Dask with Xarray.\n",
    "\n",
    "But now, if we want some result, we'll need to trigger a computation.\n",
    "\n",
    "What we are interested in is a time series representing the snow percentage evolution over the ten dates.\n",
    "\n",
    "We'll need to count snow pixel and sum them over spatial dimension, count also the not nodata values over each time step on the spatial dimension, and just divide the two time series to get the percentage.\n",
    "\n",
    "In order to trigger the computation, we'll need to use compute method.\n",
    "\n",
    "Then whatch the Daks Dashboard. This should take some time, depending on your resources (on Binder it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54285c92-8b41-4380-b570-adf21aaa112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "snow_percentage = (complete_ds.snow.sum(dim=[\"x\", \"y\"]) / complete_ds.ndsi.count(dim=[\"x\", \"y\"])).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4839e8b-6dbd-4275-835e-fc17b2676ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfa083-6bb6-4e65-9420-3cc1b0ae0d52",
   "metadata": {},
   "source": [
    "### And what is the result?\n",
    "\n",
    "Now, we just want to plot the results.\n",
    "\n",
    "As we've not sorted our dataset by dates at the beginning, you'll want to do it first, using xarray sortby method.\n",
    "\n",
    "Then just plot, or better, plot.step(where=\"mid\") for a nicer view (in my opinion).\n",
    "\n",
    "Does this snow cover percentage time serie looks right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522418ac-3cc4-4211-b5b0-26d2cc087b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_percentage.sortby(\"time\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d66943-2509-4793-ab9b-e66cce822b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_percentage.sortby(\"time\").plot.step(where=\"mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e7348-0209-4fa4-bb42-c8461be313d1",
   "metadata": {},
   "source": [
    "# Extends other statistics\n",
    "\n",
    "- Snow area\n",
    "- NDSI\n",
    "- Add elevation, and compute snowline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
