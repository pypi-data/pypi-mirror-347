{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Getting Gai Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Version:  1.0\n",
                        "TTT Client: client_type='gai' type='ttt' engine=None model=None name=None url='http://gai-ttt-svr:12031/gen/v1/chat/completions' env=None extra=None hyperparameters={}\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "file_path =  os.path.join(get_local_datadir(),\"gai.yml\")\n",
                "\n",
                "from gai.lib.config import GaiConfig\n",
                "config = GaiConfig.from_path(file_path)\n",
                "print(\"Version: \", config.version)\n",
                "print(\"TTT Client:\", config.clients[\"ttt\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Client Config\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get from Name\n",
                "\n",
                "Get from default config\n",
                "\n",
                "<div style=\"background: #f4f4f4; border-left: 5px solid black; padding: 10px;color:black\">\n",
                "    <code>\n",
                "        client_config = GaiClientConfig.from_name(name=\"dolphin\")\n",
                "    </code>\n",
                "</div>\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "client_type='ollama' type='ttt' engine='ollama' model='llama3.1' name='llama3.1' url=None env=None extra={'num_ctx': 4096} hyperparameters={}\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiClientConfig\n",
                "\n",
                "file_path =  os.path.join(get_local_datadir(),\"gai.yml\")\n",
                "\n",
                "client_config = GaiClientConfig.from_name(name=\"llama3.1\",file_path=file_path)\n",
                "print(client_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "client_type='openai' type='ttt' engine='openai' model='gpt-4o' name='gpt-4o' url=None env={'OPENAI_API_KEY': '${OPENAI_API_KEY}'} extra=None hyperparameters={}\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiClientConfig\n",
                "file_path =  os.path.join(get_local_datadir(),\"gai.yml\")\n",
                "\n",
                "client_config = GaiClientConfig.from_name(name=\"gpt-4o\",file_path=file_path)\n",
                "print(client_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "client_type='gai' type='ttt' engine='llamacpp' model='dolphin' name='ttt-llamacpp-dolphin' url='http://gai-ttt-svr:12031/gen/v1/chat/completions' env=None extra=None hyperparameters={}\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiClientConfig\n",
                "file_path =  os.path.join(get_local_datadir(),\"gai.yml\")\n",
                "\n",
                "client_config = GaiClientConfig.from_name(name=\"ttt\",file_path=file_path)\n",
                "print(client_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get from Dict\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "client_type='gai' type='ttt' engine='exllamav2' model='dolphin' name='dolphin' url='http://gai-ttt-svr:12031/gen/v1/chat/completions' env=None extra=None hyperparameters={}\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiClientConfig\n",
                "file_path =  os.path.join(get_local_datadir(),\"gai.yml\")\n",
                "\n",
                "client_config = GaiClientConfig.from_dict(client_config={\n",
                "    \"type\":\"ttt\",\n",
                "    \"engine\":\"exllamav2\",\n",
                "    \"model\":\"dolphin\",\n",
                "    \"name\":\"dolphin\",\n",
                "    \"client_type\":\"gai\",\n",
                "    \"url\":\"http://gai-ttt-svr:12031/gen/v1/chat/completions\"\n",
                "})\n",
                "\n",
                "print(client_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Generator Config\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### a) Reset Dummy Gai Config (./tmp/gai.yml)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "generators in gai config= {}\n",
                        "generators in server config= {'ttt': GaiGeneratorConfig(type='ttt', engine='exllamav2', model='dolphin3.0_llama3.1:4.25bpw', name='dolphin3.0_llama3.1:4.25bpw:exl2', hyperparameters={'temperature': 0.85, 'top_p': 0.8, 'top_k': 50, 'max_tokens': 1000, 'tool_choice': 'auto', 'max_retries': 5, 'stop': ['<|im_end|>', '</s>', '[/INST]']}, extra={'model_path': 'models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', 'max_seq_len': 8192, 'prompt_format': 'llama', 'no_flash_attn': True, 'seed': None, 'decode_special_tokens': False}, module=ModuleConfig(name='gai.chat.server.gai_exllamav2', class_='GaiExLlamav2'), source=HuggingfaceDownloadConfig(type='huggingface', local_dir='Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', repo_id='bartowski/Dolphin3.0-Llama3.1-8B-exl2', revision='896301e945342d032ef0b3a81b57f0d5a8bac6fe', file=None)), 'dolphin3.0_llama3.1:4.25bpw:exl2': GaiGeneratorConfig(type='ttt', engine='exllamav2', model='dolphin3.0_llama3.1:4.25bpw', name='dolphin3.0_llama3.1:4.25bpw:exl2', hyperparameters={'temperature': 0.85, 'top_p': 0.8, 'top_k': 50, 'max_tokens': 1000, 'tool_choice': 'auto', 'max_retries': 5, 'stop': ['<|im_end|>', '</s>', '[/INST]']}, extra={'model_path': 'models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', 'max_seq_len': 8192, 'prompt_format': 'llama', 'no_flash_attn': True, 'seed': None, 'decode_special_tokens': False}, module=ModuleConfig(name='gai.chat.server.gai_exllamav2', class_='GaiExLlamav2'), source=HuggingfaceDownloadConfig(type='huggingface', local_dir='Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', repo_id='bartowski/Dolphin3.0-Llama3.1-8B-exl2', revision='896301e945342d032ef0b3a81b57f0d5a8bac6fe', file=None))}\n"
                    ]
                }
            ],
            "source": [
                "# Reset ./tmp/gai.yml\n",
                "\n",
                "import os\n",
                "from gai.lib.tests import make_local_tmp,get_local_datadir\n",
                "data_path = get_local_datadir()\n",
                "app_path=make_local_tmp()\n",
                "\n",
                "source_gai_config_path = os.path.join(data_path,\"gai.yml\")\n",
                "gai_config_path = os.path.join(app_path,\"gai.yml\")\n",
                "\n",
                "import shutil\n",
                "shutil.copyfile(source_gai_config_path,gai_config_path)\n",
                "\n",
                "## \"./tmp/gai.yml\" should have no generators\n",
                "\n",
                "from gai.lib.config import GaiGeneratorConfig\n",
                "gai_generators = GaiGeneratorConfig._list_generator_configs(file_path=gai_config_path)\n",
                "print(\"generators in gai config=\", gai_generators)\n",
                "assert len(gai_generators) == 0\n",
                "\n",
                "## \"./data/generators_config/gai.yml\" should have 2 generators\n",
                "\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiGeneratorConfig\n",
                "\n",
                "data_path = get_local_datadir()\n",
                "server_config_path = os.path.join(data_path,\"generator_config\",\"gai.yml\")\n",
                "\n",
                "server_generators = GaiGeneratorConfig._list_generator_configs(file_path=server_config_path)\n",
                "print(\"generators in server config=\", server_generators)\n",
                "assert len(server_generators) == 2\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Update Gai Config (./tmp/gai.yml)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'ttt': GaiGeneratorConfig(type='ttt', engine='exllamav2', model='dolphin3.0_llama3.1:4.25bpw', name='dolphin3.0_llama3.1:4.25bpw:exl2', hyperparameters={'temperature': 0.85, 'top_p': 0.8, 'top_k': 50, 'max_tokens': 1000, 'tool_choice': 'auto', 'max_retries': 5, 'stop': ['<|im_end|>', '</s>', '[/INST]']}, extra={'model_path': 'models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', 'max_seq_len': 8192, 'prompt_format': 'llama', 'no_flash_attn': True, 'seed': None, 'decode_special_tokens': False}, module=ModuleConfig(name='gai.chat.server.gai_exllamav2', class_='GaiExLlamav2'), source=HuggingfaceDownloadConfig(type='huggingface', local_dir='Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', repo_id='bartowski/Dolphin3.0-Llama3.1-8B-exl2', revision='896301e945342d032ef0b3a81b57f0d5a8bac6fe', file=None)),\n",
                            " 'dolphin3.0_llama3.1:4.25bpw:exl2': GaiGeneratorConfig(type='ttt', engine='exllamav2', model='dolphin3.0_llama3.1:4.25bpw', name='dolphin3.0_llama3.1:4.25bpw:exl2', hyperparameters={'temperature': 0.85, 'top_p': 0.8, 'top_k': 50, 'max_tokens': 1000, 'tool_choice': 'auto', 'max_retries': 5, 'stop': ['<|im_end|>', '</s>', '[/INST]']}, extra={'model_path': 'models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', 'max_seq_len': 8192, 'prompt_format': 'llama', 'no_flash_attn': True, 'seed': None, 'decode_special_tokens': False}, module=ModuleConfig(name='gai.chat.server.gai_exllamav2', class_='GaiExLlamav2'), source=HuggingfaceDownloadConfig(type='huggingface', local_dir='Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', repo_id='bartowski/Dolphin3.0-Llama3.1-8B-exl2', revision='896301e945342d032ef0b3a81b57f0d5a8bac6fe', file=None))}"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "GaiGeneratorConfig.update_gai_config(local_config_path=server_config_path,global_config_path=gai_config_path)\n",
                "GaiGeneratorConfig._list_generator_configs(file_path=gai_config_path)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get from Dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "type='ttt' engine='exllamav2' model='dolphin' name='dolphin3.0_llama3.1:4.25bpw:exl2' hyperparameters={} extra=None module=ModuleConfig(name='gai.ttt.exllamav2.gai_exllamav2', class_='GaiExllamav2') source=None\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.config import GaiGeneratorConfig\n",
                "here = os.getcwd()\n",
                "file_path =  os.path.abspath(os.path.join(here,\"..\",\"..\",\"..\",\"1-lib\",\"src\",\"gai\",\"scripts\",\"data\",\"gai.yml\"))\n",
                "\n",
                "generator_config = GaiGeneratorConfig.from_dict(generator_config={\n",
                "    \"type\":\"ttt\",\n",
                "    \"engine\":\"exllamav2\",\n",
                "    \"model\":\"dolphin\",\n",
                "    \"name\":\"dolphin3.0_llama3.1:4.25bpw:exl2\",\n",
                "    \"module\": {\n",
                "        \"name\":\"gai.ttt.exllamav2.gai_exllamav2\",\n",
                "        \"class\":\"GaiExllamav2\"\n",
                "    }\n",
                "})\n",
                "\n",
                "print(generator_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get from Name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "type='ttt' engine='exllamav2' model='dolphin3.0_llama3.1:4.25bpw' name='dolphin3.0_llama3.1:4.25bpw:exl2' hyperparameters={'temperature': 0.85, 'top_p': 0.8, 'top_k': 50, 'max_tokens': 1000, 'tool_choice': 'auto', 'max_retries': 5, 'stop': ['<|im_end|>', '</s>', '[/INST]']} extra={'model_path': 'models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', 'max_seq_len': 8192, 'prompt_format': 'llama', 'no_flash_attn': True, 'seed': None, 'decode_special_tokens': False} module=ModuleConfig(name='gai.chat.server.gai_exllamav2', class_='GaiExLlamav2') source=HuggingfaceDownloadConfig(type='huggingface', local_dir='Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', repo_id='bartowski/Dolphin3.0-Llama3.1-8B-exl2', revision='896301e945342d032ef0b3a81b57f0d5a8bac6fe', file=None)\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiGeneratorConfig\n",
                "file_path =  os.path.join(get_local_datadir(),\"generator_config\",\"gai.yml\")\n",
                "\n",
                "generator_config = GaiGeneratorConfig.from_name(name=\"dolphin3.0_llama3.1:4.25bpw:exl2\",file_path=file_path)\n",
                "print(generator_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Get from Ref"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "type='ttt' engine='exllamav2' model='dolphin3.0_llama3.1:4.25bpw' name='dolphin3.0_llama3.1:4.25bpw:exl2' hyperparameters={'temperature': 0.85, 'top_p': 0.8, 'top_k': 50, 'max_tokens': 1000, 'tool_choice': 'auto', 'max_retries': 5, 'stop': ['<|im_end|>', '</s>', '[/INST]']} extra={'model_path': 'models/Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', 'max_seq_len': 8192, 'prompt_format': 'llama', 'no_flash_attn': True, 'seed': None, 'decode_special_tokens': False} module=ModuleConfig(name='gai.chat.server.gai_exllamav2', class_='GaiExLlamav2') source=HuggingfaceDownloadConfig(type='huggingface', local_dir='Dolphin3.0-Llama3.1-8B-4_25bpw-exl2', repo_id='bartowski/Dolphin3.0-Llama3.1-8B-exl2', revision='896301e945342d032ef0b3a81b57f0d5a8bac6fe', file=None)\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from gai.lib.tests import get_local_datadir\n",
                "from gai.lib.config import GaiGeneratorConfig\n",
                "file_path =  os.path.join(get_local_datadir(),\"generator_config\",\"gai.yml\")\n",
                "\n",
                "generator_config = GaiGeneratorConfig.from_name(name=\"ttt\",file_path=file_path)\n",
                "print(generator_config)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
