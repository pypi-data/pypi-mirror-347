{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpvUOuC3j27n",
        "outputId": "682e9efc-e9e7-4366-9771-f3145395568c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFIMfPmgQa0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74f9c14-9312-45ad-f15c-8bf2b084b67b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.90.201.34:8470']\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(TPU)\n",
        "tf.tpu.experimental.initialize_tpu_system(TPU)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(TPU)\n",
        "\n",
        "print('Running on TPU ', TPU.cluster_spec().as_dict()['worker'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining The Metadata"
      ],
      "metadata": {
        "id": "0t1jfsJlM3SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# META ########################################################################\n",
        "\n",
        "N_VOCABULARY_DIM = 37\n",
        "N_CONTEXT_DIM = 256\n",
        "N_EMBEDDING_DIM = 512\n",
        "N_HIDDEN_DIM = 4 * N_EMBEDDING_DIM # = 4 * N_ATTENTION_DIM * N_ATTENTION_HEAD\n",
        "N_ATTENTION_HEAD = 8\n",
        "N_ATTENTION_DIM = N_EMBEDDING_DIM // N_ATTENTION_HEAD\n",
        "N_ATTENTION_BLOCK = 2\n",
        "\n",
        "N_EPOCHS = 16\n",
        "N_EPOCHS_RAMPUP = 4\n",
        "N_EPOCHS_SUSTAIN = 0\n",
        "\n",
        "N_BATCH = 128\n",
        "\n",
        "N_SAMPLE = 256\n",
        "\n",
        "R_MIN = 0.00001\n",
        "R_MAX = 0.0001 * tpu_strategy.num_replicas_in_sync\n",
        "R_EXP = .8\n",
        "\n",
        "VERSION = 'sat-keras-125k'"
      ],
      "metadata": {
        "id": "8Z74MlibMWnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading The Data"
      ],
      "metadata": {
        "id": "dEyFtkcFNGe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD ####################################################################\n",
        "\n",
        "!wget --show-progress --continue -O sample_data/hamlet.md https://raw.githubusercontent.com/apehex/mlable/main/.data/shakespeare/hamlet.md\n",
        "!wget --show-progress --continue -O sample_data/othello.md https://raw.githubusercontent.com/apehex/mlable/main/.data/shakespeare/othello.md\n",
        "!wget --show-progress --continue -O sample_data/macbeth.md https://raw.githubusercontent.com/apehex/mlable/main/.data/shakespeare/macbeth.md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a3SQUQHNJ6M",
        "outputId": "cb269745-75d0-474c-c5ac-f8a3a0d5ac74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-02 12:02:53--  https://raw.githubusercontent.com/apehex/mlable/main/.data/shakespeare/hamlet.md\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-02-02 12:02:53--  https://raw.githubusercontent.com/apehex/mlable/main/.data/shakespeare/othello.md\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2024-02-02 12:02:53--  https://raw.githubusercontent.com/apehex/mlable/main/.data/shakespeare/macbeth.md\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD ########################################################################\n",
        "\n",
        "TEXT = tf.io.gfile.GFile('sample_data/othello.md', 'r').read() # .splitlines()\n",
        "TEXT += tf.io.gfile.GFile('sample_data/hamlet.md', 'r').read() # .splitlines()\n",
        "TEXT += tf.io.gfile.GFile('sample_data/macbeth.md', 'r').read() # .splitlines()"
      ],
      "metadata": {
        "id": "x1HZhkT9Rtz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "8-FEtzM4CvPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODING / DECODING #########################################################\n",
        "\n",
        "# CONSTANTS\n",
        "\n",
        "BLANK = chr(0)\n",
        "\n",
        "# LIST\n",
        "\n",
        "def capture(text: str, blank: str=BLANK) -> str:\n",
        "    return sorted(list(set(text).union({blank})))\n",
        "\n",
        "# MAPPINGS\n",
        "\n",
        "def mappings(vocabulary: list) -> dict:\n",
        "    __itos = {__i: __c for __i, __c in enumerate(vocabulary)}\n",
        "    __stoi = {__c: __i for __i, __c in enumerate(vocabulary)}\n",
        "    # blank placeholder\n",
        "    __blank_c = __itos[0] # blank\n",
        "    __blank_i = 0 # len(vocabulary)\n",
        "    # s => i\n",
        "    def __encode(c: str) -> int:\n",
        "        return __stoi.get(c, __blank_i)\n",
        "    # i => s\n",
        "    def __decode(i: int) -> str:\n",
        "        return __itos.get(i, __blank_c)\n",
        "    # return both\n",
        "    return {'encode': __encode, 'decode': __decode}\n",
        "\n",
        "# ENCODING\n",
        "\n",
        "def encode(text: str, stoi: callable) -> list:\n",
        "    return [stoi(__c) for __c in text]\n",
        "\n",
        "def decode(sequence: list, itos: callable) -> list:\n",
        "    return ''.join([itos(__i) for __i in sequence])"
      ],
      "metadata": {
        "id": "nS6V8b-YC0j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZE ####################################################################\n",
        "\n",
        "# TEXT TO LIST\n",
        "\n",
        "def tokenize(text: str, length: int, blank=BLANK):\n",
        "    __context = length * blank\n",
        "    for __c in text:\n",
        "        yield __context\n",
        "        __context = __context[1:] + __c\n",
        "\n",
        "# TEXT TO VECTOR\n",
        "\n",
        "def dataset(text: list, stoi: callable, depth: int, context: int) -> tuple:\n",
        "    __x = [encode(text=__n, stoi=stoi) for __n in tokenize(text=text, length=context)]\n",
        "    __y = encode(text=text, stoi=stoi)\n",
        "    return tf.constant(tf.convert_to_tensor(value=__x, dtype=tf.dtypes.int32)), tf.constant(tf.one_hot(indices=__y, depth=depth, dtype=tf.dtypes.float32))"
      ],
      "metadata": {
        "id": "19eaKm_7DPMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAPPINGS ####################################################################\n",
        "\n",
        "VOCABULARY = capture(TEXT)\n",
        "N_VOCABULARY_DIM = len(VOCABULARY)\n",
        "\n",
        "MAPPINGS = mappings(vocabulary=VOCABULARY)\n",
        "\n",
        "_stoi = MAPPINGS['encode']\n",
        "_itos = MAPPINGS['decode']"
      ],
      "metadata": {
        "id": "nc3kZZmzFDUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPLIT #######################################################################\n",
        "\n",
        "N1 = int(0.8 * len(TEXT))\n",
        "N2 = int(0.9 * len(TEXT))\n",
        "\n",
        "X_TRAIN, Y_TRAIN = dataset(text=TEXT[:N1], stoi=_stoi, context=N_CONTEXT_DIM, depth=N_VOCABULARY_DIM)\n",
        "X_DEV, Y_DEV = dataset(text=TEXT[N1:N2], stoi=_stoi, context=N_CONTEXT_DIM, depth=N_VOCABULARY_DIM)\n",
        "X_TEST, Y_TEST = dataset(text=TEXT[N2:], stoi=_stoi, context=N_CONTEXT_DIM, depth=N_VOCABULARY_DIM)"
      ],
      "metadata": {
        "id": "pTZgGWe6FBy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blocks"
      ],
      "metadata": {
        "id": "S39n2JmXG6yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FEED FORWARD BLOCK ##########################################################\n",
        "\n",
        "class ResidualFeedForwardBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int,\n",
        "        normalization_epsilon: float=0.001,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(ResidualFeedForwardBlock, self).__init__(**kwargs)\n",
        "        self._normalization = tf.keras.layers.LayerNormalization(axis=-1, epsilon=normalization_epsilon, center=True, scale=True, beta_initializer='zeros', gamma_initializer='glorot_uniform', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None,  **kwargs)\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self._hidden = tf.keras.layers.Dense(units=self._hidden_dim, activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
        "        self._projection = None\n",
        "\n",
        "    def build(self, input_shape: tuple, **kwargs) -> None:\n",
        "        # create the projection layer to match the input shape\n",
        "        self._projection = tf.keras.layers.Dense(units=input_shape[-1], activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
        "        # no need to build the activation layer\n",
        "        self._normalization.build(input_shape=input_shape) # no weights\n",
        "        self._hidden.build(input_shape=input_shape) # (C, H)\n",
        "        self._projection.build(input_shape=list(input_shape)[:-1] + [self._hidden_dim]) # (H, C), called on (x * W_h) => shape (B, T, H)\n",
        "        # notify the model\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, **kwargs):\n",
        "        __dx = inputs # (B, T, C)\n",
        "        # normalize the features\n",
        "        __dx = self._normalization(__dx, **kwargs) # (B, T, C)\n",
        "        # expand inside the hidden layer\n",
        "        __dx = self._hidden(__dx, **kwargs) # (B, T, C) * (C, H) = (B, T, H)\n",
        "        # projection: match the input shape\n",
        "        __dx = self._projection(__dx, **kwargs) # (B, T, H) * (H, C) = (B, T, C)\n",
        "        # residual\n",
        "        return inputs + __dx # (B, T, C)\n",
        "\n",
        "# ATTENTION BLOCK #############################################################\n",
        "\n",
        "class ResidualSelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attention_head_dim: int,\n",
        "        attention_head_count: int=1,\n",
        "        normalization_epsilon: float=0.001,\n",
        "        dropout: float=0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(ResidualSelfAttentionBlock, self).__init__(**kwargs)\n",
        "        self._normalization = tf.keras.layers.LayerNormalization(axis=-1, epsilon=normalization_epsilon, center=True, scale=True, beta_initializer='zeros', gamma_initializer='glorot_uniform', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None,  **kwargs)\n",
        "        self._attention = tf.keras.layers.MultiHeadAttention(num_heads=attention_head_count, key_dim=attention_head_dim, value_dim=attention_head_dim, dropout=dropout, use_bias=True, output_shape=None, attention_axes=None, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
        "\n",
        "    def build(self, input_shape: tuple, **kwargs) -> None:\n",
        "        # build\n",
        "        self._normalization.build(input_shape=input_shape)\n",
        "        self._attention.build(input_shape=input_shape)\n",
        "        # notify the model\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, training: bool=True, **kwargs):\n",
        "        __dx = inputs # (B, T, C)\n",
        "        # normalize the features\n",
        "        __dx = self._normalization(__dx, **kwargs) # (B, T, C)\n",
        "        # self-attention\n",
        "        __dx = self._attention(key=__dx, query=__dx, value=__dx, return_attention_scores=False, training=training, use_causal_mask=True, **kwargs) # (B, T, H_d * H_c) = (B, T, C) use_causal_mask=True\n",
        "        # residual\n",
        "        return inputs + __dx # (B, T, C)\n",
        "\n",
        "# META BLOCK ##################################################################\n",
        "\n",
        "class ResidualSelfAttentionDecoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int,\n",
        "        attention_head_dim: int,\n",
        "        attention_head_count: int=1,\n",
        "        normalization_epsilon: float=0.001,\n",
        "        dropout: float=0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(ResidualSelfAttentionDecoderBlock, self).__init__(**kwargs)\n",
        "        self._feedforward = ResidualFeedForwardBlock(hidden_dim=hidden_dim, normalization_epsilon=normalization_epsilon)\n",
        "        self._attention = ResidualSelfAttentionBlock(attention_head_dim=attention_head_dim, attention_head_count=attention_head_count, normalization_epsilon=normalization_epsilon, dropout=dropout)\n",
        "\n",
        "    def build(self, input_shape: tuple, **kwargs) -> None:\n",
        "        self._feedforward.build(input_shape=input_shape)\n",
        "        self._attention.build(input_shape=input_shape)\n",
        "        # notify the model\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, training: bool=True, **kwargs):\n",
        "        __dx = inputs # (B, T, C)\n",
        "        # residual self-attention\n",
        "        __dx = self._attention(__dx, training=training, **kwargs) # (B, T, C)\n",
        "        # residual FF\n",
        "        __dx = self._feedforward(__dx, **kwargs) # (B, T, C)\n",
        "        # residual\n",
        "        return __dx # (B, T, C)"
      ],
      "metadata": {
        "id": "oEtSV9twG9Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "G2vkGes5EJSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL #######################################################################\n",
        "\n",
        "def create_model(\n",
        "    n_context_dim: int=N_CONTEXT_DIM,\n",
        "    n_vocabulary_dim: int=N_VOCABULARY_DIM,\n",
        "    n_embedding_dim: int=N_EMBEDDING_DIM,\n",
        "    n_hidden_dim: int=N_HIDDEN_DIM,\n",
        "    n_attention_block: int=N_ATTENTION_BLOCK,\n",
        "    n_attention_head: int=N_ATTENTION_HEAD,\n",
        "    n_attention_dim: int=N_ATTENTION_DIM,\n",
        "    lr_min: float=R_MIN\n",
        ") -> tf.keras.Model:\n",
        "    __model = tf.keras.Sequential()\n",
        "    # embedding\n",
        "    __model.add(tf.keras.layers.Embedding(input_dim=n_vocabulary_dim, output_dim=n_embedding_dim, embeddings_initializer='he_normal', name='embedding'))\n",
        "    # blocks\n",
        "    for __i in range(n_attention_block):\n",
        "        __model.add(ResidualSelfAttentionDecoderBlock(hidden_dim=n_hidden_dim, attention_head_dim=n_attention_dim, attention_head_count=n_attention_head, normalization_epsilon=0.001, dropout=0.0, name='decoder-block-' + str(__i)))\n",
        "    # head\n",
        "    __model.add(tf.keras.layers.Reshape(target_shape=(n_context_dim * n_embedding_dim,), input_shape=(n_context_dim, n_embedding_dim)))\n",
        "    __model.add(tf.keras.layers.Dense(units=n_vocabulary_dim, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', name='head'))\n",
        "    __model.add(tf.keras.layers.Softmax(axis=-1, name='softmax'))\n",
        "    # compile\n",
        "    __model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_min),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0., axis=-1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='loss'))\n",
        "    return __model"
      ],
      "metadata": {
        "id": "T0upuSDyELmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "  MODEL = create_model()\n",
        "MODEL.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEpY1-vFIFX7",
        "outputId": "2e6a65bf-714f-4c28-8568-9aab6c4b028a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 512)         35840     \n",
            "                                                                 \n",
            " decoder-block-0 (ResidualSe  (None, None, 512)        3152384   \n",
            " lfAttentionDecoderBlock)                                        \n",
            "                                                                 \n",
            " decoder-block-1 (ResidualSe  (None, None, 512)        3152384   \n",
            " lfAttentionDecoderBlock)                                        \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 131072)            0         \n",
            "                                                                 \n",
            " head (Dense)                (None, 70)                9175110   \n",
            "                                                                 \n",
            " softmax (Softmax)           (None, 70)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,515,718\n",
            "Trainable params: 15,515,718\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "8cheN52OEchs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LEARNING RATE ###############################################################\n",
        "\n",
        "def lrfn(epoch: int, lr_min: float, lr_max: float, lr_exp: float, rampup: int, sustain: int):\n",
        "  __lr = lr_min\n",
        "  if epoch < rampup:\n",
        "    __lr = lr_min + (epoch * (lr_max - lr_min) / rampup)\n",
        "  elif epoch < rampup + sustain:\n",
        "    __lr = lr_max\n",
        "  else:\n",
        "    __lr = lr_min + (lr_max - lr_min) * lr_exp ** (epoch - rampup - sustain)\n",
        "  return __lr\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch, lr_min=R_MIN, lr_max=R_MAX, lr_exp=R_EXP, rampup=N_EPOCHS_RAMPUP, sustain=N_EPOCHS_SUSTAIN), verbose=True)"
      ],
      "metadata": {
        "id": "kW7Yc5lfld3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN #######################################################################\n",
        "\n",
        "TRAINING_HISTORY = MODEL.fit(\n",
        "    x=X_TRAIN,\n",
        "    y=Y_TRAIN,\n",
        "    batch_size=N_BATCH,\n",
        "    epochs=N_EPOCHS,\n",
        "    validation_split=None,\n",
        "    validation_data=(X_DEV, Y_DEV),\n",
        "    validation_freq=[1, N_EPOCHS],\n",
        "    verbose=2,\n",
        "    callbacks=[lr_callback]) # callbacks=[CALLBACK]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beTpALmzFdu1",
        "outputId": "35e234e9-1197-495a-97f6-92fc1640c57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
            "Epoch 1/16\n",
            "2956/2956 - 118s - loss: 2.7572 - val_loss: 2.4572 - lr: 1.0000e-05 - 118s/epoch - 40ms/step\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0002075.\n",
            "Epoch 2/16\n",
            "2956/2956 - 80s - loss: 2.2043 - lr: 2.0750e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.00040500000000000003.\n",
            "Epoch 3/16\n",
            "2956/2956 - 80s - loss: 1.9937 - lr: 4.0500e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0006025000000000001.\n",
            "Epoch 4/16\n",
            "2956/2956 - 81s - loss: 1.8889 - lr: 6.0250e-04 - 81s/epoch - 28ms/step\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0008.\n",
            "Epoch 5/16\n",
            "2956/2956 - 80s - loss: 1.8319 - lr: 8.0000e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0006420000000000001.\n",
            "Epoch 6/16\n",
            "2956/2956 - 80s - loss: 1.6138 - lr: 6.4200e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0005156000000000002.\n",
            "Epoch 7/16\n",
            "2956/2956 - 80s - loss: 1.4617 - lr: 5.1560e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0004144800000000001.\n",
            "Epoch 8/16\n",
            "2956/2956 - 81s - loss: 1.3438 - lr: 4.1448e-04 - 81s/epoch - 27ms/step\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0003335840000000001.\n",
            "Epoch 9/16\n",
            "2956/2956 - 80s - loss: 1.2457 - lr: 3.3358e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0002688672000000001.\n",
            "Epoch 10/16\n",
            "2956/2956 - 80s - loss: 1.1607 - lr: 2.6887e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.00021709376000000007.\n",
            "Epoch 11/16\n",
            "2956/2956 - 80s - loss: 1.0849 - lr: 2.1709e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.00017567500800000005.\n",
            "Epoch 12/16\n",
            "2956/2956 - 81s - loss: 1.0182 - lr: 1.7568e-04 - 81s/epoch - 28ms/step\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.00014254000640000008.\n",
            "Epoch 13/16\n",
            "2956/2956 - 81s - loss: 0.9592 - lr: 1.4254e-04 - 81s/epoch - 28ms/step\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.00011603200512000005.\n",
            "Epoch 14/16\n",
            "2956/2956 - 80s - loss: 0.9087 - lr: 1.1603e-04 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 9.482560409600004e-05.\n",
            "Epoch 15/16\n",
            "2956/2956 - 80s - loss: 0.8663 - lr: 9.4826e-05 - 80s/epoch - 27ms/step\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 7.786048327680004e-05.\n",
            "Epoch 16/16\n",
            "2956/2956 - 86s - loss: 0.8295 - val_loss: 3.9762 - lr: 7.7860e-05 - 86s/epoch - 29ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _next(model: tf.Module, x: tf.Tensor) -> int:\n",
        "    __prob = tf.squeeze(model(x, training=False))\n",
        "    return tf.argmax(__prob, axis=-1).numpy()\n",
        "\n",
        "def sample(model: tf.Module, context: int, depth: int, length: int, itos: callable) -> str:\n",
        "    __start = int(random.uniform(0, depth))\n",
        "    __result = itos(__start)\n",
        "    __ngram = (context - 1) * [0,] + [__start]\n",
        "    __x = tf.convert_to_tensor(value=[__ngram], dtype=tf.dtypes.int32)\n",
        "    __n = _next(model=model, x=__x)\n",
        "    for __i in range(length):\n",
        "        __ngram = __ngram[1:] + [__n]\n",
        "        __x = tf.convert_to_tensor(value=[__ngram], dtype=tf.dtypes.int32)\n",
        "        __n = _next(model=model, x=__x)\n",
        "        __result += itos(__n)\n",
        "    return __result\n",
        "\n",
        "sample_2048 = functools.partial(sample, model=MODEL, context=N_CONTEXT_DIM, depth=N_VOCABULARY_DIM, length=8*N_SAMPLE, itos=_itos)"
      ],
      "metadata": {
        "id": "16a7CcvCF-xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_2048())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00maJbmdeeBi",
        "outputId": "e54c5130-5410-4500-cc04-9eb7e3a592cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D OThele thereethen thereen\n",
            "\n",
            "> I lo be the sereent the the sere the mant the the reat mere there sere me the the llatuth, the the munt thimund outh huth urd outhin thour, the hour hare sher shithe serene sweld sine chis sore, to thes lfonst to the teresbyoe we\n",
            "\n",
            "> ean  f with  loinghas   int ere lee seiteoeor ateyoo\n",
            "\n",
            ">e  aail lis oy ing ty ain aibles end al wirlath\n",
            "\n",
            "> in ham shis ghasd fait the oust on s mothd ane mfit,\n",
            "\n",
            "> To houed in lave thet aid fith he isseren,\n",
            "\n",
            "> chat bure ance poom med maghand oughers\n",
            "\n",
            "> And heam all ane to ghes whis fon the linge;\n",
            "\n",
            "> And line shill ar mare sin tor and mice for hionge\n",
            "\n",
            "> 's ponsust in ta mont willous anot mofersto,\n",
            "\n",
            "> Tha with th the wiak se tigh y alave so hepr\n",
            "\n",
            "> The kith mead and ous calf in with ars wime.\n",
            "\n",
            "> To he poo mare tore tour co fare he theere\n",
            "\n",
            "> And hith y or ghe riow then. I me not me dire,\n",
            "\n",
            "> 't on mencers's are lock of poo minteris.\n",
            "\n",
            "**OTHELLO**\n",
            "\n",
            "> I he bleand she therewis pow thain seme thenct\n",
            "\n",
            "> whot my thaunstersime tore he buthes fore tur llover es,\n",
            "\n",
            "> The not my mereat he where blo ders ffor\n",
            "\n",
            "> mant ge chave is nor whald dore fout heald.\n",
            "\n",
            "> Capeso in whar for ther aive mone stace the ke vinger.\n",
            "\n",
            "> *Emyer io hice and har your ow madde.\n",
            "\n",
            "> *Exit*\n",
            "\n",
            "> Stould I hea had cich mey ura doer.\n",
            "\n",
            "**\n",
            "\n",
            "> ANO, I pallond, tho' ha ghis alleds wis of harre\n",
            "\n",
            "> 't if his cansens andit wa dad seare ble;\n",
            "\n",
            "> Thak th peast of me tor at ind at mat\n",
            "\n",
            "> Thaug shank him satu nom know hous it theme with\n",
            "\n",
            "> the she loond fatcerour ol him your and sert,\n",
            "\n",
            "> To ghis thound and be pars oon them the\n",
            "\n",
            "> I and she ver.\n",
            "\n",
            "**IAGO**\n",
            "\n",
            "> I'd poof tou the withe's, wool thes indelle sis of ins, pien wish,\n",
            "\n",
            "> I wis blonke and yos ail thand his tis aud and.\n",
            "\n",
            "> OPrease the bithe thes iffon hous le demy,\n",
            "\n",
            "> Whit in thoul ay thes conens your the songe,\n",
            "\n",
            "> For the foom stame tor has if the fficl\n",
            "\n",
            "> And the ff ther sod dork ow thring shat sp,\n",
            "\n",
            "> Well de that ho blind hin spaypurca for your\n",
            "\n",
            "> Touthe verust not tor are. fure the vicy toug, ant agrond,\n",
            "\n",
            "> Why tha dingreas on herrull, her monke the mathe\n",
            "\n",
            "> For \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRMqKWSIjj2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}