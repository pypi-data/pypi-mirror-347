"use strict";(self.webpackChunk_jupyterlite_ai=self.webpackChunk_jupyterlite_ai||[]).push([[761,32,470,89],{16601:(e,t,a)=>{a.d(t,{L:()=>l});var n,r,s=a(44112),i=a(78145),o=a(59470);!function(e){e[e.system=0]="system",e[e.user=1]="user",e[e.assistant=2]="assistant"}(n||(n={})),function(e){e[e.user=0]="user",e[e.assistant=1]="assistant"}(r||(r={}));class l extends o.I{static lc_name(){return"ChromeAI"}constructor(e){super({...e}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"topK",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"systemPrompt",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.temperature=e?.temperature??this.temperature,this.topK=e?.topK??this.topK,this.systemPrompt=e?.systemPrompt}_llmType(){return"chrome_ai"}async createSession(){let e;try{e=ai}catch(e){throw new Error(`Could not initialize ChromeAI instance. Make sure you are running a version of Chrome with the proper experimental flags enabled.\n\nError message: ${e.message}`)}const{available:t}=await e.languageModel.capabilities();if("no"===t)throw new Error("The AI model is not available.");if("after-download"===t)throw new Error("The AI model is not yet downloaded.");return await e.languageModel.create({systemPrompt:this.systemPrompt,topK:this.topK,temperature:this.temperature})}async*_streamResponseChunks(e,t,a){let n;try{n=await this.createSession();const t=n.promptStreaming(e),r=i.bO.fromReadableStream(t);let o="";for await(const e of r){const t=e;o+=t,yield new s.mu({text:t}),await(a?.handleLLMNewToken(t))}}finally{n?.destroy()}}async _call(e,t,a){const n=[];for await(const r of this._streamResponseChunks(e,t,a))n.push(r.text);return n.join("")}}},59470:(e,t,a)=>{a.d(t,{P:()=>c,I:()=>u});var n=a(5779),r=a(4645),s=a(35183),i=a(67749),o=a(78145),l=a(65695);class c extends i.j_{constructor({concurrency:e,...t}){super(e?{maxConcurrency:e,...t}:t),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","llms",this._llmType()]})}async invoke(e,t){const a=c._convertInputToPromptValue(e);return(await this.generatePrompt([a],t,t?.callbacks)).generations[0][0].text}async*_streamResponseChunks(e,t,a){throw new Error("Not implemented.")}_separateRunnableConfigFromCallOptionsCompat(e){const[t,a]=super._separateRunnableConfigFromCallOptions(e);return a.signal=t.signal,[t,a]}async*_streamIterator(e,t){if(this._streamResponseChunks===c.prototype._streamResponseChunks)yield this.invoke(e,t);else{const a=c._convertInputToPromptValue(e),[n,i]=this._separateRunnableConfigFromCallOptionsCompat(t),o=await s.Td.configure(n.callbacks,this.callbacks,n.tags,this.tags,n.metadata,this.metadata,{verbose:this.verbose}),l={options:i,invocation_params:this?.invocationParams(i),batch_size:1},u=await(o?.handleLLMStart(this.toJSON(),[a.toString()],n.runId,void 0,l,void 0,void 0,n.runName));let h=new r.mu({text:""});try{for await(const e of this._streamResponseChunks(a.toString(),i,u?.[0]))h=h?h.concat(e):e,"string"==typeof e.text&&(yield e.text)}catch(e){throw await Promise.all((u??[]).map((t=>t?.handleLLMError(e)))),e}await Promise.all((u??[]).map((e=>e?.handleLLMEnd({generations:[[h]]}))))}}async generatePrompt(e,t,a){const n=e.map((e=>e.toString()));return this.generate(n,t,a)}invocationParams(e){return{}}_flattenLLMResult(e){const t=[];for(let a=0;a<e.generations.length;a+=1){const n=e.generations[a];if(0===a)t.push({generations:[n],llmOutput:e.llmOutput});else{const a=e.llmOutput?{...e.llmOutput,tokenUsage:{}}:void 0;t.push({generations:[n],llmOutput:a})}}return t}async _generateUncached(e,t,a,n){let i,u;if(void 0!==n&&n.length===e.length)i=n;else{const n=await s.Td.configure(a.callbacks,this.callbacks,a.tags,this.tags,a.metadata,this.metadata,{verbose:this.verbose}),r={options:t,invocation_params:this?.invocationParams(t),batch_size:e.length};i=await(n?.handleLLMStart(this.toJSON(),e,a.runId,void 0,r,void 0,void 0,a?.runName))}if(i?.[0].handlers.find(l.xL)&&1===e.length&&this._streamResponseChunks!==c.prototype._streamResponseChunks)try{const a=await this._streamResponseChunks(e[0],t,i?.[0]);let n;for await(const e of a)n=void 0===n?e:(0,o.xW)(n,e);if(void 0===n)throw new Error("Received empty response from chat model call.");u={generations:[[n]],llmOutput:{}},await(i?.[0].handleLLMEnd(u))}catch(e){throw await(i?.[0].handleLLMError(e)),e}else{try{u=await this._generate(e,t,i?.[0])}catch(e){throw await Promise.all((i??[]).map((t=>t?.handleLLMError(e)))),e}const a=this._flattenLLMResult(u);await Promise.all((i??[]).map(((e,t)=>e?.handleLLMEnd(a[t]))))}const h=i?.map((e=>e.runId))||void 0;return Object.defineProperty(u,r.SP,{value:h?{runIds:h}:void 0,configurable:!0}),u}async _generateCached({prompts:e,cache:t,llmStringKey:a,parsedOptions:n,handledOptions:i,runId:o}){const l=await s.Td.configure(i.callbacks,this.callbacks,i.tags,this.tags,i.metadata,this.metadata,{verbose:this.verbose}),c={options:n,invocation_params:this?.invocationParams(n),batch_size:e.length},u=await(l?.handleLLMStart(this.toJSON(),e,o,void 0,c,void 0,void 0,i?.runName)),h=[],m=(await Promise.allSettled(e.map((async(e,n)=>{const r=await t.lookup(e,a);return null==r&&h.push(n),r})))).map(((e,t)=>({result:e,runManager:u?.[t]}))).filter((({result:e})=>"fulfilled"===e.status&&null!=e.value||"rejected"===e.status)),p=[];await Promise.all(m.map((async({result:e,runManager:t},a)=>{if("fulfilled"===e.status){const n=e.value;return p[a]=n.map((e=>(e.generationInfo={...e.generationInfo,tokenUsage:{}},e))),n.length&&await(t?.handleLLMNewToken(n[0].text)),t?.handleLLMEnd({generations:[n]},void 0,void 0,void 0,{cached:!0})}return await(t?.handleLLMError(e.reason,void 0,void 0,void 0,{cached:!0})),Promise.reject(e.reason)})));const d={generations:p,missingPromptIndices:h,startedRunManagers:u};return Object.defineProperty(d,r.SP,{value:u?{runIds:u?.map((e=>e.runId))}:void 0,configurable:!0}),d}async generate(e,t,a){if(!Array.isArray(e))throw new Error("Argument 'prompts' is expected to be a string[]");let n;n=Array.isArray(t)?{stop:t}:t;const[r,s]=this._separateRunnableConfigFromCallOptionsCompat(n);if(r.callbacks=r.callbacks??a,!this.cache)return this._generateUncached(e,s,r);const{cache:i}=this,o=this._getSerializedCacheKeyParametersForCall(s),{generations:l,missingPromptIndices:c,startedRunManagers:u}=await this._generateCached({prompts:e,cache:i,llmStringKey:o,parsedOptions:s,handledOptions:r,runId:r.runId});let h={};if(c.length>0){const t=await this._generateUncached(c.map((t=>e[t])),s,r,void 0!==u?c.map((e=>u?.[e])):void 0);await Promise.all(t.generations.map((async(t,a)=>{const n=c[a];return l[n]=t,i.update(e[n],o,t)}))),h=t.llmOutput??{}}return{generations:l,llmOutput:h}}async call(e,t,a){const{generations:n}=await this.generate([e],t,a);return n[0][0].text}async predict(e,t,a){return this.call(e,t,a)}async predictMessages(e,t,a){const r=(0,n.Sw)(e),s=await this.call(r,t,a);return new n.Od(s)}_identifyingParams(){return{}}serialize(){return{...this._identifyingParams(),_type:this._llmType(),_model:this._modelType()}}_modelType(){return"base_llm"}}class u extends c{async _generate(e,t,a){return{generations:await Promise.all(e.map(((e,n)=>this._call(e,{...t,promptIndex:n},a).then((e=>[{text:e}])))))}}}},59257:(e,t,a)=>{var n=a(95709);t.A=void 0;var r=n(a(42032)),s=a(74848);t.A=(0,r.default)((0,s.jsx)("path",{d:"M6 6h12v12H6z"}),"Stop")},42032:(e,t,a)=>{Object.defineProperty(t,"__esModule",{value:!0}),Object.defineProperty(t,"default",{enumerable:!0,get:function(){return n.createSvgIcon}});var n=a(83704)}}]);