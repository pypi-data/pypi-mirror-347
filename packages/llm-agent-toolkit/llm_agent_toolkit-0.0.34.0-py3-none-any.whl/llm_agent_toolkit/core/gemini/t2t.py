import os
import logging
import json
import time
import asyncio
from typing import Any, Optional
from random import random
from copy import deepcopy
from concurrent.futures import ThreadPoolExecutor

from google import genai
from google.genai import types
from ..._core import Core
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from .base import GeminiCore

logger = logging.getLogger(__name__)


class T2T_GMN_Core(Core, GeminiCore):
    """
    `T2T_GMN_Core` is designed to provide a unified interface for interacting with the Gemini LLM model.

    It includes methods for running asynchronous and synchronous execution, and
    handling retries with progressive backoff in case of errors.

    Attributes:
        MAX_ATTEMPT (int): The maximum number of retry attempts for API calls.
        DELAY_FACTOR (float): The factor by which the delay increases after each retry.
        MAX_DELAY (float): The maximum delay between retries.

    **Methods**:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.

    **Notes**:
    - Only supports single-turn execution.
    - Input: Text only
    - Output: Text only
    """

    MAX_ATTEMPT: int = 5
    DELAY_FACTOR: float = 1.5
    MAX_DELAY: float = 60.0

    def __init__(self, system_prompt: str, config: ChatCompletionConfig):
        Core.__init__(self, system_prompt, config)
        GeminiCore.__init__(self, config.name)
        self.profile = self.build_profile(config.name)

    def custom_config(self, max_output_tokens: int) -> types.GenerateContentConfig:
        """Adapter function.

        Transform custom ChatCompletionConfig -> types.GenerationContentConfig
        """
        config = types.GenerateContentConfig(
            system_instruction=self.system_prompt,
            temperature=self.config.temperature,
            max_output_tokens=max_output_tokens,
        )
        return config

    def __update_delay(self, delay: float) -> float:
        new_delay = delay * self.DELAY_FACTOR
        # Add some randomness to allow bulk requests to retry at a slightly different timing
        new_delay += random() * 5.0
        return min(new_delay, self.MAX_DELAY)

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        **Notes**:
        * Single-turn execution.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0
        while attempt < T2T_GMN_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name, self.system_prompt, messages, imgs=None
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )
            config = self.custom_config(max_output_tokens)
            try:
                client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
                response = client.models.generate_content(
                    model=self.model_name,
                    contents=messages,  # type: ignore
                    config=config,
                )

                token_usage = self.update_usage(
                    response.usage_metadata, token_usage=None
                )

                candidates: Optional[list[types.Candidate]] = response.candidates
                if candidates is None or len(candidates) == 0:
                    raise RuntimeError(
                        f"Malformed response (No candidates): {response}"
                    )

                candidate = candidates[0]
                finish_reason = candidate.finish_reason
                content: Optional[types.Content] = candidate.content
                if content is None:
                    raise RuntimeError(f"Malformed response (No content): {candidate}")

                texts = self.get_texts(content)
                if finish_reason == types.FinishReason.STOP and texts:
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[types.Part.from_text(text=text) for text in texts],
                        )
                    )
                elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                    logger.warning("Terminated due to length.")
                    e = {
                        "error": "Early Termination: Length",
                        "text": "\n".join(texts),
                    }
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[
                                types.Part.from_text(
                                    text=json.dumps(e, ensure_ascii=False)
                                )
                            ],
                        )
                    )
                else:
                    logger.warning("Malformed response: %s", response)
                    logger.warning("Config: %s", self.config)
                    raise RuntimeError(f"Terminated: {finish_reason}")

                output = self.postprocessing(messages[-1:])
                return output, token_usage
            except RuntimeError:
                raise
            except Exception as e:
                error_object = e.__dict__
                error_code = error_object["code"]
                if error_code not in [429, 500, 503]:
                    raise

                error_message = error_object["message"]
                logger.warning(
                    "%s\n[%d] Retrying in %.2f seconds", error_message, attempt, delay
                )
                time.sleep(delay)
                attempt += 1
                delay = self.__update_delay(delay)
                continue

        raise RuntimeError("Max re-attempt reached")

    @staticmethod
    async def acall(
        model_name: str, config: types.GenerateContentConfig, msgs: list[types.Content]
    ):
        """Use this to make the `generate_content` method asynchronous."""
        client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                client.models.generate_content,
                model=model_name,
                contents=msgs,  # type: ignore
                config=config,
            )
            response = await asyncio.wrap_future(future)  # Makes the future awaitable
            return response

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        **Notes**:
        * Single-turn execution.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0
        while attempt < T2T_GMN_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name, self.system_prompt, messages, imgs=None
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )
            config = self.custom_config(max_output_tokens)
            try:
                response = await self.acall(self.model_name, config, msgs)
                token_usage = self.update_usage(
                    response.usage_metadata, token_usage=None
                )

                candidates: Optional[list[types.Candidate]] = response.candidates
                if candidates is None or len(candidates) == 0:
                    raise RuntimeError(
                        f"Malformed response (No candidates): {response}"
                    )

                candidate = candidates[0]
                finish_reason = candidate.finish_reason
                content: Optional[types.Content] = candidate.content
                if content is None:
                    raise RuntimeError(f"Malformed response (No content): {candidate}")

                texts = self.get_texts(content)
                if finish_reason == types.FinishReason.STOP and texts:
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[types.Part.from_text(text=text) for text in texts],
                        )
                    )
                elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                    logger.warning("Terminated due to length.")
                    e = {
                        "error": "Early Termination: Length",
                        "text": "\n".join(texts),
                    }
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[
                                types.Part.from_text(
                                    text=json.dumps(e, ensure_ascii=False)
                                )
                            ],
                        )
                    )
                else:
                    logger.warning("Malformed response: %s", response)
                    logger.warning("Config: %s", self.config)
                    raise RuntimeError(f"Terminated: {finish_reason}")

                output = self.postprocessing(messages[-1:])
                return output, token_usage
            except RuntimeError:
                raise
            except Exception as e:
                error_object = e.__dict__
                error_code = error_object["code"]
                if error_code not in [429, 500, 503]:
                    raise

                error_message = error_object["message"]
                logger.warning(
                    "%s\n[%d] Retrying in %.2f seconds", error_message, attempt, delay
                )
                await asyncio.sleep(delay)
                attempt += 1
                delay = self.__update_delay(delay)
                continue

        raise RuntimeError("Max re-attempt reached")
