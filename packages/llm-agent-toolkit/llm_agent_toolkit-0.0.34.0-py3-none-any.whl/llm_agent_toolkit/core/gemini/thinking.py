import os
import logging
import time
import asyncio
import json
from typing import Any, Optional
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy

from google import genai
from google.genai import types
from ..._core import Core
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from .base import GeminiCore

logger = logging.getLogger(__name__)


class Thinking_Core(Core, GeminiCore):
    """
    `Thinking_Core` is designed to provide a unified interface for interacting with the Gemini LLM model
    with "thinking" capability.

    It includes methods for running asynchronous and synchronous execution, and
    handling retries with progressive backoff in case of errors.

    Attributes:
        MAX_ATTEMPT (int): The maximum number of retry attempts for API calls.
        DELAY_FACTOR (float): The factor by which the delay increases after each retry.
        MAX_DELAY (float): The maximum delay between retries.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.

    Notes:
    - The *thinking* process is not in the output of generation.
    - https://ai.google.dev/gemini-api/docs/thinking
    - Input: Text only
    - Output: Text only
    """

    MAX_ATTEMPT: int = 5
    DELAY_FACTOR: float = 1.5
    MAX_DELAY: float = 60.0

    def __init__(self, system_prompt: str, config: ChatCompletionConfig):
        Core.__init__(self, system_prompt, config)
        GeminiCore.__init__(self, config.name)
        self.profile = self.build_profile(config.name)

    def custom_config(self, max_output_tokens: int) -> types.GenerateContentConfig:
        """Adapter function.

        Transform custom ChatCompletionConfig -> types.GenerationContentConfig
        """
        config = types.GenerateContentConfig(
            system_instruction=self.system_prompt,
            temperature=self.config.temperature,
            max_output_tokens=max_output_tokens,
        )
        # thinking_config=types.ThinkingConfig(include_thoughts=True)
        # thinkingConfig not yet supported (google-genai==1.0.0)
        return config

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        Notes:
        * Single-Turn Execution.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        attempt: int = 1
        delay: float = 5.0
        while attempt < Thinking_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name, self.system_prompt, messages, imgs=None
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )
            config = self.custom_config(max_output_tokens)
            try:
                client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
                response = client.models.generate_content(
                    model=self.model_name,
                    contents=messages,  # type: ignore
                    config=config,
                )

                token_usage = self.update_usage(
                    response.usage_metadata, token_usage=None
                )

                candidates: Optional[list[types.Candidate]] = response.candidates
                if candidates is None or len(candidates) == 0:
                    raise RuntimeError(
                        f"Malformed response (No candidates): {response}"
                    )

                candidate = candidates[0]
                finish_reason = candidate.finish_reason
                content: Optional[types.Content] = candidate.content
                if content is None:
                    raise RuntimeError(f"Malformed response (No content): {candidate}")

                texts = self.get_texts(content)
                if finish_reason == types.FinishReason.STOP and texts:
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[types.Part.from_text(text=text) for text in texts],
                        )
                    )
                elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                    logger.warning("Terminated due to length.")
                    e = {
                        "error": "Early Termination: Length",
                        "text": "\n".join(texts),
                    }
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[
                                types.Part.from_text(
                                    text=json.dumps(e, ensure_ascii=False)
                                )
                            ],
                        )
                    )
                else:
                    logger.warning("Malformed response: %s", response)
                    logger.warning("Config: %s", self.config)
                    raise RuntimeError(f"Terminated: {finish_reason}")

                output = self.postprocessing(messages[-1:])
                return output, token_usage
            except Exception as e:
                if "502 Bad Gateway" in str(e):
                    logger.warning("RateLimitError: %s", e)
                    warn_msg = f"[{attempt}] Retrying in {delay} seconds..."
                    logger.warning(warn_msg)
                    time.sleep(delay)
                    attempt += 1
                    delay = delay * Thinking_Core.DELAY_FACTOR
                    delay = min(Thinking_Core.MAX_DELAY, delay)
                    continue

                raise

        raise RuntimeError("Max re-attempt reached")

    @staticmethod
    async def acall(
        model_name: str, config: types.GenerateContentConfig, msgs: list[types.Content]
    ):
        """Use this to make the `generate_content` method asynchronous."""
        client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                client.models.generate_content,
                model=model_name,
                contents=msgs,  # type: ignore
                config=config,
            )
            response = await asyncio.wrap_future(future)  # Makes the future awaitable
            return response

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        Notes:
        * Single-Turn Execution.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0
        while attempt < Thinking_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name, self.system_prompt, messages, imgs=None
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )
            config = self.custom_config(max_output_tokens)
            try:
                response = await self.acall(self.model_name, config, msgs)
                token_usage = self.update_usage(
                    response.usage_metadata, token_usage=None
                )

                candidates: Optional[list[types.Candidate]] = response.candidates
                if candidates is None or len(candidates) == 0:
                    raise RuntimeError(
                        f"Malformed response (No candidates): {response}"
                    )

                candidate = candidates[0]
                finish_reason = candidate.finish_reason
                content: Optional[types.Content] = candidate.content
                if content is None:
                    raise RuntimeError(f"Malformed response (No content): {candidate}")

                texts = self.get_texts(content)
                if finish_reason == types.FinishReason.STOP and texts:
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[types.Part.from_text(text=text) for text in texts],
                        )
                    )
                elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                    logger.warning("Terminated due to length.")
                    e = {
                        "error": "Early Termination: Length",
                        "text": "\n".join(texts),
                    }
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[
                                types.Part.from_text(
                                    text=json.dumps(e, ensure_ascii=False)
                                )
                            ],
                        )
                    )
                else:
                    logger.warning("Malformed response: %s", response)
                    logger.warning("Config: %s", self.config)
                    raise RuntimeError(f"Terminated: {finish_reason}")

                output = self.postprocessing(messages[-1:])
                return output, token_usage
            except Exception as e:
                if "502 Bad Gateway" in str(e):
                    logger.warning("RateLimitError: %s", e)
                    warn_msg = f"[{attempt}] Retrying in {delay} seconds..."
                    logger.warning(warn_msg)
                    time.sleep(delay)
                    attempt += 1
                    delay = delay * Thinking_Core.DELAY_FACTOR
                    delay = min(Thinking_Core.MAX_DELAY, delay)
                    continue

                raise

        raise RuntimeError("Max re-attempt reached")
