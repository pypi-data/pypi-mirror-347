import os
import logging
import json
from typing import Any
import whisper

from ..base import AudioParameter, Transcriber, TranscriptionConfig, AudioHelper
from ..._util import MessageBlock, CreatorRole

logger = logging.getLogger(__name__)


class LocalWhisperTranscriber(Transcriber):
    """
    `LocalWhisperTranscriber` is a concrete implementation of the `Transcriber` abstract class.
    It facilitates synchronous and asynchronous communication with OpenAI's Whisper to transcribe audio files locally.

    Methods:
    - transcribe(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, tmp_directory: str, **kwargs) -> list[MessageBlock | dict]:
        Synchronously create transcript from the given audio file.
    - transcribe_async(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, tmp_directory: str, **kwargs) -> list[MessageBlock | dict]:
        Asynchronously create transcript from the given audio file.

    Notes:
    - Only accept audio file in OGG and MP3 format!!!
    - Large audio files will be split into multiple chunks, overlapping is supported.

    **response_format is json**:
    ```
    {
        "filename": {{filename}},
        "transcript": [
            {
                "page_index": {{page_index}},
                "segments": [
                    {
                        "start": {{start}},
                        "end": {{end}},
                        "text": {{text}}
                    }
                ]
            }
        ]
    }
    ```

    **response_format is text**:
    ```
    {
        "filename": {{filename}},
        "transcript": [
            {
                "page_index": {{page_index}},
                "text": {{text}}
            }
        ]
    }
    ```
    """

    def __init__(
        self,
        config: TranscriptionConfig,
        directory: str,
        audio_parameter: AudioParameter | None = None,
    ):
        # Parameters to guide the transcription
        # Only accept text | json
        if config.response_format == "verbose_json":
            config.response_format = "json"
        Transcriber.__init__(self, config)
        # Downloaded models will be stored here.
        self.__dir = directory
        # Parameter to guide the chunking process. None = Default.
        if audio_parameter is None:
            self.__audio_parameter = AudioParameter()
        else:
            self.__audio_parameter = audio_parameter
        if not self.__available():
            raise ValueError(
                "%s is not available in the model listing.", self.model_name
            )

    def __available(self) -> bool:
        try:
            if self.model_name not in whisper.available_models():
                return False
            whisper.load_model(
                name=self.model_name, download_root=self.directory, in_memory=False
            )
            return True
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise
        return False

    @property
    def model_name(self) -> str:
        return self.config.name

    @property
    def directory(self) -> str:
        """
        Downloaded models will be stored here.
        """
        return self.__dir

    async def transcribe_async(
        self, prompt: str, filepath: str, tmp_directory: str, **kwargs
    ) -> list[MessageBlock | dict[str, Any]]:
        """Calling the async function is meaningless because it's runniing on local CPU."""
        return self.transcribe(prompt, filepath, tmp_directory, **kwargs)

    def transcribe(
        self, prompt: str, filepath: str, tmp_directory: str, **kwargs
    ) -> list[MessageBlock | dict[str, Any]]:
        """Synchronously run the LLM model to create a transcript from the audio in `filepath`.

        Args:
            prompt (str): User's prompt
            filepath (str): Path of the audio file
            tmp_directory (str): Folder to temporary store audio chunks

        Returns:
            output (List[MessageBlock | dict]): The list of messages generated by the LLM model.
        """
        if filepath is None or tmp_directory is None:
            raise ValueError("filepath and tmp_directory are required")

        ext = os.path.splitext(filepath)[-1]
        params: dict = self.__audio_parameter.model_dump()
        params["output_format"] = ext[1:]
        try:
            model = whisper.load_model(
                name=self.model_name, download_root=self.directory, in_memory=True
            )
            chunks = AudioHelper.generate_chunks(
                input_path=filepath, tmp_directory=tmp_directory, **params
            )
            pages = []
            file_object: dict[str, str | list] = {
                "filename": os.path.basename(filepath)
            }
            for idx, chunk_path in enumerate(chunks, start=1):
                result: dict = model.transcribe(
                    audio=chunk_path,
                    temperature=self.config.temperature,
                    word_timestamps=False,
                    condition_on_previous_text=False,
                    initial_prompt=prompt,
                )
                page: dict[str, str | int | list] = {"page_index": idx}

                if self.config.response_format == "json":
                    segments = result["segments"]
                    minimal_segments = []
                    for segment in segments:
                        minimal_segments.append(
                            {
                                "start": round(segment["start"], 2),
                                "end": round(segment["end"], 2),
                                "text": segment["text"].strip(),
                            }
                        )
                    page["segments"] = minimal_segments
                else:  # if self.config.response_format == "text":
                    transcript: str = result["text"]
                    page["text"] = transcript.strip()
                pages.append(page)
            file_object["transcript"] = pages
            output_string = json.dumps(file_object, ensure_ascii=False)
            return [{"role": CreatorRole.ASSISTANT.value, "content": output_string}]
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise
