import os
import io
import logging
import json
from typing import Any
import openai

from ..base import AudioParameter, Transcriber, TranscriptionConfig, AudioHelper
from ..._util import MessageBlock, CreatorRole

logger = logging.getLogger(__name__)


class OpenAITranscriber(Transcriber):
    """
    `OpenAITranscriber` is a concrete implementation of the `Transcriber` abstract class.
    It facilitates synchronous and asynchronous communication with OpenAI's API to transcribe audio files.

    Methods:
    - transcribe(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, tmp_directory: str, **kwargs) -> list[MessageBlock | dict]:
        Synchronously create transcript from the given audio file.
    - transcribe_async(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, tmp_directory: str, **kwargs) -> list[MessageBlock | dict]:
        Asynchronously create transcript from the given audio file.

    Notes:
    - Only accept audio file in OGG and MP3 format!!!
    - Large audio files will be split into multiple chunks, overlapping is supported.

    **response_format is json**:
    ```
    {
        "filename": {{filename}},
        "transcript": [
            {
                "page_index": {{page_index}},
                "segments": [
                    {
                        "start": {{start}},
                        "end": {{end}},
                        "text": {{text}}
                    }
                ]
            }
        ]
    }
    ```

    **response_format is text**:
    ```
    {
        "filename": {{filename}},
        "transcript": [
            {
                "page_index": {{page_index}},
                "text": {{text}}
            }
        ]
    }
    ```
    """

    def __init__(
        self,
        config: TranscriptionConfig,
        audio_parameter: AudioParameter | None = None,
    ):
        # response_format as `json` yield same outcome like `text`
        if config.response_format == "json":
            # force it to `verbose_json`
            config.response_format = "verbose_json"
        Transcriber.__init__(self, config)
        # Parameter to guide the chunking process. None = Default.
        if audio_parameter is None:
            self.__audio_parameter = AudioParameter()
        else:
            self.__audio_parameter = audio_parameter
        if not self.__available():
            raise ValueError("%s is not available in OpenAI's model listing.")

    def __available(self) -> bool:
        """
        This is not the real fix, I basically pass the responsibility back to the user
        to pick the available models.

        Always return True!!!

        If `client.models.list()` continue to fail,
        it will show warning without raising the Exception.
        """
        try:
            client = openai.Client(api_key=os.environ["OPENAI_API_KEY"])
            for model in client.models.list():
                if self.model_name == model.id:
                    return True
            return False
        except Exception as e:
            logger.warning("Exception: %s", e)
        return True

    @property
    def model_name(self) -> str:
        return self.config.name

    async def transcribe_async(
        self, prompt: str, filepath: str, tmp_directory: str, **kwargs
    ) -> list[MessageBlock | dict[str, Any]]:
        """Asynchronously run the LLM model to create a transcript from the audio in `filepath`.

        Args:
            prompt (str): User's prompt
            filepath (str): Path of the audio file
            tmp_directory (str): Folder to temporary store audio chunks

        Returns:
            output (List[MessageBlock | dict]): The list of messages generated by the LLM model.
        """
        if filepath is None or tmp_directory is None:
            raise ValueError("filepath and tmp_directory are required")

        ext = os.path.splitext(filepath)[-1]
        params = self.config.__dict__
        params["model"] = self.model_name
        for kw in ["name", "return_n", "max_iteration"]:
            del params[kw]

        chunk_params: dict = self.__audio_parameter.model_dump()
        chunk_params["output_format"] = ext[1:]
        try:
            chunks = AudioHelper.generate_chunks(
                input_path=filepath, tmp_directory=tmp_directory, **chunk_params
            )
            client = openai.AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
            pages = []
            file_object: dict[str, str | list] = {
                "filename": os.path.basename(filepath)
            }
            for idx, chunk_path in enumerate(chunks, start=1):
                with open(chunk_path, "rb") as f:
                    audio_data = f.read()
                    buffer = io.BytesIO(audio_data)
                    buffer.name = filepath
                    buffer.seek(0)

                params["file"] = buffer
                params["prompt"] = f"PROMPT={prompt}\nPage={idx}"
                transcript = await client.audio.transcriptions.create(**params)

                page: dict[str, str | int | list] = {"page_index": idx}
                if self.config.response_format == "verbose_json":
                    segments = transcript.segments
                    minimal_segments = []
                    for segment in segments:
                        minimal_segments.append(
                            {
                                "start": segment.start,
                                "end": segment.end,
                                "text": segment.text,
                            }
                        )
                    page["segments"] = minimal_segments
                else:  # text
                    page["text"] = transcript
                pages.append(page)
            file_object["transcript"] = pages
            output_string = json.dumps(file_object, ensure_ascii=False)
            return [{"role": CreatorRole.ASSISTANT.value, "content": output_string}]
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    def transcribe(
        self, prompt: str, filepath: str, tmp_directory: str, **kwargs
    ) -> list[MessageBlock | dict[str, Any]]:
        """Synchronously run the LLM model to create a transcript from the audio in `filepath`.

        Args:
            prompt (str): User's prompt
            filepath (str): Path of the audio file
            tmp_directory (str): Folder to temporary store audio chunks

        Returns:
            output (List[MessageBlock | dict]): The list of messages generated by the LLM model.
        """
        if filepath is None or tmp_directory is None:
            raise ValueError("filepath and tmp_directory are required")

        ext = os.path.splitext(filepath)[-1]
        params = self.config.__dict__
        params["model"] = self.model_name
        for kw in ["name", "return_n", "max_iteration"]:
            del params[kw]
        chunk_params: dict = self.__audio_parameter.model_dump()
        chunk_params["output_format"] = ext[1:]
        try:
            chunks = AudioHelper.generate_chunks(
                input_path=filepath, tmp_directory=tmp_directory, **chunk_params
            )
            client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
            pages = []
            file_object: dict[str, str | list] = {
                "filename": os.path.basename(filepath)
            }
            for idx, chunk_path in enumerate(chunks, start=1):
                with open(chunk_path, "rb") as f:
                    audio_data = f.read()
                    buffer = io.BytesIO(audio_data)
                    buffer.name = filepath
                    buffer.seek(0)

                params["file"] = buffer
                params["prompt"] = f"PROMPT={prompt}\nPage={idx}"
                transcript = client.audio.transcriptions.create(**params)
                page: dict[str, str | int | list] = {"page_index": idx}
                if self.config.response_format == "verbose_json":
                    segments = transcript.segments
                    minimal_segments = []
                    for segment in segments:
                        minimal_segments.append(
                            {
                                "start": segment.start,
                                "end": segment.end,
                                "text": segment.text,
                            }
                        )
                    page["segments"] = minimal_segments
                else:  # text and json yield the same structure
                    page["text"] = transcript
                pages.append(page)
            file_object["transcript"] = pages
            output_string = json.dumps(file_object, ensure_ascii=False)
            return [{"role": CreatorRole.ASSISTANT.value, "content": output_string}]
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise
