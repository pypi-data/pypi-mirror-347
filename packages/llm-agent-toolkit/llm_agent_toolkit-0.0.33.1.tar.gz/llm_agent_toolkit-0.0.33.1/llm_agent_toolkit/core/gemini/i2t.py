import os
import logging
import json
import time
import asyncio
from typing import Optional
from random import random
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy

from google import genai
from google.genai import types
from ..._core import Core, ImageInterpreter
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from .base import GeminiCore

logger = logging.getLogger(__name__)


class I2T_GMN_Core(Core, GeminiCore, ImageInterpreter):
    """
    `T2T_GMN_Core` is a multimodal interface with image interpretation support.

    It provides a unified interface to work with Gemini LLM model.

    It includes methods for running asynchronous and synchronous execution, and
    handling retries with progressive backoff in case of errors.

    Attributes:
        SUPPORTED_IMAGE_FORMATS (tuple[str]): A tuple of supported image formats.
        MAX_ATTEMPT (int): The maximum number of retry attempts for API calls.
        DELAY_FACTOR (float): The factor by which the delay increases after each retry.
        MAX_DELAY (float): The maximum delay between retries.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.
    - interpret(query: str, context: list[MessageBlock | dict] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously interpret the given image.
    - interpret_async(query: str, context: list[MessageBlock | dict] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously interpret the given image.

    **Notes**:
    - Only supports single-turn execution.
    - Input: Text & Image only
    - Output: Text only
    """

    SUPPORTED_IMAGE_FORMATS = (".png", ".jpeg", ".jpg", ".webp")
    MAX_ATTEMPT: int = 5
    DELAY_FACTOR: float = 1.5
    MAX_DELAY: float = 60.0

    def __init__(self, system_prompt: str, config: ChatCompletionConfig):
        Core.__init__(self, system_prompt, config)
        GeminiCore.__init__(self, config.name)
        self.profile = self.build_profile(config.name)

    def custom_config(self, max_output_tokens: int) -> types.GenerateContentConfig:
        """Adapter function.

        Transform custom ChatCompletionConfig -> types.GenerationContentConfig
        """
        config = types.GenerateContentConfig(
            system_instruction=self.system_prompt,
            temperature=self.config.temperature,
            max_output_tokens=max_output_tokens,
        )
        return config

    def __update_delay(self, delay: float) -> float:
        new_delay = delay * self.DELAY_FACTOR
        # Add some randomness to allow bulk requests to retry at a slightly different timing
        new_delay += random() * 5.0
        return min(new_delay, self.MAX_DELAY)

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        """
        Synchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.
                * `filepath` (str | None): Path to the image file.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        **Notes**:
        * Single-turn execution.
        """
        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            ext = os.path.splitext(filepath)[-1]
            if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
                raise ValueError(f"Unsupported image type: {ext}")

        msgs: list[types.Content] = self.preprocessing(query, context, filepath)

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        attempt: int = 1
        delay: float = 5.0
        while attempt < I2T_GMN_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name,
                self.system_prompt,
                messages,
                imgs=None if filepath is None else [filepath],
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )

            config = self.custom_config(max_output_tokens)
            try:
                client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
                response = client.models.generate_content(
                    model=self.model_name,
                    contents=messages,  # type: ignore
                    config=config,
                )

                token_usage = self.update_usage(
                    response.usage_metadata, token_usage=None
                )

                candidates: Optional[list[types.Candidate]] = response.candidates
                if candidates is None or len(candidates) == 0:
                    raise RuntimeError(
                        f"Malformed response (No candidates): {response}"
                    )

                candidate = candidates[0]
                finish_reason = candidate.finish_reason
                content: Optional[types.Content] = candidate.content
                if content is None:
                    raise RuntimeError(f"Malformed response (No content): {candidate}")

                texts = self.get_texts(content)
                if finish_reason == types.FinishReason.STOP and texts:
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[types.Part.from_text(text=text) for text in texts],
                        )
                    )
                elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                    logger.warning("Terminated due to length.")
                    e = {
                        "error": "Early Termination: Length",
                        "text": "\n".join(texts),
                    }
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[
                                types.Part.from_text(
                                    text=json.dumps(e, ensure_ascii=False)
                                )
                            ],
                        )
                    )
                else:
                    logger.warning("Malformed response: %s", response)
                    logger.warning("Config: %s", self.config)
                    raise RuntimeError(f"Terminated: {finish_reason}")

                output = self.postprocessing(messages[-1:])
                return output, token_usage
            except RuntimeError:
                raise
            except Exception as e:
                error_object = e.__dict__
                error_code = error_object["code"]
                if error_code not in [429, 500, 503]:
                    raise

                error_message = error_object["message"]
                logger.warning(
                    "%s\n[%d] Retrying in %.2f seconds", error_message, attempt, delay
                )
                time.sleep(delay)
                attempt += 1
                delay = self.__update_delay(delay)
                continue

        raise RuntimeError("Max re-attempt reached")

    @staticmethod
    async def acall(
        model_name: str, config: types.GenerateContentConfig, msgs: list[types.Content]
    ):
        """Use this to make the `generate_content` method asynchronous."""
        client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                client.models.generate_content,
                model=model_name,
                contents=msgs,  # type: ignore
                config=config,
            )
            response = await asyncio.wrap_future(future)  # Makes the future awaitable
            return response

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.
                * `filepath` (str | None): Path to the image file.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        **Notes**:
        * Single-turn execution.
        """
        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            ext = os.path.splitext(filepath)[-1]
            if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
                raise ValueError(f"Unsupported image type: {ext}")

        msgs: list[types.Content] = self.preprocessing(query, context, filepath)

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )
        attempt: int = 1
        delay: float = 5.0
        while attempt < I2T_GMN_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name,
                self.system_prompt,
                messages,
                imgs=None if filepath is None else [filepath],
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )

            config = self.custom_config(max_output_tokens)
            try:
                response = await self.acall(self.model_name, config, messages)
                token_usage = self.update_usage(
                    response.usage_metadata, token_usage=None
                )

                candidates: Optional[list[types.Candidate]] = response.candidates
                if candidates is None or len(candidates) == 0:
                    raise RuntimeError(
                        f"Malformed response (No candidates): {response}"
                    )

                candidate = candidates[0]
                finish_reason = candidate.finish_reason
                content: Optional[types.Content] = candidate.content
                if content is None:
                    raise RuntimeError(f"Malformed response (No content): {candidate}")

                texts = self.get_texts(content)
                if finish_reason == types.FinishReason.STOP and texts:
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[types.Part.from_text(text=text) for text in texts],
                        )
                    )
                elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                    logger.warning("Terminated due to length.")
                    e = {
                        "error": "Early Termination: Length",
                        "text": "\n".join(texts),
                    }
                    messages.append(
                        types.Content(
                            role=CreatorRole.MODEL.value,
                            parts=[
                                types.Part.from_text(
                                    text=json.dumps(e, ensure_ascii=False)
                                )
                            ],
                        )
                    )
                else:
                    logger.warning("Malformed response: %s", response)
                    logger.warning("Config: %s", self.config)
                    raise RuntimeError(f"Terminated: {finish_reason}")

                output = self.postprocessing(messages[-1:])
                return output, token_usage
            except RuntimeError:
                raise
            except Exception as e:
                error_object = e.__dict__
                error_code = error_object["code"]
                if error_code not in [429, 500, 503]:
                    raise

                error_message = error_object["message"]
                logger.warning(
                    "%s\n[%d] Retrying in %.2f seconds", error_message, attempt, delay
                )
                await asyncio.sleep(delay)
                attempt += 1
                delay = self.__update_delay(delay)
                continue

        raise RuntimeError("Max re-attempt reached")

    def interpret(
        self,
        query: str,
        context: list[MessageBlock | dict] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        ext = os.path.splitext(filepath)[-1]
        if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
            raise ValueError(f"Unsupported image type: {ext}")

        return self.run(query=query, context=context, filepath=filepath, **kwargs)

    async def interpret_async(
        self,
        query: str,
        context: list[MessageBlock | dict] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        ext = os.path.splitext(filepath)[-1]
        if ext not in I2T_GMN_Core.SUPPORTED_IMAGE_FORMATS:
            raise ValueError(f"Unsupported image type: {ext}")

        return await self.run_async(
            query=query, context=context, filepath=filepath, **kwargs
        )
