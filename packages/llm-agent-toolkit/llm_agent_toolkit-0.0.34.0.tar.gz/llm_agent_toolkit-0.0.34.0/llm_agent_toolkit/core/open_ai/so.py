import os
import json
import logging
import time
import asyncio
from typing import Any, Optional, Type, TypeVar
from copy import deepcopy
import base64

# External Packages
import openai
from openai import RateLimitError
from pydantic import BaseModel

# Self Defined Packages
from ..._util import (
    CreatorRole,
    ChatCompletionConfig,
    MessageBlock,
    ResponseMode,
    TokenUsage,
)
from ..._core import Core, ImageInterpreter
from .base import OpenAICore

T = TypeVar("T", bound=BaseModel)
logger = logging.getLogger(__name__)


class OAI_StructuredOutput_Core(Core, OpenAICore, ImageInterpreter):
    """
    `OAI_StructuredOutput_Core` is a multimodel LLM model that provide image interpretation
    and structured output support.

    It is designed to work with OpenAI's API and supports various image formats.

    It includes methods for running asynchronous and synchronous execution, and
    handling retries with progressive backoff in case of errors.

    Attributes:
        SUPPORTED_IMAGE_FORMATS (tuple[str]): A tuple of supported image formats.
        MAX_ATTEMPT (int): The maximum number of retry attempts for API calls.
        DELAY_FACTOR (float): The factor by which the delay increases after each retry.
        MAX_DELAY (float): The maximum delay between retries.

    **Methods**:
        run(query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
            Synchronously run the LLM model with the given query and context.
        run_async(query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
            Asynchronously run the LLM model with the given query and context.
        interpret(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
            Synchronously interpret the given image.
        interpret_async(query: str, context: list[MessageBlock | dict[str, Any]] | None, filepath: str, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
            Asynchronously interpret the given image.

    **Notes**:
    - Backend URL: https://api.openai.com
    - Best suited for chaining operations where structured data flow is essential.
    - https://platform.openai.com/docs/guides/structured-outputs
    """

    SUPPORTED_IMAGE_FORMATS = (".png", ".jpeg", ".jpg", ".gif", ".webp")
    MAX_ATTEMPT: int = 5
    DELAY_FACTOR: float = 1.5
    MAX_DELAY: float = 60.0

    def __init__(
        self,
        system_prompt: str,
        config: ChatCompletionConfig,
    ):
        Core.__init__(self, system_prompt, config)
        OpenAICore.__init__(self, config.name)
        self.profile = self.build_profile(model_name=config.name)

    def validate(
        self, response_mode: Optional[ResponseMode], response_format: Optional[Type[T]]
    ) -> None:
        if response_mode:
            if not isinstance(response_mode, ResponseMode):
                raise TypeError(
                    f"Expect mode to be an instance of 'ResponseMode', but got '{type(response_mode).__name__}'."
                )
            if response_mode is response_mode.SO:
                if response_format is None:
                    raise TypeError(
                        "Expect format to be a subclass of 'BaseModel', but got 'NoneType'."
                    )
                if not issubclass(response_format, BaseModel):
                    raise TypeError(
                        f"Expect format to be a subclass of 'BaseModel', but got '{type(response_format).__name__}'."
                    )

    def run(
        self, query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously generate text based on the given query and context.

        Support image interpretation and structured output.

        Args:
            query (str): The query to generate text for.
            context (list): A list of context messages or dictionaries.
            **kwargs: Additional keyword arguments:
                * `filepath` (str | None): Path to the image file.
                * `mode` (ResponseMode | None): Ouput mode. [ResponseMode.DEFAULT, ResponseMode.JSON, ResponseMode.SO]
                * `format` (BaseModel | None): Output structure. Required when `mode` is `ResponseMode.SO`.

        Returns:
            response_tuple (tuple[list[MessageBlock|dict], TokenUsage]):
                * output: The list of messages generated by the LLM model. Only 1 element in the list, it's content can be decoded by `json.loads()`.
                * usage: The recorded token usage.

        Raises:
            TypeError:
                * `mode` is not type `ResponseMode`.
                * `format` is not a subclass of `BaseModel` when `mode` is `ResponseMode.SO`.
            FileNotFoundError:
                * If `filepath` is found.
            ValueError:
                * If max_output_tokens <= 0. Theres no capacity left for text-generation.
                * If `filepath` is not a supported image format.

        **Rate Limit Error Handling**:
        - If a RateLimitError occurs, the function will retry the request with an increasing delay.
        - The delay will be multiplied by a factor (DELAY_FACTOR) after each attempt, up to a maximum delay (MAX_DELAY).
        """
        response_mode: Optional[ResponseMode] = kwargs.get("mode", ResponseMode.DEFAULT)
        response_format: Optional[Type[T]] = kwargs.get("format")  # type: ignore
        self.validate(response_mode, response_format)  # Raise an exception if invalid

        msgs: list[MessageBlock | dict[str, Any]] = [
            {"role": CreatorRole.SYSTEM.value, "content": self.system_prompt}
        ]

        if context:
            msgs.extend(context)

        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            # detail hardcode as "high"
            resized, newpath = self.resize(filepath, "high")
            if resized and newpath:
                img_url = self.get_image_url(newpath)
                os.remove(newpath)
            else:
                img_url = self.get_image_url(filepath)

            msgs.append(
                {
                    "role": CreatorRole.USER.value,
                    "content": [
                        {"type": "text", "text": query},
                        {
                            "type": "image_url",
                            "image_url": {"url": img_url, "detail": "high"},
                        },
                    ],  # type: ignore
                }
            )
        else:
            msgs.append(MessageBlock(role=CreatorRole.USER.value, content=query))

        # Determine the maximum number of tokens allowed for the response
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0

        while attempt < OAI_StructuredOutput_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                msgs,
                None,
                images=[filepath] if filepath else None,
                image_detail="high" if filepath else None,
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )

            if max_output_tokens <= 0:
                raise ValueError(
                    f"max_output_tokens <= 0. Prompt token count: {prompt_token_count}"
                )

            try:
                client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
                if response_mode is ResponseMode.SO and response_format:
                    response = client.beta.chat.completions.parse(
                        model=self.model_name,
                        messages=messages,  # type: ignore
                        frequency_penalty=0.5,
                        max_tokens=max_output_tokens,
                        temperature=self.config.temperature,
                        n=self.config.return_n,
                        response_format=response_format,  # type: ignore
                    )
                elif response_mode is ResponseMode.JSON:
                    response = client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,  # type: ignore
                        frequency_penalty=0.5,
                        max_tokens=max_output_tokens,
                        temperature=self.config.temperature,
                        n=self.config.return_n,
                        response_format={"type": "json_object"},  # type: ignore
                    )
                else:
                    # response_mode is ResponseMode.DEFAULT
                    response = client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,  # type: ignore
                        frequency_penalty=0.5,
                        max_tokens=max_output_tokens,
                        temperature=self.config.temperature,
                        n=self.config.return_n,
                    )

                token_usage = self.update_usage(response.usage)

                choice = response.choices[0]
                finish_reason = choice.finish_reason
                content = choice.message.content
                if finish_reason == "stop" and content:
                    if response_mode is not ResponseMode.DEFAULT:
                        try:
                            _ = json.loads(content)
                            output_string = content
                        except json.JSONDecodeError as decode_error:
                            e = {"error": str(decode_error), "text": content}
                            output_string = json.dumps(e)
                    else:
                        output_string = content

                    output: list[dict | MessageBlock] = [
                        MessageBlock(
                            role=CreatorRole.ASSISTANT.value, content=output_string
                        )
                    ]
                    return output, token_usage

                if finish_reason == "length" and content:
                    e = {"error": "Early Termination: Length", "text": content}
                    output: list[dict | MessageBlock] = [
                        MessageBlock(
                            role=CreatorRole.ASSISTANT.value,
                            content=json.dumps(e, ensure_ascii=False),
                        )
                    ]
                    return output, token_usage

                logger.warning("Malformed response: %s", response)
                logger.warning("Config: %s", self.config)
                raise RuntimeError(f"Terminated: {finish_reason}")
            except RateLimitError as rle:
                logger.warning("RateLimitError: %s", rle)
                warn_msg = f"[{attempt}] Retrying in {delay} seconds..."
                logger.warning(warn_msg)
                time.sleep(delay)
                attempt += 1
                delay = delay * OAI_StructuredOutput_Core.DELAY_FACTOR
                delay = min(OAI_StructuredOutput_Core.MAX_DELAY, delay)
                continue
            except Exception as e:
                logger.error("Exception: %s", e, exc_info=True, stack_info=True)
                raise

        raise RuntimeError("Max re-attempt reached")

    async def run_async(
        self, query: str, context: list[MessageBlock | dict[str, Any]] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously generate text based on the given query and context.

        Support image interpretation and structured output.

        Args:
            query (str): The query to generate text for.
            context (list): A list of context messages or dictionaries.
            **kwargs: Additional keyword arguments:
                * `filepath` (str | None): Path to the image file.
                * `mode` (ResponseMode | None): Ouput mode. [ResponseMode.DEFAULT, ResponseMode.JSON, ResponseMode.SO]
                * `format` (BaseModel | None): Output structure. Required when `mode` is `ResponseMode.SO`.

        Returns:
            response_tuple (tuple[list[MessageBlock|dict], TokenUsage]):
                * output: The list of messages generated by the LLM model. Only 1 element in the list, it's content can be decoded by `json.loads()`.
                * usage: The recorded token usage.

        Raises:
            TypeError:
                * `mode` is not type `ResponseMode`.
                * `format` is not a subclass of `BaseModel` when `mode` is `ResponseMode.SO`.
            FileNotFoundError:
                * If `filepath` is found.
            ValueError:
                * If max_output_tokens <= 0. Theres no capacity left for text-generation.
                * If `filepath` is not a supported image format.

        **Rate Limit Error Handling**:
        - If a RateLimitError occurs, the function will retry the request with an increasing delay.
        - The delay will be multiplied by a factor (DELAY_FACTOR) after each attempt, up to a maximum delay (MAX_DELAY).
        """
        response_mode: Optional[ResponseMode] = kwargs.get("mode", ResponseMode.DEFAULT)
        response_format: Optional[Type[T]] = kwargs.get("format")  # type: ignore
        self.validate(response_mode, response_format)  # Raise an exception if invalid

        msgs: list[MessageBlock | dict[str, Any]] = [
            {"role": CreatorRole.SYSTEM.value, "content": self.system_prompt}
        ]

        if context:
            msgs.extend(context)

        filepath: str | None = kwargs.get("filepath", None)
        if filepath:
            # detail hardcode as "high"
            resized, newpath = self.resize(filepath, "high")
            if resized and newpath:
                img_url = self.get_image_url(newpath)
                os.remove(newpath)
            else:
                img_url = self.get_image_url(filepath)

            msgs.append(
                {
                    "role": CreatorRole.USER.value,
                    "content": [
                        {"type": "text", "text": query},
                        {
                            "type": "image_url",
                            "image_url": {"url": img_url, "detail": "high"},
                        },
                    ],  # type: ignore
                }
            )
        else:
            msgs.append(MessageBlock(role=CreatorRole.USER.value, content=query))

        # Determine the maximum number of tokens allowed for the response
        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0

        while attempt < OAI_StructuredOutput_Core.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                msgs,
                None,
                images=[filepath] if filepath else None,
                image_detail="high" if filepath else None,
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )

            if max_output_tokens <= 0:
                raise ValueError(
                    f"max_output_tokens <= 0. Prompt token count: {prompt_token_count}"
                )

            try:
                client = openai.AsyncOpenAI(api_key=os.environ["OPENAI_API_KEY"])
                if response_mode is ResponseMode.SO and response_format:
                    response = await client.beta.chat.completions.parse(
                        model=self.model_name,
                        messages=messages,  # type: ignore
                        frequency_penalty=0.5,
                        max_tokens=max_output_tokens,
                        temperature=self.config.temperature,
                        n=self.config.return_n,
                        response_format=response_format,  # type: ignore
                    )
                elif response_mode is ResponseMode.JSON:
                    response = await client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,  # type: ignore
                        frequency_penalty=0.5,
                        max_tokens=max_output_tokens,
                        temperature=self.config.temperature,
                        n=self.config.return_n,
                        response_format={"type": "json_object"},  # type: ignore
                    )
                else:
                    # response_mode is ResponseMode.DEFAULT
                    response = await client.chat.completions.create(
                        model=self.model_name,
                        messages=messages,  # type: ignore
                        frequency_penalty=0.5,
                        max_tokens=max_output_tokens,
                        temperature=self.config.temperature,
                        n=self.config.return_n,
                    )

                token_usage = self.update_usage(response.usage)

                choice = response.choices[0]
                finish_reason = choice.finish_reason
                content = choice.message.content
                if finish_reason == "stop" and content:
                    if response_mode is not ResponseMode.DEFAULT:
                        try:
                            _ = json.loads(content)
                            output_string = content
                        except json.JSONDecodeError as decode_error:
                            e = {"error": str(decode_error), "text": content}
                            output_string = json.dumps(e)
                    else:
                        output_string = content

                    output: list[dict | MessageBlock] = [
                        MessageBlock(
                            role=CreatorRole.ASSISTANT.value, content=output_string
                        )
                    ]
                    return output, token_usage

                if finish_reason == "length" and content:
                    e = {"error": "Early Termination: Length", "text": content}
                    output: list[dict | MessageBlock] = [
                        MessageBlock(
                            role=CreatorRole.ASSISTANT.value,
                            content=json.dumps(e, ensure_ascii=False),
                        )
                    ]
                    return output, token_usage

                logger.warning("Malformed response: %s", response)
                logger.warning("Config: %s", self.config)
                raise RuntimeError(f"Terminated: {finish_reason}")
            except RateLimitError as rle:
                logger.warning("RateLimitError: %s", rle)
                warn_msg = f"[{attempt}] Retrying in {delay} seconds..."
                logger.warning(warn_msg)
                await asyncio.sleep(delay)
                attempt += 1
                delay = delay * OAI_StructuredOutput_Core.DELAY_FACTOR
                delay = min(OAI_StructuredOutput_Core.MAX_DELAY, delay)
                continue
            except Exception as e:
                logger.error("Exception: %s", e, exc_info=True, stack_info=True)
                raise

        raise RuntimeError("Max re-attempt reached")

    @staticmethod
    def get_image_url(filepath: str) -> str:
        ext = os.path.splitext(filepath)[-1]
        if ext not in OAI_StructuredOutput_Core.SUPPORTED_IMAGE_FORMATS:
            raise ValueError(f"Unsupported image type: {ext}")
        ext = ext[1:] if ext != ".jpg" else "jpeg"
        try:
            with open(filepath, "rb") as f:
                encoded_image = base64.b64encode(f.read()).decode("utf-8")
                return f"data:image/{ext};base64,{encoded_image}"
        except FileNotFoundError as fnfe:
            logger.error("FileNotFoundError: %s", fnfe, exc_info=True, stack_info=True)
            raise
        except Exception as e:
            logger.error("Exception: %s", e, exc_info=True, stack_info=True)
            raise

    def interpret(
        self,
        query: str,
        context: list[MessageBlock | dict[str, Any]] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        return self.run(query=query, context=context, filepath=filepath, **kwargs)

    async def interpret_async(
        self,
        query: str,
        context: list[MessageBlock | dict[str, Any]] | None,
        filepath: str,
        **kwargs,
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        return await self.run_async(
            query=query, context=context, filepath=filepath, **kwargs
        )
