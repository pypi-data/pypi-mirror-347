import os
import logging
import time
import asyncio
import json
from typing import Any, Optional
from copy import deepcopy
from concurrent.futures import ThreadPoolExecutor
from random import random
from google import genai
from google.genai import types
from ..._core import Core, ToolSupport
from ..._util import CreatorRole, ChatCompletionConfig, MessageBlock, TokenUsage
from ..._tool import Tool
from .base import GeminiCore, TOOL_PROMPT

logger = logging.getLogger(__name__)


class T2T_GMN_Core_W_Tool(Core, GeminiCore, ToolSupport):
    """
    `T2T_GMN_Core_W_Tool` is abstract base classes `Core`, `ToolSupport` and `GeminiCore`.
    It facilitates synchronous and asynchronous communication with Gemini's API.

    Methods:
    - run(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously run the LLM model with the given query and context.
    - run_async(query: str, context: list[MessageBlock | dict] | None, **kwargs) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously run the LLM model with the given query and context.
    - call_tools_async(selected_tools: list) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Asynchronously call tools.
    - call_tools(selected_tools: list) -> tuple[list[MessageBlock | dict], TokenUsage]:
        Synchronously call tools.

    **Notes:**
    - Input: Text & Image only
    - Output: Text only
    """

    MAX_ATTEMPT: int = 5
    DELAY_FACTOR: float = 1.5
    MAX_DELAY: float = 60.0

    def __init__(
        self,
        system_prompt: str,
        config: ChatCompletionConfig,
        tools: list[Tool] | None = None,
    ):
        Core.__init__(self, system_prompt, config)
        GeminiCore.__init__(self, config.name)
        ToolSupport.__init__(self, tools)
        self.profile = self.build_profile(config.name)

    def gemini_compatible_tool_definition(self):
        tools = []

        if self.tools is None:
            return None

        for tool in self.tools:
            f_info: dict = tool.info["function"]

            properties = {}
            for key, value in f_info["parameters"]["properties"].items():
                properties[key] = types.Schema(
                    type=value["type"],
                    description=value["description"],
                )
            required = []
            if "required" in f_info["parameters"]:
                required = f_info["parameters"]["required"]

            t = types.Tool(
                function_declarations=[
                    types.FunctionDeclaration(
                        name=f_info["name"],
                        description=f_info["description"],
                        parameters=types.Schema(
                            type=types.Type.OBJECT,
                            properties=properties,
                            required=required,
                        ),
                    )
                ]
            )
            tools.append(t)

        return tools

    def custom_config(
        self, max_output_tokens: int, use_tool: bool = False
    ) -> types.GenerateContentConfig:
        """Adapter function.

        Transform custom ChatCompletionConfig -> types.GenerationContentConfig
        """
        si = self.system_prompt
        tools = None
        if use_tool:
            tools = self.gemini_compatible_tool_definition()
            if tools:
                si += f"\n{TOOL_PROMPT}"

        config = types.GenerateContentConfig(
            system_instruction=si,
            temperature=self.config.temperature,
            max_output_tokens=max_output_tokens,
            tools=tools if use_tool else None,
        )
        return config

    def __update_delay(self, delay: float) -> float:
        new_delay = delay * self.DELAY_FACTOR
        # Add some randomness to allow bulk requests to retry at a slightly different timing
        new_delay += random() * 5.0
        return min(new_delay, self.MAX_DELAY)

    def run(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Synchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        **Rate Limit Error Handling**:
        - If a RateLimitError occurs, the function will retry the request with an increasing delay.
        - The delay will be multiplied by a factor (DELAY_FACTOR) after each attempt, up to a maximum delay (MAX_DELAY).

        **Early Termination Condition**:
        The function will terminate early if any of the following conditions are met:
        - **finish_reason**:
            - "STOP": A solution is found.
            - "MAX_TOKENS": The model has reached the maximum token limit.
                If the content is not empty, it will terminate and output the generated responses.
                Otherwise, it will raise an error.
            - Other reason listed under types.FinishReason: Unexpected response. The function will raise an error.
        - **iteration**: The maximum number of iterations is reached.
        - **token_usage**: The accumulated token count is greater than or equal to the maximum token count.
        - **max_output_tokens**: The maximum output tokens are less than or equal to zero.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        # later use this to skip the preloaded messages
        number_of_inputs = len(msgs)

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0
        while attempt < T2T_GMN_Core_W_Tool.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name, self.system_prompt, messages, imgs=None
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )
            config = self.custom_config(max_output_tokens, self.tools is not None)

            iteration, solved = 0, False
            token_usage = TokenUsage(input_tokens=0, output_tokens=0)
            response = None
            try:
                client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
                while (
                    not solved
                    and max_output_tokens > 0
                    and iteration < self.config.max_iteration
                    and token_usage.total_tokens < MAX_TOKENS
                ):
                    logger.debug("Iteration [%d]", iteration)
                    response = client.models.generate_content(
                        model=self.model_name,
                        contents=messages,  # type: ignore
                        config=config,
                    )

                    token_usage = self.update_usage(
                        response.usage_metadata, token_usage=token_usage
                    )

                    candidates: Optional[list[types.Candidate]] = response.candidates
                    if candidates is None or len(candidates) == 0:
                        raise RuntimeError(
                            f"Malformed response (No candidates): {response}"
                        )

                    candidate = candidates[0]
                    finish_reason = candidate.finish_reason
                    content: Optional[types.Content] = candidate.content
                    if content is None:
                        raise RuntimeError(
                            f"Malformed response (No content): {candidate}"
                        )

                    texts = self.get_texts(content)
                    functs = self.get_functs(content)
                    logger.warning("Finish Reason: %s", finish_reason)
                    logger.warning("Texts: %s", texts)
                    logger.warning("Functions: %s", functs)
                    if finish_reason == types.FinishReason.STOP and functs:
                        logger.info("Function call detected.")
                        if texts:
                            messages.append(
                                types.Content(
                                    role=CreatorRole.MODEL.value,
                                    parts=[
                                        types.Part.from_text(text=text)
                                        for text in texts
                                    ],
                                )
                            )
                        tool_outputs, tool_token_usage = self.call_tools(
                            selected_tools=functs
                        )
                        f_c_p = self.bind_function_call_response(functs, tool_outputs)
                        messages.extend(f_c_p)
                        token_usage += tool_token_usage
                        iteration += 1
                        prompt_token_count = self.calculate_token_count(
                            self.model_name, self.system_prompt, messages, imgs=None
                        )
                        max_output_tokens = min(
                            MAX_OUTPUT_TOKENS,
                            self.context_length - prompt_token_count,
                        )
                    elif finish_reason == types.FinishReason.STOP and texts:
                        logger.info("SOLVED")
                        messages.append(
                            types.Content(
                                role=CreatorRole.MODEL.value,
                                parts=[
                                    types.Part.from_text(text=text) for text in texts
                                ],
                            )
                        )
                        solved = True
                    elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                        logger.warning("Terminated due to length.")
                        e = {
                            "error": "Early Termination: Length",
                            "text": "\n".join(texts),
                        }
                        messages.append(
                            types.Content(
                                role=CreatorRole.MODEL.value,
                                parts=[
                                    types.Part.from_text(
                                        text=json.dumps(e, ensure_ascii=False)
                                    )
                                ],
                            )
                        )
                        solved = True
                    else:
                        logger.warning("Malformed response: %s", response)
                        logger.warning("Config: %s", self.config)
                        raise RuntimeError(f"Terminated: {finish_reason}")

                # End while

                if not solved:
                    warning_message = self.warning_message(
                        iteration,
                        self.config.max_iteration,
                        token_usage,
                        MAX_TOKENS,
                        max_output_tokens,
                    )
                    logger.warning(warning_message)
                    raise RuntimeError(warning_message)

                # Return only the generated messages messages
                output = self.postprocessing(messages[number_of_inputs:])
                return output, token_usage
            except RuntimeError:
                raise
            except Exception as e:
                error_object = e.__dict__
                error_code = error_object["code"]
                if error_code not in [429, 500, 503]:
                    raise

                error_message = error_object["message"]
                logger.warning(
                    "%s\n[%d] Retrying in %.2f seconds", error_message, attempt, delay
                )
                time.sleep(delay)
                attempt += 1
                delay = self.__update_delay(delay)
                continue

        raise RuntimeError("Max re-attempt reached")

    @staticmethod
    async def acall(
        model_name: str,
        config: types.GenerateContentConfig,
        messages: list[types.Content],
    ):
        """Use this to make the `generate_content` method asynchronous."""
        client = genai.Client(api_key=os.environ["GEMINI_API_KEY"])
        with ThreadPoolExecutor() as executor:
            future = executor.submit(
                client.models.generate_content,
                model=model_name,
                contents=messages,  # type: ignore
                config=config,
            )
            response = await asyncio.wrap_future(future)  # Makes the future awaitable
            return response

    async def run_async(
        self, query: str, context: list[MessageBlock | dict] | None, **kwargs
    ) -> tuple[list[MessageBlock | dict[str, Any]], TokenUsage]:
        """
        Asynchronously run the LLM model with the given query and context.

        Args:
            query (str): The query to be processed by the LLM model.
            context (list[MessageBlock | dict] | None): The context to be used for the LLM model.
            **kwargs: Additional keyword arguments.

        Returns:
            output (tuple[list[MessageBlock | dict], TokenUsage]):
                1. The list of messages generated by the LLM model.
                2. The recorded token usage.

        **Rate Limit Error Handling**:
        - If a RateLimitError occurs, the function will retry the request with an increasing delay.
        - The delay will be multiplied by a factor (DELAY_FACTOR) after each attempt, up to a maximum delay (MAX_DELAY).

        **Early Termination Condition**:
        The function will terminate early if any of the following conditions are met:
        - **finish_reason**:
            - "STOP": A solution is found.
            - "MAX_TOKENS": The model has reached the maximum token limit.
                If the content is not empty, it will terminate and output the generated responses.
                Otherwise, it will raise an error.
            - Other reason listed under types.FinishReason: Unexpected response. The function will raise an error.
        - **iteration**: The maximum number of iterations is reached.
        - **token_usage**: The accumulated token count is greater than or equal to the maximum token count.
        - **max_output_tokens**: The maximum output tokens are less than or equal to zero.
        """
        msgs: list[types.Content] = self.preprocessing(query, context)
        # later use this to skip the preloaded messages
        number_of_inputs = len(msgs)

        MAX_TOKENS = min(self.config.max_tokens, self.context_length)
        MAX_OUTPUT_TOKENS = min(
            MAX_TOKENS, self.max_output_tokens, self.config.max_output_tokens
        )

        attempt: int = 1
        delay: float = 5.0
        while attempt < T2T_GMN_Core_W_Tool.MAX_ATTEMPT:
            logger.debug("Attempt %d", attempt)
            messages = deepcopy(msgs)
            prompt_token_count = self.calculate_token_count(
                self.model_name, self.system_prompt, messages, imgs=None
            )
            max_output_tokens = min(
                MAX_OUTPUT_TOKENS,
                self.context_length - prompt_token_count,
            )
            config = self.custom_config(max_output_tokens, self.tools is not None)

            iteration, solved = 0, False
            token_usage = TokenUsage(input_tokens=0, output_tokens=0)
            response = None
            try:
                while (
                    not solved
                    and max_output_tokens > 0
                    and iteration < self.config.max_iteration
                    and token_usage.total_tokens < MAX_TOKENS
                ):
                    logger.debug("Iteration [%d]", iteration)
                    response = await self.acall(self.model_name, config, messages)

                    token_usage = self.update_usage(
                        response.usage_metadata, token_usage=token_usage
                    )

                    candidates: Optional[list[types.Candidate]] = response.candidates
                    if candidates is None or len(candidates) == 0:
                        raise RuntimeError(
                            f"Malformed response (No candidates): {response}"
                        )

                    candidate = candidates[0]
                    finish_reason = candidate.finish_reason
                    content: Optional[types.Content] = candidate.content
                    if content is None:
                        raise RuntimeError(
                            f"Malformed response (No content): {candidate}"
                        )

                    texts = self.get_texts(content)
                    functs = self.get_functs(content)
                    if finish_reason == types.FinishReason.STOP and functs:
                        logger.info("Function call detected.")
                        if texts:
                            messages.append(
                                types.Content(
                                    role=CreatorRole.MODEL.value,
                                    parts=[
                                        types.Part.from_text(text=text)
                                        for text in texts
                                    ],
                                )
                            )
                        tool_outputs, tool_token_usage = await self.call_tools_async(
                            selected_tools=functs
                        )
                        f_c_p = self.bind_function_call_response(functs, tool_outputs)
                        messages.extend(f_c_p)
                        token_usage += tool_token_usage
                        iteration += 1
                        prompt_token_count = self.calculate_token_count(
                            self.model_name, self.system_prompt, messages, imgs=None
                        )
                        max_output_tokens = min(
                            MAX_OUTPUT_TOKENS,
                            self.context_length - prompt_token_count,
                        )
                    elif finish_reason == types.FinishReason.STOP and texts:
                        logger.info("SOLVED")
                        messages.append(
                            types.Content(
                                role=CreatorRole.MODEL.value,
                                parts=[
                                    types.Part.from_text(text=text) for text in texts
                                ],
                            )
                        )
                        solved = True
                    elif finish_reason == types.FinishReason.MAX_TOKENS and texts:
                        logger.warning("Terminated due to length.")
                        e = {
                            "error": "Early Termination: Length",
                            "text": "\n".join(texts),
                        }
                        messages.append(
                            types.Content(
                                role=CreatorRole.MODEL.value,
                                parts=[
                                    types.Part.from_text(
                                        text=json.dumps(e, ensure_ascii=False)
                                    )
                                ],
                            )
                        )
                        solved = True

                    else:
                        logger.warning("Malformed response: %s", response)
                        logger.warning("Config: %s", self.config)
                        raise RuntimeError(f"Terminated: {finish_reason}")

                # End while

                if not solved:
                    warning_message = self.warning_message(
                        iteration,
                        self.config.max_iteration,
                        token_usage,
                        MAX_TOKENS,
                        max_output_tokens,
                    )
                    logger.warning(warning_message)
                    raise RuntimeError(warning_message)

                output = self.postprocessing(messages[number_of_inputs:])
                # Return only the generated messages messages
                return output, token_usage
            except RuntimeError:
                raise
            except Exception as e:
                error_object = e.__dict__
                error_code = error_object["code"]
                if error_code not in [429, 500, 503]:
                    raise

                error_message = error_object["message"]
                logger.warning(
                    "%s\n[%d] Retrying in %.2f seconds", error_message, attempt, delay
                )
                await asyncio.sleep(delay)
                attempt += 1
                delay = self.__update_delay(delay)
                continue

        raise RuntimeError("Max re-attempt reached")

    async def call_tools_async(
        self, selected_tools: list
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        """
        Asynchronously call every selected tools.

        Args:
            selected_tools (list): A list of selected tools.

        Returns:
            list: A list of messages generated by the tools.
            TokenUsage: The recorded token usage.

        Notes:
            - If more than one tool is selected, they are executed independently and separately.
            - Tools chaining is not supported.
            - Does not raise exception on failed tool execution, an error message is returned instead to guide the calling LLM.
        """
        output: list[MessageBlock | dict] = []
        token_usage = TokenUsage()
        for tool_call in selected_tools:
            for tool in self.tools:  # type: ignore
                if tool.token_usage.total_tokens > 0:
                    tool.reset_token_usage()

                if tool.info["function"]["name"] != tool_call["name"]:
                    continue
                args = tool_call["arguments"]
                args_str = "JSONDecodeError"
                try:
                    args_str = json.dumps(args, ensure_ascii=False)
                    result = await tool.run_async(args_str)
                    output_string = f"{tool_call['name']}({args_str}) -> {result}"
                    output.append(
                        MessageBlock(
                            role=CreatorRole.USER.value,
                            content=output_string,
                        )
                    )
                except json.JSONDecodeError as jde:
                    output.append(
                        {
                            "role": CreatorRole.USER.value,
                            "content": f"Function {tool_call['name']} called. Failed: JSONDecodeError|{str(jde)}",
                        }
                    )
                except Exception as e:
                    output.append(
                        {
                            "role": CreatorRole.USER.value,
                            "content": f"{tool_call['name']}({args_str}) -> {e}",
                        }
                    )
                finally:
                    token_usage += tool.token_usage

        return output, token_usage

    def call_tools(
        self, selected_tools: list
    ) -> tuple[list[MessageBlock | dict], TokenUsage]:
        """
        Synchronously call every selected tools.

        Args:
            selected_tools (list): A list of selected tools.

        Returns:
            list: A list of messages generated by the tools.
            TokenUsage: The recorded token usage.

        Notes:
            - If more than one tool is selected, they are executed independently and separately.
            - Tools chaining is not supported.
            - Does not raise exception on failed tool execution, an error message is returned instead to guide the calling LLM.
        """
        output: list[MessageBlock | dict] = []
        token_usage = TokenUsage()
        for tool_call in selected_tools:
            for tool in self.tools:  # type: ignore
                if tool.token_usage.total_tokens > 0:
                    tool.reset_token_usage()

                if tool.info["function"]["name"] != tool_call["name"]:
                    continue
                args = tool_call["arguments"]

                logger.warning(
                    "Calling %s with %s", tool.info["function"]["name"], args
                )
                try:
                    args_str = json.dumps(args, ensure_ascii=False)
                    result = tool.run(args_str)
                    output_string = f"{tool_call['name']}({args_str}) -> {result}"
                    output.append(
                        MessageBlock(
                            role=CreatorRole.USER.value,
                            content=output_string,
                        )
                    )
                except json.JSONDecodeError as jde:
                    output.append(
                        {
                            "role": CreatorRole.USER.value,
                            "content": f"Function {tool_call['name']} called. Failed: JSONDecodeError|{str(jde)}",
                        }
                    )
                except Exception as e:
                    output.append(
                        {
                            "role": CreatorRole.USER.value,
                            "content": f"{tool_call['name']}({args_str}) -> {e}",
                        }
                    )
                finally:
                    token_usage += tool.token_usage

        return output, token_usage

    @staticmethod
    def bind_function_call_response(
        fcalls: list[dict[str, Any]], fresps: list[MessageBlock | dict]
    ) -> list[types.Content]:
        """
        Adapter function to bind function call and function_call_response.
        """
        output: list[types.Content] = []
        for fc, fr in zip(fcalls, fresps):
            output.append(
                types.Content(
                    role=CreatorRole.MODEL.value,
                    parts=[
                        types.Part.from_function_call(
                            name=fc["name"],
                            args=fc["arguments"],
                        )
                    ],
                )
            )
            output.append(
                types.Content(
                    role=CreatorRole.USER.value,
                    parts=[types.Part.from_text(text=fr["content"])],
                )
            )
        return output
