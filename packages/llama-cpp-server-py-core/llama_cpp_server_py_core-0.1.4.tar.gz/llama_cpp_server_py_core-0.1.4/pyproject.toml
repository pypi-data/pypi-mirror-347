[project]
name = "llama-cpp-server-py-core"
version = "0.1.4"
description = "Add your description here"
authors = [
    { name = "Fangyin Cheng", email = "staneyffer@gmail.com" },
]
dependencies = []
readme = "README.md"
requires-python = ">= 3.10"

[build-system]
requires = [
    "scikit-build-core[pyproject]",
]
build-backend = "scikit_build_core.build"

[tool.rye]
managed = true
dev-dependencies = [
    "jupyter>=1.1.1",
    "numpy>=2.2.1",
    "torch>=2.5.1",
    "safetensors>=0.4.5",
    "transformers>=4.47.1",
]

[tool.rye.scripts]
hf2gguf = "llama_cpp_server_py_core/convert_hf_to_gguf.py"
build-cpp = "pip install --no-deps -e ."

[tool.rye.scripts.quantize]
cmd = "llama_cpp_server_py_core/lib/llama-quantize"

[tool.scikit-build]
minimum-version = "0.5.1"

[tool.scikit-build.wheel]
packages = [
    "llama_cpp_server_py_core",
]
exclude = [
    "include/*",
    "lib/*",
    "bin/*",
]

[tool.scikit-build.cmake]
verbose = true
minimum-version = "3.21"
build-type = "Release"
args = [
    "-DLLAMA_CURL=OFF",
]

[tool.scikit-build.sdist]
include = [
    "llama.cpp/*",
    "bin/convert_hf_to_gguf.py",
]
exclude = [
    "llama.cpp/.git",
]
