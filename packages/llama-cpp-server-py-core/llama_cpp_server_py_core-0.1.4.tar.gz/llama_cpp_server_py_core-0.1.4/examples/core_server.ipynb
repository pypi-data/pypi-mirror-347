{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from llama_cpp_server_py_core import (\n",
    "    ServerProcess,\n",
    "    ServerConfig,\n",
    "    LlamaCppServer,\n",
    "    CompletionRequest,\n",
    "    CompletionResponse,\n",
    "    ChatCompletionRequest,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 19:05:11,958 | INFO | llama_cpp_server_py_core.utils | Randomly selected available port: 55758\n",
      "2024-12-31 19:05:11,959 | INFO | llama_cpp_server_py_core.utils | Starting server process, start command: ['/home/staneyffer/codes/dl/llm/llamacpp/llama-cpp-server-py/llama-cpp-server-py-core/llama_cpp_server_py_core/lib/llama-server', '--host', '127.0.0.1', '--port', '55758', '--temp', '0.8', '--seed', '42', '--model', '/data/dl/llm/qwen/Qwen2.5-Coder-14B-Instruct-Q4_k_m.gguf', '--n-gpu-layers', '1000000000', '--alias', 'tinyllama-2']\n",
      "2024-12-31 19:05:11,960 | INFO | llama_cpp_server_py_core.utils | Started server process with PID 2946107, current process: 2946030\n",
      "2024-12-31 19:05:11,961 | WARNING | urllib3.connectionpool | Retrying (Retry(total=2, connect=2, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74e7dc1fd7e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /health\n",
      "2024-12-31 19:05:11,962 | INFO | llama_cpp_server_py_core.utils | warning: no usable GPU found, --gpu-layers option will be ignored\n",
      "2024-12-31 19:05:11,977 | INFO | llama_cpp_server_py_core.utils | warning: one possible reason is that llama.cpp was compiled without GPU support\n",
      "warning: consult docs/build.md for compilation instructions\n",
      "build: 4372 (e34c5af4) with gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
      "system info: n_threads = 16, n_threads_batch = 16, total_threads = 32\n",
      "\n",
      "system_info: n_threads = 16 (n_threads_batch = 16) / 32 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: HTTP server is listening, hostname: 127.0.0.1, port: 55758, http threads: 31\n",
      "main: loading model\n",
      "srv    load_model: loading model '/data/dl/llm/qwen/Qwen2.5-Coder-14B-Instruct-Q4_k_m.gguf'\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 579 tensors from /data/dl/llm/qwen/Qwen2.5-Coder-14B-Instruct-Q4_k_m.gguf (version GGUF V3 (latest))\n",
      "2024-12-31 19:05:11,977 | INFO | llama_cpp_server_py_core.utils | llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 14B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 14B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 14B\n",
      "2024-12-31 19:05:11,977 | INFO | llama_cpp_server_py_core.utils | llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,6]       = [\"code\", \"codeqwen\", \"chat\", \"qwen\", ...\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 48\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "2024-12-31 19:05:11,995 | INFO | llama_cpp_server_py_core.utils | llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "2024-12-31 19:05:12,084 | INFO | llama_cpp_server_py_core.utils | llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  241 tensors\n",
      "llama_model_loader: - type q4_K:  289 tensors\n",
      "llama_model_loader: - type q6_K:   49 tensors\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "2024-12-31 19:05:12,084 | INFO | llama_cpp_server_py_core.utils | llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "2024-12-31 19:05:12,084 | INFO | llama_cpp_server_py_core.utils | llm_load_print_meta: n_gqa            = 5\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "2024-12-31 19:05:12,084 | INFO | llama_cpp_server_py_core.utils | llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "2024-12-31 19:05:12,084 | INFO | llama_cpp_server_py_core.utils | llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 14B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 14.77 B\n",
      "llm_load_print_meta: model size       = 8.37 GiB (4.87 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 Coder 14B Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "2024-12-31 19:05:12,085 | INFO | llama_cpp_server_py_core.utils | llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staneyffer/.rye/py/cpython@3.10.14/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/home/staneyffer/.rye/py/cpython@3.10.14/lib/python3.10/subprocess.py:966: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stderr = io.open(errread, 'rb', bufsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 19:05:12,212 | INFO | llama_cpp_server_py_core.utils | llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  8566.04 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 4096\n",
      "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
      "llama_new_context_with_model: n_batch       = 2048\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "2024-12-31 19:05:12,324 | INFO | llama_cpp_server_py_core.utils | llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48\n",
      "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   368.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1686\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "2024-12-31 19:05:12,464 | INFO | llama_cpp_server_py_core.utils | srv          init: initializing slots, n_slots = 1\n",
      "slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\n",
      "main: model loaded\n",
      "main: chat template, built_in: 1, chat_example: '<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "2024-12-31 19:05:12,562 | WARNING | urllib3.connectionpool | Retrying (Retry(total=1, connect=1, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74e7dc1fda20>: Failed to establish a new connection: [Errno 111] Connection refused')': /health\n",
      "2024-12-31 19:05:12,563 | INFO | llama_cpp_server_py_core.utils | Server is now running and healthy\n"
     ]
    }
   ],
   "source": [
    "config = ServerConfig(\n",
    "    model_file=\"/data/dl/llm/qwen/Qwen2.5-Coder-14B-Instruct-Q4_k_m.gguf\",\n",
    "    model_hf_repo=None,\n",
    "    model_hf_file=None,\n",
    "    n_gpu_layers=1000000000,\n",
    ")\n",
    "\n",
    "server = ServerProcess(config)\n",
    "server.start(timeout_seconds=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ServerStatus.RUNNING: 'running'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = LlamaCppServer(server, config)\n",
    "ms.health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default_generation_settings': {'id': 0,\n",
       "  'id_task': -1,\n",
       "  'n_ctx': 4096,\n",
       "  'speculative': False,\n",
       "  'is_processing': False,\n",
       "  'non_causal': False,\n",
       "  'params': {'n_predict': -1,\n",
       "   'seed': 42,\n",
       "   'temperature': 0.800000011920929,\n",
       "   'dynatemp_range': 0.0,\n",
       "   'dynatemp_exponent': 1.0,\n",
       "   'top_k': 40,\n",
       "   'top_p': 0.949999988079071,\n",
       "   'min_p': 0.05000000074505806,\n",
       "   'xtc_probability': 0.0,\n",
       "   'xtc_threshold': 0.10000000149011612,\n",
       "   'typical_p': 1.0,\n",
       "   'repeat_last_n': 64,\n",
       "   'repeat_penalty': 1.0,\n",
       "   'presence_penalty': 0.0,\n",
       "   'frequency_penalty': 0.0,\n",
       "   'dry_multiplier': 0.0,\n",
       "   'dry_base': 1.75,\n",
       "   'dry_allowed_length': 2,\n",
       "   'dry_penalty_last_n': 4096,\n",
       "   'dry_sequence_breakers': ['\\n', ':', '\"', '*'],\n",
       "   'mirostat': 0,\n",
       "   'mirostat_tau': 5.0,\n",
       "   'mirostat_eta': 0.10000000149011612,\n",
       "   'stop': [],\n",
       "   'max_tokens': -1,\n",
       "   'n_keep': 0,\n",
       "   'n_discard': 0,\n",
       "   'ignore_eos': False,\n",
       "   'stream': True,\n",
       "   'logit_bias': [],\n",
       "   'n_probs': 0,\n",
       "   'min_keep': 0,\n",
       "   'grammar': '',\n",
       "   'samplers': ['penalties',\n",
       "    'dry',\n",
       "    'top_k',\n",
       "    'typ_p',\n",
       "    'top_p',\n",
       "    'min_p',\n",
       "    'xtc',\n",
       "    'temperature'],\n",
       "   'speculative.n_max': 16,\n",
       "   'speculative.n_min': 5,\n",
       "   'speculative.p_min': 0.8999999761581421,\n",
       "   'timings_per_token': False,\n",
       "   'post_sampling_probs': False},\n",
       "  'prompt': '',\n",
       "  'next_token': {'has_next_token': True,\n",
       "   'has_new_line': False,\n",
       "   'n_remain': -1,\n",
       "   'n_decoded': 0,\n",
       "   'stopping_word': ''}},\n",
       " 'total_slots': 1,\n",
       " 'model_path': '/data/dl/llm/qwen/Qwen2.5-Coder-14B-Instruct-Q4_k_m.gguf',\n",
       " 'chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.props()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelList(object='list', data=[ModelCard(id='tinyllama-2', object='model', created=1735643113, owned_by='llamacpp', meta={'vocab_type': 2, 'n_vocab': 152064, 'n_ctx_train': 32768, 'n_embd': 5120, 'n_params': 14770033664, 'size': 8982142976})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 19:05:13,567 | INFO | llama_cpp_server_py_core.utils | slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 1, n_tokens = 1, progress = 1.000000\n",
      "slot update_slots: id  0 | task 0 | prompt done, n_past = 1, n_tokens = 1\n",
      "2024-12-31 19:05:15,632 | INFO | llama_cpp_server_py_core.utils | slot      release: id  0 | task 0 | stop processing: n_past = 20, truncated = 0\n",
      "2024-12-31 19:05:15,639 | INFO | llama_cpp_server_py_core.utils | slot print_timing: id  0 | task 0 | \n",
      "prompt eval time =     129.72 ms /     1 tokens (  129.72 ms per token,     7.71 tokens per second)\n",
      "       eval time =    2436.39 ms /    20 tokens (  121.82 ms per token,     8.21 tokens per second)\n",
      "      total time =    2566.10 ms /    21 tokens\n",
      "srv  update_slots: all slots are idle\n",
      "request: POST /completion 127.0.0.1 200\n",
      "slot launch_slot_: id  0 | task 21 | processing task\n",
      "slot update_slots: id  0 | task 21 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 2\n",
      "slot update_slots: id  0 | task 21 | kv cache rm [1, end)\n",
      "slot update_slots: id  0 | task 21 | prompt processing progress, n_past = 2, n_tokens = 1, progress = 0.500000\n",
      "slot update_slots: id  0 | task 21 | prompt done, n_past = 2, n_tokens = 1\n",
      "2024-12-31 19:05:18,241 | INFO | llama_cpp_server_py_core.utils | slot      release: id  0 | task 21 | stop processing: n_past = 21, truncated = 0\n",
      "slot print_timing: id  0 | task 21 | \n",
      "prompt eval time =     129.36 ms /     1 tokens (  129.36 ms per token,     7.73 tokens per second)\n",
      "       eval time =    2457.33 ms /    20 tokens (  122.87 ms per token,     8.14 tokens per second)\n",
      "      total time =    2586.69 ms /    21 tokens\n",
      "srv  update_slots: all slots are idle\n",
      "request: POST /completion 127.0.0.1 200\n",
      "slot launch_slot_: id  0 | task 42 | processing task\n",
      "slot update_slots: id  0 | task 42 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 18\n",
      "slot update_slots: id  0 | task 42 | kv cache rm [0, end)\n",
      "slot update_slots: id  0 | task 42 | prompt processing progress, n_past = 18, n_tokens = 18, progress = 1.000000\n",
      "2024-12-31 19:05:18,742 | INFO | llama_cpp_server_py_core.utils | slot update_slots: id  0 | task 42 | prompt done, n_past = 18, n_tokens = 18\n",
      "2024-12-31 19:05:19,578 | INFO | llama_cpp_server_py_core.utils | slot      release: id  0 | task 42 | stop processing: n_past = 26, truncated = 0\n",
      "slot print_timing: id  0 | task 42 | \n",
      "prompt eval time =     298.05 ms /    18 tokens (   16.56 ms per token,    60.39 tokens per second)\n",
      "       eval time =    1033.69 ms /     9 tokens (  114.85 ms per token,     8.71 tokens per second)\n",
      "      total time =    1331.73 ms /    27 tokens\n",
      "srv  update_slots: all slots are idle\n",
      "request: POST /chat/completions 127.0.0.1 200\n",
      "slot launch_slot_: id  0 | task 52 | processing task\n",
      "slot update_slots: id  0 | task 52 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 18\n",
      "slot update_slots: id  0 | task 52 | need to evaluate at least 1 token to generate logits, n_past = 18, n_prompt_tokens = 18\n",
      "slot update_slots: id  0 | task 52 | kv cache rm [17, end)\n",
      "2024-12-31 19:05:20,079 | INFO | llama_cpp_server_py_core.utils | slot update_slots: id  0 | task 52 | prompt processing progress, n_past = 18, n_tokens = 1, progress = 0.055556\n",
      "slot update_slots: id  0 | task 52 | prompt done, n_past = 18, n_tokens = 1\n",
      "2024-12-31 19:05:20,754 | INFO | llama_cpp_server_py_core.utils | slot      release: id  0 | task 52 | stop processing: n_past = 26, truncated = 0\n",
      "slot print_timing: id  0 | task 52 | \n",
      "prompt eval time =     129.70 ms /     1 tokens (  129.70 ms per token,     7.71 tokens per second)\n",
      "       eval time =    1039.48 ms /     9 tokens (  115.50 ms per token,     8.66 tokens per second)\n",
      "      total time =    1169.19 ms /    10 tokens\n",
      "srv  update_slots: all slots are idle\n",
      "request: POST /chat/completions 127.0.0.1 200\n",
      "slot launch_slot_: id  0 | task 62 | processing task\n",
      "slot update_slots: id  0 | task 62 | new prompt, n_ctx_slot = 4096, n_keep = 0, n_prompt_tokens = 18\n",
      "slot update_slots: id  0 | task 62 | need to evaluate at least 1 token to generate logits, n_past = 18, n_prompt_tokens = 18\n",
      "slot update_slots: id  0 | task 62 | kv cache rm [17, end)\n",
      "2024-12-31 19:05:21,516 | INFO | llama_cpp_server_py_core.utils | slot update_slots: id  0 | task 62 | prompt processing progress, n_past = 18, n_tokens = 1, progress = 0.055556\n",
      "slot update_slots: id  0 | task 62 | prompt done, n_past = 18, n_tokens = 1\n",
      "slot      release: id  0 | task 62 | stop processing: n_past = 19, truncated = 0\n",
      "slot print_timing: id  0 | task 62 | \n",
      "prompt eval time =     130.66 ms /     1 tokens (  130.66 ms per token,     7.65 tokens per second)\n",
      "       eval time =     130.35 ms /     2 tokens (   65.17 ms per token,    15.34 tokens per second)\n",
      "      total time =     261.00 ms /     3 tokens\n",
      "srv  update_slots: all slots are idle\n",
      "request: POST /chat/completions 127.0.0.1 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(index=0, content=',你好,你好! 你好吗? 你好,你好,你好! 你好吗?', tokens=[], stop=True, id_slot=0, tokens_predicted=20, tokens_evaluated=1, model='tinyllama-2', generation_settings=GenerationSettings(n_predict=20, seed=4294967295, temperature=0.800000011920929, dynatemp_range=0.0, dynatemp_exponent=1.0, top_k=40, top_p=0.949999988079071, min_p=0.05000000074505806, xtc_probability=0.0, xtc_threshold=0.10000000149011612, typical_p=1.0, repeat_last_n=64, repeat_penalty=1.100000023841858, presence_penalty=0.0, frequency_penalty=0.0, dry_multiplier=0.0, dry_base=1.75, dry_allowed_length=2, dry_penalty_last_n=4096, dry_sequence_breakers=['\\n', ':', '\"', '*'], mirostat=0, mirostat_tau=5.0, mirostat_eta=0.10000000149011612, stop=[], max_tokens=20, n_keep=0, n_discard=0, ignore_eos=False, stream=False, logit_bias=[], n_probs=0, min_keep=0, grammar='', samplers=['dry', 'top_k', 'typ_p', 'top_p', 'min_p', 'xtc', 'temperature'], speculative_n_max=16, speculative_n_min=5, speculative_p_min=0.8999999761581421, timings_per_token=False, post_sampling_probs=False), prompt='你好', has_new_line=False, truncated=False, stop_type='limit', stopping_word='', tokens_cached=20, timings={'prompt_n': 1, 'prompt_ms': 129.715, 'prompt_per_token_ms': 129.715, 'prompt_per_second': 7.709208649732105, 'predicted_n': 20, 'predicted_ms': 2436.386, 'predicted_per_token_ms': 121.8193, 'predicted_per_second': 8.208879873714592})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.completion(CompletionRequest(prompt=\"你好\", n_predict=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionResponse(index=0, content='，', tokens=[3837], stop=False, id_slot=-1, tokens_predicted=1, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='小', tokens=[30709], stop=False, id_slot=-1, tokens_predicted=2, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='猫咪', tokens=[116076], stop=False, id_slot=-1, tokens_predicted=3, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='！\\n\\n', tokens=[17701], stop=False, id_slot=-1, tokens_predicted=4, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='你好', tokens=[108386], stop=False, id_slot=-1, tokens_predicted=5, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='呀', tokens=[104256], stop=False, id_slot=-1, tokens_predicted=6, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='，', tokens=[3837], stop=False, id_slot=-1, tokens_predicted=7, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='大', tokens=[26288], stop=False, id_slot=-1, tokens_predicted=8, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='狗狗', tokens=[109032], stop=False, id_slot=-1, tokens_predicted=9, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='！\\n\\n', tokens=[17701], stop=False, id_slot=-1, tokens_predicted=10, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='你好', tokens=[108386], stop=False, id_slot=-1, tokens_predicted=11, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='呀', tokens=[104256], stop=False, id_slot=-1, tokens_predicted=12, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='，', tokens=[3837], stop=False, id_slot=-1, tokens_predicted=13, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='小', tokens=[30709], stop=False, id_slot=-1, tokens_predicted=14, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='狐狸', tokens=[114647], stop=False, id_slot=-1, tokens_predicted=15, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='！\\n\\n', tokens=[17701], stop=False, id_slot=-1, tokens_predicted=16, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='你好', tokens=[108386], stop=False, id_slot=-1, tokens_predicted=17, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='呀', tokens=[104256], stop=False, id_slot=-1, tokens_predicted=18, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='，', tokens=[3837], stop=False, id_slot=-1, tokens_predicted=19, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='大', tokens=[26288], stop=False, id_slot=-1, tokens_predicted=20, tokens_evaluated=2, model=None, generation_settings=None, prompt=None, has_new_line=None, truncated=None, stop_type=None, stopping_word=None, tokens_cached=None, timings=None)\n",
      "CompletionResponse(index=0, content='', tokens=[], stop=True, id_slot=0, tokens_predicted=20, tokens_evaluated=2, model='tinyllama-2', generation_settings=GenerationSettings(n_predict=20, seed=4294967295, temperature=0.800000011920929, dynatemp_range=0.0, dynatemp_exponent=1.0, top_k=40, top_p=0.949999988079071, min_p=0.05000000074505806, xtc_probability=0.0, xtc_threshold=0.10000000149011612, typical_p=1.0, repeat_last_n=64, repeat_penalty=1.100000023841858, presence_penalty=0.0, frequency_penalty=0.0, dry_multiplier=0.0, dry_base=1.75, dry_allowed_length=2, dry_penalty_last_n=4096, dry_sequence_breakers=['\\n', ':', '\"', '*'], mirostat=0, mirostat_tau=5.0, mirostat_eta=0.10000000149011612, stop=[], max_tokens=20, n_keep=0, n_discard=0, ignore_eos=False, stream=True, logit_bias=[], n_probs=0, min_keep=0, grammar='', samplers=['dry', 'top_k', 'typ_p', 'top_p', 'min_p', 'xtc', 'temperature'], speculative_n_max=16, speculative_n_min=5, speculative_p_min=0.8999999761581421, timings_per_token=False, post_sampling_probs=False), prompt='你好呀', has_new_line=True, truncated=False, stop_type='limit', stopping_word='', tokens_cached=21, timings={'prompt_n': 1, 'prompt_ms': 129.36, 'prompt_per_token_ms': 129.36, 'prompt_per_second': 7.730364873222015, 'predicted_n': 20, 'predicted_ms': 2457.334, 'predicted_per_token_ms': 122.8667, 'predicted_per_second': 8.138901752875272})\n"
     ]
    }
   ],
   "source": [
    "for out in ms.stream_completion(\n",
    "    CompletionRequest(prompt=\"你好呀\", n_predict=20, stream=True)\n",
    "):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'prompt_n': 1,\n",
       "  'prompt_ms': 129.36,\n",
       "  'prompt_per_token_ms': 129.36,\n",
       "  'prompt_per_second': 7.730364873222015,\n",
       "  'predicted_n': 20,\n",
       "  'predicted_ms': 2457.334,\n",
       "  'predicted_per_token_ms': 122.8667,\n",
       "  'predicted_per_second': 8.138901752875272},\n",
       " GenerationSettings(n_predict=20, seed=4294967295, temperature=0.800000011920929, dynatemp_range=0.0, dynatemp_exponent=1.0, top_k=40, top_p=0.949999988079071, min_p=0.05000000074505806, xtc_probability=0.0, xtc_threshold=0.10000000149011612, typical_p=1.0, repeat_last_n=64, repeat_penalty=1.100000023841858, presence_penalty=0.0, frequency_penalty=0.0, dry_multiplier=0.0, dry_base=1.75, dry_allowed_length=2, dry_penalty_last_n=4096, dry_sequence_breakers=['\\n', ':', '\"', '*'], mirostat=0, mirostat_tau=5.0, mirostat_eta=0.10000000149011612, stop=[], max_tokens=20, n_keep=0, n_discard=0, ignore_eos=False, stream=True, logit_bias=[], n_probs=0, min_keep=0, grammar='', samplers=['dry', 'top_k', 'typ_p', 'top_p', 'min_p', 'xtc', 'temperature'], speculative_n_max=16, speculative_n_min=5, speculative_p_min=0.8999999761581421, timings_per_token=False, post_sampling_probs=False))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.timings, out.generation_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643118, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='你好'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643118, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='！'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643118, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='有什么'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643118, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='我可以'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643119, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='帮'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643119, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='你的'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643119, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='吗'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643119, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content='？'), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643119, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content=''), finish_reason=None)], usage=None, timings=None)\n",
      "ChatCompletionStreamResponse(id='chatcmpl-Hnv0XJB4g864sZalKFYiXsWQXGWP7G6V', object='chat.completion.chunk', created=1735643119, model='tinyllama-2', choices=[ChatCompletionResponseStreamChoice(index=0, delta=DeltaMessage(role=None, content=None), finish_reason='stop')], usage={'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27}, timings={'prompt_n': 18, 'prompt_ms': 298.048, 'prompt_per_token_ms': 16.558222222222224, 'prompt_per_second': 60.39295683916685, 'predicted_n': 9, 'predicted_ms': 1033.687, 'predicted_per_token_ms': 114.8541111111111, 'predicted_per_second': 8.706697481926348})\n"
     ]
    }
   ],
   "source": [
    "req = ChatCompletionRequest(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个聊天机器人\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "    ],\n",
    "    max_tokens=20,\n",
    "    stream=True,\n",
    ")\n",
    "for out in ms.stream_chat_completion(req):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(id='chatcmpl-c27suKshuSqFa4yo98uFhhObJkfpX7Gu', object='chat.completion', created=1735643120, model='tinyllama-2', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='你好！有什么我可以帮你的吗？'), finish_reason='stop')], usage={'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27}, timings={'prompt_n': 1, 'prompt_ms': 129.704, 'prompt_per_token_ms': 129.704, 'prompt_per_second': 7.709862456053783, 'predicted_n': 9, 'predicted_ms': 1039.483, 'predicted_per_token_ms': 115.4981111111111, 'predicted_per_second': 8.6581502535395})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req = ChatCompletionRequest(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个聊天机器人\"},\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "    ],\n",
    "    max_tokens=20,\n",
    "    stream=False,\n",
    ")\n",
    "ms.chat_completion(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\u4f60\\u597d\\uff01\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1735643121,\n",
      "  \"model\": \"tinyllama-2\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 2,\n",
      "    \"prompt_tokens\": 18,\n",
      "    \"total_tokens\": 20\n",
      "  },\n",
      "  \"id\": \"chatcmpl-CMiSZfJwh4uzanNkctSxg69esAnbLV69\",\n",
      "  \"timings\": {\n",
      "    \"prompt_n\": 1,\n",
      "    \"prompt_ms\": 130.655,\n",
      "    \"prompt_per_token_ms\": 130.655,\n",
      "    \"prompt_per_second\": 7.65374459454288,\n",
      "    \"predicted_n\": 2,\n",
      "    \"predicted_ms\": 130.35,\n",
      "    \"predicted_per_token_ms\": 65.175,\n",
      "    \"predicted_per_second\": 15.34330648254699\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "res = server.make_request(\n",
    "    \"POST\",\n",
    "    \"/chat/completions\",\n",
    "    data={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个聊天机器人\"},\n",
    "            {\"role\": \"user\", \"content\": \"你好\"},\n",
    "        ],\n",
    "        \"max_tokens\": 2,\n",
    "    },\n",
    ")\n",
    "\n",
    "# res.body\n",
    "print(json.dumps(res.body, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
