# This file was auto-generated by Fern from our API Definition.

import typing

import httpx
from .batch_processor_run.client import AsyncBatchProcessorRunClient, BatchProcessorRunClient
from .batch_workflow_run.client import AsyncBatchWorkflowRunClient, BatchWorkflowRunClient
from .core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from .core.request_options import RequestOptions
from .environment import ExtendEnvironment
from .evaluation_set.client import AsyncEvaluationSetClient, EvaluationSetClient
from .evaluation_set_item.client import AsyncEvaluationSetItemClient, EvaluationSetItemClient
from .file.client import AsyncFileClient, FileClient
from .file_endpoints.client import AsyncFileEndpointsClient, FileEndpointsClient
from .processor.client import AsyncProcessorClient, ProcessorClient
from .processor_run.client import AsyncProcessorRunClient, ProcessorRunClient
from .processor_version.client import AsyncProcessorVersionClient, ProcessorVersionClient
from .raw_client import AsyncRawExtend, RawExtend
from .types.api_version_enum import ApiVersionEnum
from .types.json_object import JsonObject
from .types.parse_config import ParseConfig
from .types.parse_request_file import ParseRequestFile
from .types.parse_response import ParseResponse
from .types.processor_id import ProcessorId
from .types.processor_run_file_input import ProcessorRunFileInput
from .types.run_processor_request_config import RunProcessorRequestConfig
from .types.run_processor_response import RunProcessorResponse
from .types.run_workflow_response import RunWorkflowResponse
from .types.workflow_run_file_input import WorkflowRunFileInput
from .workflow.client import AsyncWorkflowClient, WorkflowClient
from .workflow_run.client import AsyncWorkflowRunClient, WorkflowRunClient
from .workflow_run_output.client import AsyncWorkflowRunOutputClient, WorkflowRunOutputClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class Extend:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : ExtendEnvironment
        The environment to use for requests from the client. from .environment import ExtendEnvironment

        Defaults to ExtendEnvironment.PRODUCTION



    extend_api_version : typing.Optional[ApiVersionEnum]
    token : typing.Union[str, typing.Callable[[], str]]
    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.Client]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from extend_ai import Extend
    client = Extend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: ExtendEnvironment = ExtendEnvironment.PRODUCTION,
        extend_api_version: typing.Optional[ApiVersionEnum] = None,
        token: typing.Union[str, typing.Callable[[], str]],
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.Client] = None,
    ):
        _defaulted_timeout = (
            timeout if timeout is not None else 60 if httpx_client is None else httpx_client.timeout.read
        )
        self._client_wrapper = SyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            extend_api_version=extend_api_version,
            token=token,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.Client(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.Client(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self._raw_client = RawExtend(client_wrapper=self._client_wrapper)
        self.workflow_run = WorkflowRunClient(client_wrapper=self._client_wrapper)
        self.batch_workflow_run = BatchWorkflowRunClient(client_wrapper=self._client_wrapper)
        self.processor_run = ProcessorRunClient(client_wrapper=self._client_wrapper)
        self.processor = ProcessorClient(client_wrapper=self._client_wrapper)
        self.processor_version = ProcessorVersionClient(client_wrapper=self._client_wrapper)
        self.file = FileClient(client_wrapper=self._client_wrapper)
        self.file_endpoints = FileEndpointsClient(client_wrapper=self._client_wrapper)
        self.evaluation_set = EvaluationSetClient(client_wrapper=self._client_wrapper)
        self.evaluation_set_item = EvaluationSetItemClient(client_wrapper=self._client_wrapper)
        self.workflow_run_output = WorkflowRunOutputClient(client_wrapper=self._client_wrapper)
        self.batch_processor_run = BatchProcessorRunClient(client_wrapper=self._client_wrapper)
        self.workflow = WorkflowClient(client_wrapper=self._client_wrapper)

    @property
    def with_raw_response(self) -> RawExtend:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawExtend
        """
        return self._raw_client

    def run_workflow(
        self,
        *,
        workflow_id: str,
        files: typing.Optional[typing.Sequence[WorkflowRunFileInput]] = OMIT,
        raw_texts: typing.Optional[typing.Sequence[str]] = OMIT,
        version: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunWorkflowResponse:
        """
        Run a Workflow with files. A Workflow is a sequence of steps that process files and data in a specific order to achieve a desired outcome. A WorkflowRun will be created for each file processed. A WorkflowRun represents a single execution of a workflow against a file.

        Parameters
        ----------
        workflow_id : str
            The ID of the workflow to run. The ID will start with "workflow". This ID can be found viewing the workflow on the Extend platform.

            Example: `"workflow_BMdfq_yWM3sT-ZzvCnA3f"`

        files : typing.Optional[typing.Sequence[WorkflowRunFileInput]]
            An array of files to process through the workflow. Either the `files` array or `rawTexts` array must be provided. Supported file types can be found [here](https://docs.extend.ai/2025-04-21/developers/guides/supported-file-types).

        raw_texts : typing.Optional[typing.Sequence[str]]
            An array of raw strings. Can be used in place of files when passing raw data. The raw data will be converted to `.txt` files and run through the workflow. If the data follows a specific format, it is recommended to use the files parameter instead. Either `files` or `rawTexts` must be provided.

        version : typing.Optional[str]
            An optional version of the workflow that files will be run through. This number can be found when viewing the workflow on the Extend platform. When a version number is not supplied, the most recent published version of the workflow will be used. If no published versions exist, the draft version will be used. To run the `"draft"` version of a workflow, use `"draft"` as the version.

            Examples:
            - `"3"` - Run version 3 of the workflow
            - `"draft"` - Run the draft version of the workflow

        priority : typing.Optional[int]
            An optional value used to determine the relative order of WorkflowRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            A optional metadata object that can be assigned to a specific WorkflowRun to help identify it. It will be returned in the response and webhooks. You can place any arbitrary `key : value` pairs in this object.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunWorkflowResponse
            Successfully created workflow runs

        Examples
        --------
        from extend_ai import Extend
        client = Extend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
        client.run_workflow(workflow_id='workflow_id_here', )
        """
        _response = self._raw_client.run_workflow(
            workflow_id=workflow_id,
            files=files,
            raw_texts=raw_texts,
            version=version,
            priority=priority,
            metadata=metadata,
            request_options=request_options,
        )
        return _response.data

    def run_processor(
        self,
        *,
        processor_id: ProcessorId,
        version: typing.Optional[str] = OMIT,
        file: typing.Optional[ProcessorRunFileInput] = OMIT,
        raw_text: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        config: typing.Optional[RunProcessorRequestConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunProcessorResponse:
        """
        Run processors (extraction, classification, splitting, etc.) on a given document.

        In general, the recommended way to integrate with Extend in production is via workflows, using the [Run Workflow](https://docs.extend.ai/2025-04-21/developers/api-reference/workflow-endpoints/run-workflow) endpoint. This is due to several factors:
        * file parsing/pre-processing will automatically be reused across multiple processors, which will give you simplicity and cost savings given that many use cases will require multiple processors to be run on the same document.
        * workflows provide dedicated human in the loop document review, when needed.
        * workflows allow you to model and manage your pipeline with a single endpoint and corresponding UI for modeling and monitoring.

        However, there are a number of legitimate use cases and systems where it might be easier to model the pipeline via code and run processors directly. This endpoint is provided for this purpose.

        Similar to workflow runs, processor runs are asynchronous and will return a status of `PROCESSING` until the run is complete. You can [configure webhooks](https://docs.extend.ai/2025-04-21/developers/webhooks/configuration) to receive notifications when a processor run is complete or failed.

        Parameters
        ----------
        processor_id : ProcessorId

        version : typing.Optional[str]
            An optional version of the processor to use. When not supplied, the most recent published version of the processor will be used. Special values include:
            - `"latest"` for the most recent published version. If there are no published versions, the draft version will be used.
            - `"draft"` for the draft version.
            - Specific version numbers corresponding to versions your team has published, e.g. `"1.0"`, `"2.2"`, etc.

        file : typing.Optional[ProcessorRunFileInput]
            The file to be processed. One of `file` or `rawText` must be provided. Supported file types can be found [here](https://docs.extend.ai/2025-04-21/developers/guides/supported-file-types).

        raw_text : typing.Optional[str]
            A raw string to be processed. Can be used in place of file when passing raw text data streams. One of `file` or `rawText` must be provided.

        priority : typing.Optional[int]
            An optional value used to determine the relative order of ProcessorRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            An optional object that can be passed in to identify the run of the document processor. It will be returned back to you in the response and webhooks.

        config : typing.Optional[RunProcessorRequestConfig]
            The configuration for the processor run. If this is provided, this config will be used. If not provided, the config for the specific version you provide will be used. The type of configuration must match the processor type.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunProcessorResponse
            Successfully created processor run

        Examples
        --------
        from extend_ai import Extend
        client = Extend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
        client.run_processor(processor_id='processor_id_here', )
        """
        _response = self._raw_client.run_processor(
            processor_id=processor_id,
            version=version,
            file=file,
            raw_text=raw_text,
            priority=priority,
            metadata=metadata,
            config=config,
            request_options=request_options,
        )
        return _response.data

    def parse(
        self, *, file: ParseRequestFile, config: ParseConfig, request_options: typing.Optional[RequestOptions] = None
    ) -> ParseResponse:
        """
        Parse files to get cleaned, chunked target content (e.g. markdown).

        The Parse endpoint allows you to convert documents into structured, machine-readable formats with fine-grained control over the parsing process. This endpoint is ideal for extracting cleaned document content to be used as context for downstream processing, e.g. RAG pipelines, custom ingestion pipelines, embeddings classification, etc.

        Unlike processor and workflow runs, parsing is a synchronous endpoint and returns the parsed content in the response. Expected latency depends primarily on file size. This makes it suitable for workflows where you need immediate access to document content without waiting for asynchronous processing.

        For more details, see the [Parse File guide](https://docs.extend.ai/2025-04-21/developers/guides/parse).

        Parameters
        ----------
        file : ParseRequestFile
            A file object containing either a URL or a fileId.

        config : ParseConfig

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ParseResponse
            Successfully parsed file

        Examples
        --------
        from extend_ai import Extend
        from extend_ai import ParseRequestFile
        from extend_ai import ParseConfig
        client = Extend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
        client.parse(file=ParseRequestFile(), config=ParseConfig(), )
        """
        _response = self._raw_client.parse(file=file, config=config, request_options=request_options)
        return _response.data


class AsyncExtend:
    """
    Use this class to access the different functions within the SDK. You can instantiate any number of clients with different configuration that will propagate to these functions.

    Parameters
    ----------
    base_url : typing.Optional[str]
        The base url to use for requests from the client.

    environment : ExtendEnvironment
        The environment to use for requests from the client. from .environment import ExtendEnvironment

        Defaults to ExtendEnvironment.PRODUCTION



    extend_api_version : typing.Optional[ApiVersionEnum]
    token : typing.Union[str, typing.Callable[[], str]]
    timeout : typing.Optional[float]
        The timeout to be used, in seconds, for requests. By default the timeout is 60 seconds, unless a custom httpx client is used, in which case this default is not enforced.

    follow_redirects : typing.Optional[bool]
        Whether the default httpx client follows redirects or not, this is irrelevant if a custom httpx client is passed in.

    httpx_client : typing.Optional[httpx.AsyncClient]
        The httpx client to use for making requests, a preconfigured client is used by default, however this is useful should you want to pass in any custom httpx configuration.

    Examples
    --------
    from extend_ai import AsyncExtend
    client = AsyncExtend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
    """

    def __init__(
        self,
        *,
        base_url: typing.Optional[str] = None,
        environment: ExtendEnvironment = ExtendEnvironment.PRODUCTION,
        extend_api_version: typing.Optional[ApiVersionEnum] = None,
        token: typing.Union[str, typing.Callable[[], str]],
        timeout: typing.Optional[float] = None,
        follow_redirects: typing.Optional[bool] = True,
        httpx_client: typing.Optional[httpx.AsyncClient] = None,
    ):
        _defaulted_timeout = (
            timeout if timeout is not None else 60 if httpx_client is None else httpx_client.timeout.read
        )
        self._client_wrapper = AsyncClientWrapper(
            base_url=_get_base_url(base_url=base_url, environment=environment),
            extend_api_version=extend_api_version,
            token=token,
            httpx_client=httpx_client
            if httpx_client is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout, follow_redirects=follow_redirects)
            if follow_redirects is not None
            else httpx.AsyncClient(timeout=_defaulted_timeout),
            timeout=_defaulted_timeout,
        )
        self._raw_client = AsyncRawExtend(client_wrapper=self._client_wrapper)
        self.workflow_run = AsyncWorkflowRunClient(client_wrapper=self._client_wrapper)
        self.batch_workflow_run = AsyncBatchWorkflowRunClient(client_wrapper=self._client_wrapper)
        self.processor_run = AsyncProcessorRunClient(client_wrapper=self._client_wrapper)
        self.processor = AsyncProcessorClient(client_wrapper=self._client_wrapper)
        self.processor_version = AsyncProcessorVersionClient(client_wrapper=self._client_wrapper)
        self.file = AsyncFileClient(client_wrapper=self._client_wrapper)
        self.file_endpoints = AsyncFileEndpointsClient(client_wrapper=self._client_wrapper)
        self.evaluation_set = AsyncEvaluationSetClient(client_wrapper=self._client_wrapper)
        self.evaluation_set_item = AsyncEvaluationSetItemClient(client_wrapper=self._client_wrapper)
        self.workflow_run_output = AsyncWorkflowRunOutputClient(client_wrapper=self._client_wrapper)
        self.batch_processor_run = AsyncBatchProcessorRunClient(client_wrapper=self._client_wrapper)
        self.workflow = AsyncWorkflowClient(client_wrapper=self._client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawExtend:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawExtend
        """
        return self._raw_client

    async def run_workflow(
        self,
        *,
        workflow_id: str,
        files: typing.Optional[typing.Sequence[WorkflowRunFileInput]] = OMIT,
        raw_texts: typing.Optional[typing.Sequence[str]] = OMIT,
        version: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunWorkflowResponse:
        """
        Run a Workflow with files. A Workflow is a sequence of steps that process files and data in a specific order to achieve a desired outcome. A WorkflowRun will be created for each file processed. A WorkflowRun represents a single execution of a workflow against a file.

        Parameters
        ----------
        workflow_id : str
            The ID of the workflow to run. The ID will start with "workflow". This ID can be found viewing the workflow on the Extend platform.

            Example: `"workflow_BMdfq_yWM3sT-ZzvCnA3f"`

        files : typing.Optional[typing.Sequence[WorkflowRunFileInput]]
            An array of files to process through the workflow. Either the `files` array or `rawTexts` array must be provided. Supported file types can be found [here](https://docs.extend.ai/2025-04-21/developers/guides/supported-file-types).

        raw_texts : typing.Optional[typing.Sequence[str]]
            An array of raw strings. Can be used in place of files when passing raw data. The raw data will be converted to `.txt` files and run through the workflow. If the data follows a specific format, it is recommended to use the files parameter instead. Either `files` or `rawTexts` must be provided.

        version : typing.Optional[str]
            An optional version of the workflow that files will be run through. This number can be found when viewing the workflow on the Extend platform. When a version number is not supplied, the most recent published version of the workflow will be used. If no published versions exist, the draft version will be used. To run the `"draft"` version of a workflow, use `"draft"` as the version.

            Examples:
            - `"3"` - Run version 3 of the workflow
            - `"draft"` - Run the draft version of the workflow

        priority : typing.Optional[int]
            An optional value used to determine the relative order of WorkflowRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            A optional metadata object that can be assigned to a specific WorkflowRun to help identify it. It will be returned in the response and webhooks. You can place any arbitrary `key : value` pairs in this object.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunWorkflowResponse
            Successfully created workflow runs

        Examples
        --------
        from extend_ai import AsyncExtend
        import asyncio
        client = AsyncExtend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
        async def main() -> None:
            await client.run_workflow(workflow_id='workflow_id_here', )
        asyncio.run(main())
        """
        _response = await self._raw_client.run_workflow(
            workflow_id=workflow_id,
            files=files,
            raw_texts=raw_texts,
            version=version,
            priority=priority,
            metadata=metadata,
            request_options=request_options,
        )
        return _response.data

    async def run_processor(
        self,
        *,
        processor_id: ProcessorId,
        version: typing.Optional[str] = OMIT,
        file: typing.Optional[ProcessorRunFileInput] = OMIT,
        raw_text: typing.Optional[str] = OMIT,
        priority: typing.Optional[int] = OMIT,
        metadata: typing.Optional[JsonObject] = OMIT,
        config: typing.Optional[RunProcessorRequestConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RunProcessorResponse:
        """
        Run processors (extraction, classification, splitting, etc.) on a given document.

        In general, the recommended way to integrate with Extend in production is via workflows, using the [Run Workflow](https://docs.extend.ai/2025-04-21/developers/api-reference/workflow-endpoints/run-workflow) endpoint. This is due to several factors:
        * file parsing/pre-processing will automatically be reused across multiple processors, which will give you simplicity and cost savings given that many use cases will require multiple processors to be run on the same document.
        * workflows provide dedicated human in the loop document review, when needed.
        * workflows allow you to model and manage your pipeline with a single endpoint and corresponding UI for modeling and monitoring.

        However, there are a number of legitimate use cases and systems where it might be easier to model the pipeline via code and run processors directly. This endpoint is provided for this purpose.

        Similar to workflow runs, processor runs are asynchronous and will return a status of `PROCESSING` until the run is complete. You can [configure webhooks](https://docs.extend.ai/2025-04-21/developers/webhooks/configuration) to receive notifications when a processor run is complete or failed.

        Parameters
        ----------
        processor_id : ProcessorId

        version : typing.Optional[str]
            An optional version of the processor to use. When not supplied, the most recent published version of the processor will be used. Special values include:
            - `"latest"` for the most recent published version. If there are no published versions, the draft version will be used.
            - `"draft"` for the draft version.
            - Specific version numbers corresponding to versions your team has published, e.g. `"1.0"`, `"2.2"`, etc.

        file : typing.Optional[ProcessorRunFileInput]
            The file to be processed. One of `file` or `rawText` must be provided. Supported file types can be found [here](https://docs.extend.ai/2025-04-21/developers/guides/supported-file-types).

        raw_text : typing.Optional[str]
            A raw string to be processed. Can be used in place of file when passing raw text data streams. One of `file` or `rawText` must be provided.

        priority : typing.Optional[int]
            An optional value used to determine the relative order of ProcessorRuns when rate limiting is in effect. Lower values will be prioritized before higher values.

        metadata : typing.Optional[JsonObject]
            An optional object that can be passed in to identify the run of the document processor. It will be returned back to you in the response and webhooks.

        config : typing.Optional[RunProcessorRequestConfig]
            The configuration for the processor run. If this is provided, this config will be used. If not provided, the config for the specific version you provide will be used. The type of configuration must match the processor type.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RunProcessorResponse
            Successfully created processor run

        Examples
        --------
        from extend_ai import AsyncExtend
        import asyncio
        client = AsyncExtend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
        async def main() -> None:
            await client.run_processor(processor_id='processor_id_here', )
        asyncio.run(main())
        """
        _response = await self._raw_client.run_processor(
            processor_id=processor_id,
            version=version,
            file=file,
            raw_text=raw_text,
            priority=priority,
            metadata=metadata,
            config=config,
            request_options=request_options,
        )
        return _response.data

    async def parse(
        self, *, file: ParseRequestFile, config: ParseConfig, request_options: typing.Optional[RequestOptions] = None
    ) -> ParseResponse:
        """
        Parse files to get cleaned, chunked target content (e.g. markdown).

        The Parse endpoint allows you to convert documents into structured, machine-readable formats with fine-grained control over the parsing process. This endpoint is ideal for extracting cleaned document content to be used as context for downstream processing, e.g. RAG pipelines, custom ingestion pipelines, embeddings classification, etc.

        Unlike processor and workflow runs, parsing is a synchronous endpoint and returns the parsed content in the response. Expected latency depends primarily on file size. This makes it suitable for workflows where you need immediate access to document content without waiting for asynchronous processing.

        For more details, see the [Parse File guide](https://docs.extend.ai/2025-04-21/developers/guides/parse).

        Parameters
        ----------
        file : ParseRequestFile
            A file object containing either a URL or a fileId.

        config : ParseConfig

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ParseResponse
            Successfully parsed file

        Examples
        --------
        from extend_ai import AsyncExtend
        from extend_ai import ParseRequestFile
        from extend_ai import ParseConfig
        import asyncio
        client = AsyncExtend(extend_api_version="YOUR_EXTEND_API_VERSION", token="YOUR_TOKEN", )
        async def main() -> None:
            await client.parse(file=ParseRequestFile(), config=ParseConfig(), )
        asyncio.run(main())
        """
        _response = await self._raw_client.parse(file=file, config=config, request_options=request_options)
        return _response.data


def _get_base_url(*, base_url: typing.Optional[str] = None, environment: ExtendEnvironment) -> str:
    if base_url is not None:
        return base_url
    elif environment is not None:
        return environment.value
    else:
        raise Exception("Please pass in either base_url or environment to construct the client")
