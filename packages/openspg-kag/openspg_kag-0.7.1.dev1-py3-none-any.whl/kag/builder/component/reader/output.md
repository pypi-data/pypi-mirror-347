# 算法岗八股Transformer篇之注意力

自注意力Self attention公式：

Atten = softmax(\frac{QK^T}{\sqrt{d}})V

## 为什么attention中要除以根号d?

答：

当我们计算点积注意力时，输入序列的每个元素都会与其他元素进行点积运算，得到一个分数。这个分数在经过softmax函数后，会得到一个注意力权重。然而，当输入序列较长时，点积的结果可能会非常大，导致softmax函数在计算时出现梯度爆炸或梯度消失的问题。为了解决这个问题，我们通常会使用一个缩放因子来缩放这些分数，使其变得更加平滑。

而除以根号d的操作就是这个缩放因子的一种实现方式。其中，d是输入序列中每个元素的维度大小。通过除以根号d，我们可以将点积的结果标准化，使其不会因为输入序列的长度或维度而变得过大或过小。这样，softmax函数在计算注意力权重时就能更加稳定地工作，避免梯度爆炸或梯度消失的问题。

简答版：对点积结果进行正则化，防止点积结果过大导致softmax出现梯度爆炸或消失。

## Transformer为什么Q,K使用不同的权重矩阵生成，而不是进行自身的点乘？





## 多头注意力机制的作用是什么？

答：多头注意力通过并行运行多个Self-Attention层并综合其结果，能够同时捕捉输入序列在不同子空间中的信息，从而增强模型的表达能力。Multi-Head Attention通过不同的 W_{Q},W_{K},W_{V}
把X映射到不同子空间，实际上MHSA做了多个并行的Self-Attention运算，每个“头”都独立地学习不同的注意力权重。再将这些头的输出拼接并线性变换，从而实现在不同表示子空间中同时捕获和整合多种交互信息，提升模型的表达能力。

## Self-Attention和Multi-Head Attention的对比

Self-Attention关注序列内每个位置对其他所有位置的重要性

Multi-Head Attention则通过在多个子空间中并行计算注意力，使模型能够同时捕获和整合不同空间的上下文信息，从而增强了对复杂数据内在结构的建模能力。

## KV cache原理

KV Cache（键-值缓存）是一种在大模型推理中广泛应用的优化技术，其核心思想是利用缓存 key 和 value 来避免重复计算，从而提高推理效率。代价是显存占用会增加。

kvcache一句话来说就是把每个token在过Transformer时乘以W_K,W_V这俩参数矩阵得到的key，value缓存下来。

![Image](GZimbb9OLorwMXxJavfctCjunwd)

- Key和Value向量的计算及缓存：
在自注意力机制中，给定一个输入序列，模型会为每个token计算key和value向量。这些向量在序列生成过程中是固定的，不会随着新token的生成而改变。因此，当我们在多次迭代或推理过程中，可以缓存这些key和value向量，以便在后续步骤中直接使用，而不需要每次都重新计算。这种缓存策略可以显著提高计算效率。
- Decoder的推理过程：
Decoder在每次推理时只输出一个token。这个输出的token会与之前的输入tokens拼接在一起，形成新的输入序列，并作为下一次推理的输入。这个过程会一直持续，直到遇到终止符为止。
- Embedding层与Logits层：
在每一步中，输入的token序列首先会经过Embedding层，将每个token转换为固定大小的向量表示。这个向量表示（即三维张量[b, s, h]）随后会经过一系列的计算层，最终通过logits层将计算结果映射到词表空间，输出一个张量。这个张量的维度是[b, s, vocab_size]，表示batch中每个序列的每个位置上可能的词汇表大小。
- 第i+1轮的输入数据相比于第i轮新增了一个token，但其他部分是相同的。这意味着第i轮中已经计算过的key和value向量在第i+1轮仍然是可用的。KV Cache的出发点就是利用这种重复性，缓存可重复利用的计算结果，以便在下一轮计算时直接读取，从而避免不必要的重复计算。
  ## 为什么有KV cache没有Q cache?

  1. 计算效率：在自注意力层的计算中，每个token都会产生一个query向量和一个与之对应的key-value对。由于query是根据当前token的输入直接计算得出的，其值会随着输入序列的改变而改变。而key和value则是根据整个输入序列计算得到的，它们在整个推理过程中是固定的。因此，在多次推理步骤中，我们往往需要重新计算query向量，而key和value对则可以在不同的推理步骤中重复使用。
  1. 模型设计：Transformer模型的设计使得key和value向量在整个推理过程中是可复用的。因此，通过缓存这些向量，可以显著减少计算量并加速推理过程。而query向量由于其动态性，不适合进行缓存。
  1. 内存与计算权衡：缓存机制需要权衡内存和计算效率。由于query向量的数量与序列长度成正比，如果缓存query向量，可能会导致内存消耗巨大。而key和value向量由于在整个推理过程中是固定的，其缓存成本更加可控。
  1. 系统实现复杂性：从系统实现的角度来看，仅实现KV Cache可以简化模型的复杂性。实现一个有效的Q Cache可能需要额外的逻辑来管理查询的缓存和检索机制，这可能会增加实现的复杂性和潜在的错误来源。









