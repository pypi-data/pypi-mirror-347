Metadata-Version: 2.4
Name: asflow
Version: 0.0.2
Summary: A lightweight, asynchronous workflow runner for ETL pipelines
Project-URL: Documentation, https://github.com/k24d/asflow/docs
Project-URL: Repository, https://github.com/k24d/asflow
License-Expression: MIT
License-File: LICENSE
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: >=3.11
Requires-Dist: rich>=14.0.0
Description-Content-Type: text/markdown

# AsFlow

**AsFlow** (short for "Async workflow") is a lightweight, asynchronous workflow runner built in pure Python. It's designed for ETL (Extractâ€“Transformâ€“Load) pipelines with a focus on minimal setup, fast iteration, and seamless integration with tools like [Streamlit](https://streamlit.io).

![ScreenRecording](https://raw.githubusercontent.com/k24d/asflow/main/docs/assets/img/ScreenRecording.webp)

### Key Features

- ğŸ Pure Python, single-process â€” no external scheduler or services required
- âš™ï¸ Asynchronous by design â€” built on `asyncio` for parallel, non-blocking execution
- ğŸ“Š Rich console output â€” powered by [Rich](https://rich.readthedocs.io) for clean logs and progress bars
- ğŸ”„ Built for data workflows â€” integrates naturally with [Polars](https://pola.rs) and [Joblib](https://joblib.readthedocs.io)

## Installation

```
% pip install asflow
```

## Quick Guide

AsFlow allows you to define ETL workflows using regular `async` Python functions. All you have to do is decorate your functions with `@flow` and `@flow.task`. Everything else works just like standard Python.

Hereâ€™s a simple example:

```python
import asyncio
import polars as pl
from asflow import flow

# Extract: simulate saving raw data
@flow.task(on="words/*.jsonl.gz")
async def extract(word):
    await asyncio.sleep(1)  # Simulate a slow operation
    flow.task.write({"word": word, "count": 1})

# Transform: load the raw data into a DataFrame
@flow.task
def transform():
    return pl.read_ndjson("words/*.jsonl.gz")

# Main workflow
@flow(verbose=True)
async def main():
    words = ["Hello", "World"]

    # Run extractions in parallel
    async with asyncio.TaskGroup() as tg:
        for word in words:
            tg.create_task(extract(word))

    print(transform())

if __name__ == "__main__":
    asyncio.run(main())
```

When you run this script:

```
% python main.py
[12:34:56] Task extract('Hello') finished in 1.00s
           Task extract('World') finished in 1.01s
           Task transform() finished in 0.00s
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ word  â”† count â”‚
â”‚ ---   â”† ---   â”‚
â”‚ str   â”† i64   â”‚
â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡
â”‚ Hello â”† 1     â”‚
â”‚ World â”† 1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Whatâ€™s Going On?

- The `extract()` function simulates downloading or generating raw data. Itâ€™s asynchronous, so multiple tasks can run in parallel.
- The `transform()` function loads all the saved data into a Polars DataFrame for further analysis.
- The raw data is saved to files (e.g., `words/*.jsonl.gz`), so tasks wonâ€™t re-run if the data already existsâ€”making your pipeline more stable and efficient.

## Documentation

- [Getting Started with AsFlow](docs/index.md)
