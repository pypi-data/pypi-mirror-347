Metadata-Version: 2.4
Name: asflow
Version: 0.0.2
Summary: A lightweight, asynchronous workflow runner for ETL pipelines
Project-URL: Documentation, https://github.com/k24d/asflow/docs
Project-URL: Repository, https://github.com/k24d/asflow
License-Expression: MIT
License-File: LICENSE
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: >=3.11
Requires-Dist: rich>=14.0.0
Description-Content-Type: text/markdown

# AsFlow

**AsFlow** (short for "Async workflow") is a lightweight, asynchronous workflow runner built in pure Python. It's designed for ETL (Extract–Transform–Load) pipelines with a focus on minimal setup, fast iteration, and seamless integration with tools like [Streamlit](https://streamlit.io).

![ScreenRecording](https://raw.githubusercontent.com/k24d/asflow/main/docs/assets/img/ScreenRecording.webp)

### Key Features

- 🐍 Pure Python, single-process — no external scheduler or services required
- ⚙️ Asynchronous by design — built on `asyncio` for parallel, non-blocking execution
- 📊 Rich console output — powered by [Rich](https://rich.readthedocs.io) for clean logs and progress bars
- 🔄 Built for data workflows — integrates naturally with [Polars](https://pola.rs) and [Joblib](https://joblib.readthedocs.io)

## Installation

```
% pip install asflow
```

## Quick Guide

AsFlow allows you to define ETL workflows using regular `async` Python functions. All you have to do is decorate your functions with `@flow` and `@flow.task`. Everything else works just like standard Python.

Here’s a simple example:

```python
import asyncio
import polars as pl
from asflow import flow

# Extract: simulate saving raw data
@flow.task(on="words/*.jsonl.gz")
async def extract(word):
    await asyncio.sleep(1)  # Simulate a slow operation
    flow.task.write({"word": word, "count": 1})

# Transform: load the raw data into a DataFrame
@flow.task
def transform():
    return pl.read_ndjson("words/*.jsonl.gz")

# Main workflow
@flow(verbose=True)
async def main():
    words = ["Hello", "World"]

    # Run extractions in parallel
    async with asyncio.TaskGroup() as tg:
        for word in words:
            tg.create_task(extract(word))

    print(transform())

if __name__ == "__main__":
    asyncio.run(main())
```

When you run this script:

```
% python main.py
[12:34:56] Task extract('Hello') finished in 1.00s
           Task extract('World') finished in 1.01s
           Task transform() finished in 0.00s
shape: (2, 2)
┌───────┬───────┐
│ word  ┆ count │
│ ---   ┆ ---   │
│ str   ┆ i64   │
╞═══════╪═══════╡
│ Hello ┆ 1     │
│ World ┆ 1     │
└───────┴───────┘
```

### What’s Going On?

- The `extract()` function simulates downloading or generating raw data. It’s asynchronous, so multiple tasks can run in parallel.
- The `transform()` function loads all the saved data into a Polars DataFrame for further analysis.
- The raw data is saved to files (e.g., `words/*.jsonl.gz`), so tasks won’t re-run if the data already exists—making your pipeline more stable and efficient.

## Documentation

- [Getting Started with AsFlow](docs/index.md)
