print("in some cases there will be some warning in the beginning dont worry about that until execution is completed")
print("replace the txt file path ")
import numpy as np 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense 
from tensorflow.keras.layers import LSTM 
import tensorflow as tf 
EPOCHS = 2
BATCH_SIZE = 256 
INPUT_FILE_NAME = '/kaggle/input/text-data/eg.txt' 
WINDOW_LENGTH = 40 
WINDOW_STEP = 3 
import logging 
tf.get_logger( ).setLevel(logging.ERROR)        
# Open the input file. 
file = open(INPUT_FILE_NAME, 'r', encoding='utf-8') 
text = file.read( ) 
file.close( ) 
# Make lowercase and remove newline and extra spaces. 
text = text.lower( ) 
text = text.replace('\n', '  ') 
text = text.replace('   ', ' ') 
# Encode characters as indices. 
unique_chars = list(set(text)) 
char_to_index = dict((ch, index) for index, ch in enumerate(unique_chars)) 
index_to_char = dict((index, ch) for index, ch in enumerate(unique_chars)) 
encoding_width = len(char_to_index) 
# Create training examples. 
fragments = [ ] 
targets = [ ] 
for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP): 
 fragments.append(text[i: i + WINDOW_LENGTH]) 
 targets.append(text[i + WINDOW_LENGTH]) 
# Convert to one-hot encoded training data. 
X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width)) 
y = np.zeros((len(fragments), encoding_width)) 
for i, fragment in enumerate(fragments): 
 for j, char in enumerate(fragment): 
  X[i, j, char_to_index[char]] = 1 
 target_char = targets[i] 
 y[i, char_to_index[target_char]] = 1 
# Build and train model. 
model = Sequential( ) 
model.add(LSTM(128, return_sequences=True, 
dropout=0.2, recurrent_dropout=0.2, 
input_shape=(None, encoding_width))) 
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) 
model.add(Dense(encoding_width, activation='softmax')) 
model.compile(loss='categorical_crossentropy', optimizer='adam') 
model.summary( ) 
history = model.fit(X, y, validation_split=0.05, batch_size=BATCH_SIZE,epochs=EPOCHS, 
verbose=2, 
shuffle=True) # Original input (43 characters) 
input_seq = 'the body was found near the lake. it was co' 
# Trim to last 40 characters 
input_seq = input_seq[-40:] 
# Confirm correct length 
assert len(input_seq) == WINDOW_LENGTH 
# One-hot encode the input sequence 
input_tensor = np.zeros((1, WINDOW_LENGTH, encoding_width)) 
for t, char in enumerate(input_seq): 
 if char in char_to_index: 
  input_tensor[0, t, char_to_index[char]] = 1 
 else: 
  print(f"Warning: Character '{char}' not in vocabulary.") 
# Predict the next character 
pred = model.predict(input_tensor, verbose=0) 
next_char_index = np.argmax(pred[0]) 
next_char = index_to_char[next_char_index] 
print("Predicted next character:", next_char ,"is it")