# Elijah Cole
seed_everything: 42
data:
  class_path: modelgenerator.data.ClockDataModule
  init_args:
    path: '/workspace/modelgenerator/cell-downstream-tasks/clocks/'
    batch_size: 16 # On 1xH100, this can be 4 for the 100M model and 16 for the 3M or 10M models. 
    train_split_files: 
      - 'blood/effector_memory_CD8-positive_alpha-beta_T_cell.h5ad'
    split_column: 'split'
    filter_columns:
      - 'numeric_age'
    rename_columns:
      - 'labels'
    label_scaling: 'z_scaling'
model:
  class_path: modelgenerator.tasks.SequenceRegression
  init_args:
    use_legacy_adapter: false
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 1e-4
        weight_decay: 0.01
        betas: [0.9, 0.95]
    lr_scheduler:
      class_path: modelgenerator.lr_schedulers.CosineWithWarmup
      init_args:
        warmup_ratio: 0.01
    backbone:
      class_path: modelgenerator.backbones.aido_cell_3m
      init_args:
        from_scratch: False
        frozen: False
    loss_func:
      class_path: torch.nn.L1Loss
    adapter:
      class_path: modelgenerator.adapters.LinearMeanPoolAdapter
trainer:
  limit_train_batches: 128
  log_every_n_steps: 10
  accumulate_grad_batches: 4
  precision: bf16
  devices: auto
  max_epochs: 50
  gradient_clip_val: 0
  profiler: null
  callbacks: 
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      dict_kwargs:
        logging_interval: "step"
    # Save a checkpoint for max val f1:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      dict_kwargs:
        monitor: val_spearman
        mode: max
        save_top_k: 1
        filename: "best_val_spearman:{step}-{val_spearman:.3f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      dict_kwargs:
        monitor: train_loss
        mode: min
        save_top_k: 1
        filename: "best_train_loss:{step}-{train_loss:.3f}"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      dict_kwargs:
        monitor: val_loss
        mode: min
        save_top_k: 1
        filename: "best_val_loss:{step}-{val_loss:.3f}"
    # Save latest:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      dict_kwargs:
        filename: "latest:{step}"
  default_root_dir: '/workspace/modelgenerator/logs'
  strategy:
    class_path: lightning.pytorch.strategies.DDPStrategy
  logger: 
    class_path: lightning.pytorch.loggers.wandb.WandbLogger
    init_args:
      name: "default"
      save_dir: "logs"
      project: "mgen-clocks"
      save_code: true
