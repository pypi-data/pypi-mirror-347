trainer:
  accelerator: auto
  devices: auto
  max_epochs: 30
  gradient_clip_val: 1
  default_root_dir: logs
  logger: false
  callbacks:
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint # save ckpt at the end of each epoch, and save the best val_mcc ckpt
    init_args:
      dirpath: null
      filename: epoch_{epoch}-val_mcc:{val_mcc:.3f}
      monitor: val_mcc
      mode: max
      every_n_epochs: 1
  - class_path: lightning.pytorch.callbacks.early_stopping.EarlyStopping
    dict_kwargs:
      monitor: val_mcc
      mode: max
      patience: 30
model:
  class_path: modelgenerator.tasks.SequenceClassification
  init_args:
    backbone:
      class_path: modelgenerator.backbones.aido_dna_7b
      init_args:
        use_peft: true
        lora_r: 16
        lora_alpha: 32
        lora_dropout: 0.1
        lora_target_modules:
        - query
        - value
    adapter: modelgenerator.adapters.LinearCLSAdapter
    n_classes: 2
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.001
        weight_decay: 0.1
    lr_scheduler:
      class_path: modelgenerator.lr_schedulers.CosineWithWarmup
      init_args:
        warmup_ratio: 0.1
data:
  class_path: modelgenerator.data.NTClassification
  init_args:
    config_name: enhancers
    train_split_name: train
    test_split_name: test
    valid_split_size: 0.1
    batch_size: 8
