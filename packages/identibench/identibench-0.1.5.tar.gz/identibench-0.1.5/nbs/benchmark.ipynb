{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from collections.abc import Iterator, Callable\n",
    "from typing import Any\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from identibench.utils import get_default_data_root,_load_sequences_from_files, hdf_files_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq, test_ne# Import nbdev testing functions\n",
    "import identibench.metrics\n",
    "from identibench.utils import _dummy_dataset_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def aggregate_metric_score(test_results,metric_func, score_name=None ,sequence_aggregation_func=np.mean,window_aggregation_func=np.mean):\n",
    "    # iterate over test_results and calculate metric score for each (y_pred,y_test) tuple, if prediction, iterate over nested tuples with nested loop\n",
    "    if score_name is None:\n",
    "        score_name = metric_func.__name__\n",
    "    if isinstance(test_results[0], list):\n",
    "        scores = []\n",
    "        for windowed_sequence in test_results:\n",
    "            scores.append(window_aggregation_func([metric_func(y_pred,y_test) for y_pred,y_test in windowed_sequence]))\n",
    "    else:\n",
    "        scores = [[metric_func(y_pred,y_test) for y_pred,y_test in test_results]]\n",
    "    return {score_name: sequence_aggregation_func(scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class BenchmarkSpecBase: pass \n",
    "class BenchmarkSpecBase:\n",
    "    \"\"\"\n",
    "    Base class for benchmark specifications, holding common attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str, # Unique name identifying this benchmark task.\n",
    "                 dataset_id: str, # Identifier for the raw dataset source.\n",
    "                 u_cols: list[str], # list of column names for input signals (u).\n",
    "                 y_cols: list[str], # list of column names for output signals (y).\n",
    "                 metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "                 x_cols: list[str]|None = None, # Optional state inputs (x).\n",
    "                 sampling_time: float|None = None, # Optional sampling time (seconds).\n",
    "                 download_func: Callable[[Path, bool], None]|None = None, # Dataset preparation func.\n",
    "                 test_model_func: Callable[[BenchmarkSpecBase, Callable], dict[str, Any]] = None,\n",
    "                 custom_test_evaluation = None, \n",
    "                 init_window: int|None = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "                 data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "                ):\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_model_func = test_model_func\n",
    "        self.custom_test_evaluation = custom_test_evaluation\n",
    "        self.init_window = init_window\n",
    "        self._data_root = data_root\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        \"\"\"Returns the evaluated data root path.\"\"\"\n",
    "        if isinstance(self._data_root, Callable):\n",
    "            return self._data_root()\n",
    "        return self._data_root\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the dataset directory.\"\"\"\n",
    "        return self.data_root / self.dataset_id\n",
    "    \n",
    "    @property\n",
    "    def test_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the test files .\"\"\"\n",
    "        return self.dataset_path / 'test'\n",
    "    \n",
    "    @property\n",
    "    def train_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the train files.\"\"\"\n",
    "        return self.dataset_path / 'train'\n",
    "    \n",
    "    @property\n",
    "    def valid_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the valid files.\"\"\"\n",
    "        return self.dataset_path / 'valid'\n",
    "    \n",
    "    @property\n",
    "    def train_valid_path(self) -> Path:\n",
    "        \"\"\"Returns the full path to the directory containing the train_valid files.\"\"\"\n",
    "        return self.dataset_path / 'train_valid'\n",
    "    \n",
    "    @property\n",
    "    def train_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the training directory.\"\"\"\n",
    "        return hdf_files_from_path(self.train_path)\n",
    "    \n",
    "    @property\n",
    "    def valid_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the validation directory.\"\"\"\n",
    "        return hdf_files_from_path(self.valid_path)\n",
    "    \n",
    "    @property\n",
    "    def train_valid_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the train_valid directory if it exists, otherwise returns the union of the train and valid directories.\"\"\"\n",
    "        train_valid_files = hdf_files_from_path(self.train_valid_path)\n",
    "        if train_valid_files: \n",
    "            return train_valid_files\n",
    "        else:\n",
    "            train_files = hdf_files_from_path(self.train_path)\n",
    "            valid_files = hdf_files_from_path(self.valid_path)\n",
    "            return sorted(train_files+valid_files)\n",
    "\n",
    "    @property\n",
    "    def test_files(self) -> list[Path]:\n",
    "        \"\"\"Returns the list of hdf5 files in the test directory.\"\"\"\n",
    "        return hdf_files_from_path(self.test_path)\n",
    "\n",
    "    def ensure_dataset_exists(self, force_download: bool = False) -> None:\n",
    "        \"\"\"Checks if the dataset exists, downloads/prepares it if needed.\"\"\"\n",
    "        # (Implementation remains the same as before)\n",
    "        dataset_path = self.dataset_path\n",
    "        if self.download_func is None:\n",
    "            print(f\"Warning: No download function for '{self.name}'. Assuming data exists at {dataset_path}\")\n",
    "            if not dataset_path.is_dir():\n",
    "                 print(f\"Warning: Dataset directory {dataset_path} not found.\")\n",
    "            return\n",
    "\n",
    "        if not dataset_path.is_dir() or force_download:\n",
    "            print(f\"Preparing dataset for '{self.name}' at {dataset_path}...\")\n",
    "            self.data_root.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                self.download_func(dataset_path, force_download)\n",
    "                print(f\"Dataset '{self.name}' prepared successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparing dataset '{self.name}': {e}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export    \n",
    "def _test_simulation(specs, model):\n",
    "    results = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(specs.test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        y_pred = model(u_test,y_test[:specs.init_window])\n",
    "        y_test_win = y_test[specs.init_window:]\n",
    "        y_pred = y_pred[-y_test_win.shape[0]:]\n",
    "        results.append((y_pred,y_test_win))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecSimulation(BenchmarkSpecBase):\n",
    "    \"\"\"\n",
    "    Specification for a simulation benchmark task.\n",
    "\n",
    "    Inherits common parameters from BaseBenchmarkSpec.\n",
    "    Use this when the goal is to simulate the system's output given the input `u`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                name: str, # Unique name identifying this benchmark task.\n",
    "                dataset_id: str, # Identifier for the raw dataset source.\n",
    "                u_cols: list[str], # list of column names for input signals (u).\n",
    "                y_cols: list[str], # list of column names for output signals (y).\n",
    "                metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "                x_cols: list[str]|None = None, # Optional state inputs (x).\n",
    "                sampling_time: float|None = None, # Optional sampling time (seconds).\n",
    "                download_func: Callable[[Path, bool], None]|None = None, # Dataset preparation func.\n",
    "                test_model_func: Callable[[BenchmarkSpecBase, Callable], dict[str, Any]] = _test_simulation,\n",
    "                custom_test_evaluation = None, \n",
    "                init_window: int|None = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "                data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "            ):\n",
    "        self.name = name\n",
    "        self.dataset_id = dataset_id\n",
    "        self.u_cols = u_cols\n",
    "        self.y_cols = y_cols\n",
    "        self.metric_func = metric_func\n",
    "        self.x_cols = x_cols\n",
    "        self.sampling_time = sampling_time\n",
    "        self.download_func = download_func\n",
    "        self.test_model_func = test_model_func\n",
    "        self.custom_test_evaluation = custom_test_evaluation\n",
    "        self.init_window = init_window\n",
    "        self._data_root = data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti    \n",
    "def _test_prediction(specs, model):\n",
    "    results = []\n",
    "    for u_test, y_test, _ in _load_sequences_from_files(specs.test_files, specs.u_cols, specs.y_cols, specs.x_cols):\n",
    "        #iterate through windows of u_test and y_test\n",
    "        window_results = []\n",
    "        for i in range(0, u_test.shape[0] - specs.init_window - specs.pred_horizon, specs.pred_step):\n",
    "            u_test_win = u_test[i:i+specs.init_window+specs.pred_horizon]\n",
    "            y_test_win = y_test[i:i+specs.init_window+specs.pred_horizon]\n",
    "            y_pred = model(u_test_win,y_test_win[:specs.init_window])\n",
    "            window_results.append((y_pred,y_test_win))\n",
    "        results.append(window_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkSpecPrediction(BenchmarkSpecBase):\n",
    "     \"\"\"\n",
    "     Specification for a k-step ahead prediction benchmark task.\n",
    "\n",
    "     Inherits common parameters from BaseBenchmarkSpec and adds prediction-specific ones.\n",
    "     Use this when the goal is to predict `y` some steps ahead based on past `u` and `y`.\n",
    "     \"\"\"\n",
    "     def __init__(self,\n",
    "               name: str, # Unique name identifying this benchmark task.\n",
    "               dataset_id: str, # Identifier for the raw dataset source.\n",
    "               u_cols: list[str], # list of column names for input signals (u).\n",
    "               y_cols: list[str], # list of column names for output signals (y).\n",
    "               metric_func: Callable[[np.ndarray, np.ndarray], float], # Primary metric: `func(y_true, y_pred)`.\n",
    "               pred_horizon: int, # The 'k' in k-step ahead prediction (mandatory for this type).\n",
    "               pred_step: int, # Step size for k-step ahead prediction (e.g., predict y[t+k] using data up to t).\n",
    "               x_cols: list[str]|None = None, # Optional state inputs (x).\n",
    "               sampling_time: float|None = None, # Optional sampling time (seconds).\n",
    "               download_func: Callable[[Path, bool], None]|None = None, # Dataset preparation func.\n",
    "               test_model_func: Callable[[BenchmarkSpecBase, Callable], dict[str, Any]] = _test_prediction,\n",
    "               custom_test_evaluation = None, \n",
    "               init_window: int|None = None, # Steps for warm-up, potentially ignored in evaluation.\n",
    "               data_root: [Path, Callable[[], Path]] = get_default_data_root # root dir for dataset, may be a callable or path\n",
    "          ):\n",
    "          if pred_horizon <= 0:\n",
    "               raise ValueError(\"pred_horizon must be a positive integer for PredictionBenchmarkSpec.\")\n",
    "          self.pred_horizon = pred_horizon\n",
    "          self.pred_step = pred_step\n",
    "          \n",
    "          self.name = name\n",
    "          self.dataset_id = dataset_id\n",
    "          self.u_cols = u_cols\n",
    "          self.y_cols = y_cols\n",
    "          self.metric_func = metric_func\n",
    "          self.x_cols = x_cols\n",
    "          self.sampling_time = sampling_time\n",
    "          self.download_func = download_func\n",
    "          self.test_model_func = test_model_func\n",
    "          self.custom_test_evaluation = custom_test_evaluation\n",
    "          self.init_window = init_window\n",
    "          self._data_root = data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec basic initialization and defaults\n",
    "_spec_sim = BenchmarkSpecSimulation(\n",
    "    name='_spec_default', dataset_id='_dummy_default',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader\n",
    ")\n",
    "test_eq(_spec_sim.init_window, None)\n",
    "test_eq(_spec_sim.name, '_spec_default') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec initialization with prediction-related parameters\n",
    "_spec_pred = BenchmarkSpecPrediction(\n",
    "    name='_spec_pred_params', dataset_id='_dummy_pred_params',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader, \n",
    "    init_window=20, pred_horizon=5, pred_step=2\n",
    ")\n",
    "test_eq(_spec_pred.init_window, 20)\n",
    "test_eq(_spec_pred.pred_horizon, 5)\n",
    "test_eq(_spec_pred.pred_step, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - first call (creation)\n",
    "_spec_ensure = BenchmarkSpecSimulation(\n",
    "    name='_spec_ensure', dataset_id='_dummy_ensure',\n",
    "    u_cols=['u0'], y_cols=['y0'], metric_func=identibench.metrics.rmse, \n",
    "    download_func=_dummy_dataset_loader\n",
    ")\n",
    "_spec_ensure.ensure_dataset_exists()\n",
    "_dataset_path_ensure = _spec_ensure.dataset_path\n",
    "test_eq(_dataset_path_ensure.is_dir(), True)\n",
    "test_eq((_dataset_path_ensure / 'train' / 'train_0.hdf5').is_file(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - second call (skip)\n",
    "_mtime_before_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists() \n",
    "_mtime_after_skip = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_eq(_mtime_before_skip, _mtime_after_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for '_spec_ensure' at /Users/daniel/.identibench_data/_dummy_ensure...\n",
      "Dataset '_spec_ensure' prepared successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test: BenchmarkSpec ensure_dataset_exists - third call (force_download=True)\n",
    "_mtime_before_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "time.sleep(0.1) \n",
    "_spec_ensure.ensure_dataset_exists(force_download=True) \n",
    "_mtime_after_force = (_dataset_path_ensure / 'train' / 'train_0.hdf5').stat().st_mtime\n",
    "test_ne(_mtime_before_force, _mtime_after_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainingContext:\n",
    "    \"\"\"\n",
    "    Context object passed to the user's training function (`build_predictor`).\n",
    "\n",
    "    Holds the benchmark specification, hyperparameters, and seed.\n",
    "    Provides methods to access the raw, full-length training and validation data sequences.\n",
    "    Windowing/batching for training must be handled within the user's `build_predictor` function.\n",
    "    \"\"\"\n",
    "    # Explicit __init__ for nbdev documentation compatibility\n",
    "    def __init__(self, \n",
    "                 spec: BenchmarkSpecBase, # The benchmark specification.\n",
    "                 hyperparameters: dict[str, Any], # User-provided dictionary containing model and training hyperparameters.\n",
    "                 seed: int|None = None # Optional random seed for reproducibility.\n",
    "                ):\n",
    "        # Standard attribute assignment\n",
    "        self.spec = spec\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.seed = seed\n",
    "\n",
    "    # --- Data Access Methods ---\n",
    "\n",
    "    def get_train_sequences(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray|None]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'train' subset.\"\"\"\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=self.spec.train_files,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_valid_sequences(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray|None]]:\n",
    "        \"\"\"Returns a lazy iterator yielding raw (u, y, x) tuples for the 'valid' subset.\"\"\"\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=self.spec.valid_files,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )\n",
    "\n",
    "    def get_train_valid_sequences(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray|None]]:\n",
    "        \"\"\"\n",
    "        Returns a lazy iterator yielding raw (u, y, x) tuples for combined training and validation.\n",
    "\n",
    "        Checks for a 'train_valid' subset directory first. If it exists, loads data from there.\n",
    "        If not, it loads data from 'train' and 'valid' subsets sequentially.\n",
    "        \"\"\"\n",
    "        return _load_sequences_from_files(\n",
    "            file_paths=self.spec.train_valid_files,\n",
    "            u_cols=self.spec.u_cols,\n",
    "            y_cols=self.spec.y_cols,\n",
    "            x_cols=self.spec.x_cols,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmark(spec, build_model, hyperparameters={}, seed=None):\n",
    "\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "    \n",
    "    results = {\n",
    "        'benchmark_name': spec.name,\n",
    "        'dataset_id': spec.dataset_id,\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'seed': seed,\n",
    "        'training_time_seconds': np.nan,\n",
    "        'test_time_seconds': np.nan,\n",
    "        'benchmark_type' : type(spec).__name__,\n",
    "        'metric_name': spec.metric_func.__name__,\n",
    "        'metric_score': np.nan,\n",
    "        'custom_scores': {},\n",
    "        'model_predictions': []\n",
    "    }\n",
    "\n",
    "    spec.ensure_dataset_exists() \n",
    "\n",
    "    context = TrainingContext(spec=spec, hyperparameters=hyperparameters, seed=seed) \n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    model = build_model(context) \n",
    "    train_end_time = time.monotonic()\n",
    "    results['training_time_seconds'] = train_end_time - train_start_time\n",
    "\n",
    "    if model is None:\n",
    "        raise RuntimeError(f\"build_model for {spec.name} did not return a model.\") \n",
    "        \n",
    "    test_start_time = time.monotonic()\n",
    "    test_results = spec.test_model_func(spec, model) # get list of (y_pred,y_test) tuples\n",
    "    test_end_time = time.monotonic()\n",
    "    results['test_time_seconds'] = test_end_time - test_start_time\n",
    "\n",
    "    results['model_predictions'] = test_results\n",
    "\n",
    "    results.update(aggregate_metric_score(test_results, spec.metric_func, score_name='metric_score'))\n",
    "    if spec.custom_test_evaluation is not None:\n",
    "        results['custom_scores'] = spec.custom_test_evaluation(test_results, spec)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# Define a very simple build_model function for the example\n",
    "def _dummy_build_model(context):\n",
    "    print(f\"Building model with spec: {context.spec.name}, seed: {context.seed}\")\n",
    "\n",
    "    def dummy_model(u_test,y_test):\n",
    "        output_dim = len(context.spec.y_cols) \n",
    "        return np.zeros((u_test.shape[0], output_dim))\n",
    "        \n",
    "    return dummy_model # Return the callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: _spec_default, seed: 138830228\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "hyperparams = {'learning_rate': 0.01, 'epochs': 5} # Example hyperparameters\n",
    "\n",
    "benchmark_results = run_benchmark(\n",
    "    spec=_spec_sim, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_name': '_spec_default',\n",
       " 'dataset_id': '_dummy_default',\n",
       " 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5},\n",
       " 'seed': 138830228,\n",
       " 'training_time_seconds': 4.279200220480561e-05,\n",
       " 'test_time_seconds': 0.0013009580434300005,\n",
       " 'benchmark_type': 'BenchmarkSpecSimulation',\n",
       " 'metric_name': 'rmse',\n",
       " 'metric_score': 0.5644842382745956,\n",
       " 'custom_scores': {}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "benchmark_results.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: _spec_pred_params, seed: 3900254360\n"
     ]
    }
   ],
   "source": [
    "# Example usage of run_benchmark\n",
    "benchmark_results = run_benchmark(\n",
    "    spec=_spec_pred, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_name': '_spec_pred_params',\n",
       " 'dataset_id': '_dummy_pred_params',\n",
       " 'hyperparameters': {'learning_rate': 0.01, 'epochs': 5},\n",
       " 'seed': 3900254360,\n",
       " 'training_time_seconds': 6.71250163577497e-05,\n",
       " 'test_time_seconds': 0.0010067080147564411,\n",
       " 'benchmark_type': 'BenchmarkSpecPrediction',\n",
       " 'metric_name': 'rmse',\n",
       " 'metric_score': 0.5594019958882623,\n",
       " 'custom_scores': {}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "benchmark_results.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evaluation(results,spec):\n",
    "    def get_max_abs_error(y_pred,y_test):\n",
    "        return np.max(np.abs(y_test - y_pred))\n",
    "    def get_max_error(y_pred,y_test):\n",
    "        return np.max(y_test - y_pred)\n",
    "\n",
    "    avg_max_abs_error = aggregate_metric_score(results, get_max_abs_error, score_name='avg_max_abs_error',sequence_aggregation_func=np.mean,window_aggregation_func=np.mean)\n",
    "    median_max_error = aggregate_metric_score(results, get_max_error, score_name='median_max_abs_error',sequence_aggregation_func=np.median,window_aggregation_func=np.median)\n",
    "    return {**avg_max_abs_error, **median_max_error}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_with_custom_test = BenchmarkSpecSimulation(\n",
    "    name=\"CustomTestExampleBench\",\n",
    "    dataset_id=\"dummy_core_data_v1\", # Same dataset ID as before\n",
    "    download_func=_dummy_dataset_loader, \n",
    "    u_cols=['u0', 'u1'], \n",
    "    y_cols=['y0'],\n",
    "    custom_test_evaluation=custom_evaluation,\n",
    "    metric_func=identibench.metrics.rmse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with spec: CustomTestExampleBench, seed: 1172241199\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark using the spec with the custom test function\n",
    "hyperparams = {'model_type': 'dummy_v2'} \n",
    "\n",
    "benchmark_results = run_benchmark(\n",
    "    spec=spec_with_custom_test, \n",
    "    build_model=_dummy_build_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_name': 'CustomTestExampleBench',\n",
       " 'dataset_id': 'dummy_core_data_v1',\n",
       " 'hyperparameters': {'model_type': 'dummy_v2'},\n",
       " 'seed': 1172241199,\n",
       " 'training_time_seconds': 2.1415995433926582e-05,\n",
       " 'test_time_seconds': 0.0015841670101508498,\n",
       " 'benchmark_type': 'BenchmarkSpecSimulation',\n",
       " 'metric_name': 'rmse',\n",
       " 'metric_score': 0.5739597924041242,\n",
       " 'custom_scores': {'avg_max_abs_error': 0.9934645593166351,\n",
       "  'median_max_abs_error': 0.9934645593166351}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "#remove the model_predictions entry from the benchmark_results before printing for cleaner documentation\n",
    "benchmark_results.pop('model_predictions')\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def benchmark_results_to_dataframe(\n",
    "    results_list: list[dict[str, Any]] # List of benchmark result dictionaries from `run_benchmark`.\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Transforms a list of benchmark result dictionaries into a pandas DataFrame.\"\"\"\n",
    "    if not results_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Flatten custom_scores if the column exists and contains dictionaries\n",
    "    if 'custom_scores' in df.columns and df['custom_scores'].apply(isinstance, args=(dict,)).any():\n",
    "        # Fill missing/non-dict entries with empty dicts for normalization\n",
    "        custom_scores_filled = df['custom_scores'].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "        custom_scores_df = pd.json_normalize(custom_scores_filled).add_prefix('cs_')\n",
    "        # Drop the original nested column and join the flattened one\n",
    "        df = pd.concat([df.drop(columns=['custom_scores']), custom_scores_df], axis=1)\n",
    "\n",
    "    # Drop the 'model_predictions' column as it's usually large and not suitable for a summary DataFrame\n",
    "    if 'model_predictions' in df.columns:\n",
    "        df = df.drop(columns=['model_predictions'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_benchmarks(\n",
    "    specs: list[BenchmarkSpecBase] | dict[str, BenchmarkSpecBase], # Collection of specs to run.\n",
    "    build_model: Callable[[TrainingContext], Callable], # User function to build the model/predictor.\n",
    "    hyperparameters: dict[str, Any] | list[dict[str, Any]] | None = None, # Single dict, list of dicts (matching specs), or None.\n",
    "    n_times: int = 1, # Number of times to repeat each benchmark specification.\n",
    "    continue_on_error: bool = True, # If True, continue running benchmarks even if one fails.\n",
    "    return_dataframe: bool = True # If True, return results as a pandas DataFrame, otherwise return a list of dicts.\n",
    ") -> pd.DataFrame | list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Runs multiple benchmarks sequentially, with repetitions and flexible hyperparameters.\n",
    "\n",
    "    Returns either a pandas DataFrame summarizing the results (default)\n",
    "    or a list of raw result dictionaries.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    spec_objects = list(specs.values()) if isinstance(specs, dict) else list(specs)\n",
    "    num_specs = len(spec_objects)\n",
    "\n",
    "    # Validate hyperparameters input\n",
    "    if isinstance(hyperparameters, list):\n",
    "        if len(hyperparameters) != num_specs:\n",
    "            raise ValueError(f\"If hyperparameters is a list, its length ({len(hyperparameters)}) must match the number of specs ({num_specs}).\")\n",
    "        get_hps = lambda i: hyperparameters[i] # Function to get hp based on spec index\n",
    "    else:\n",
    "        # If None or a single dict, use the same for all. Ensure it's a dict or None.\n",
    "        hps_single = hyperparameters or {}\n",
    "        get_hps = lambda i: hps_single # Function always returns the same hp dict\n",
    "\n",
    "    print(f\"--- Starting benchmark run for {num_specs} specifications, repeating each {n_times} times ---\")\n",
    "\n",
    "    total_runs = num_specs * n_times\n",
    "    current_run = 0\n",
    "\n",
    "    for repetition in range(n_times):\n",
    "        print(f\"\\n-- Repetition {repetition + 1}/{n_times} --\")\n",
    "        for i, spec in enumerate(spec_objects):\n",
    "            current_run += 1\n",
    "            spec_name = getattr(spec, 'name', f'Unnamed Spec {i+1}')\n",
    "            print(f\"\\n[{current_run}/{total_runs}] Running: {spec_name} (Rep {repetition + 1})\")\n",
    "\n",
    "            current_hyperparameters = get_hps(i)\n",
    "\n",
    "            try:\n",
    "                result = run_benchmark(\n",
    "                    spec=spec,\n",
    "                    build_model=build_model,\n",
    "                    hyperparameters=current_hyperparameters\n",
    "                )\n",
    "                # result['repetition'] = repetition # Add repetition number to results\n",
    "                results_list.append(result)\n",
    "                print(f\"  -> Success: {spec_name} (Rep {repetition + 1}) completed.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  -> ERROR running benchmark '{spec_name}' (Rep {repetition + 1}): {e}\")\n",
    "                if not continue_on_error:\n",
    "                    print(\"Stopping due to error (continue_on_error=False).\")\n",
    "                    raise\n",
    "\n",
    "    print(f\"\\n--- Benchmark run finished. {len(results_list)}/{total_runs} individual runs completed successfully. ---\")\n",
    "\n",
    "    if return_dataframe:\n",
    "        return benchmark_results_to_dataframe(results_list)\n",
    "    else:\n",
    "        return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting benchmark run for 3 specifications, repeating each 1 times ---\n",
      "\n",
      "-- Repetition 1/1 --\n",
      "\n",
      "[1/3] Running: _spec_default (Rep 1)\n",
      "Building model with spec: _spec_default, seed: 2979218856\n",
      "  -> Success: _spec_default (Rep 1) completed.\n",
      "\n",
      "[2/3] Running: _spec_pred_params (Rep 1)\n",
      "Building model with spec: _spec_pred_params, seed: 2767908549\n",
      "  -> Success: _spec_pred_params (Rep 1) completed.\n",
      "\n",
      "[3/3] Running: CustomTestExampleBench (Rep 1)\n",
      "Building model with spec: CustomTestExampleBench, seed: 3139743514\n",
      "  -> Success: CustomTestExampleBench (Rep 1) completed.\n",
      "\n",
      "--- Benchmark run finished. 3/3 individual runs completed successfully. ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark_name</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_time_seconds</th>\n",
       "      <th>test_time_seconds</th>\n",
       "      <th>benchmark_type</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_score</th>\n",
       "      <th>cs_avg_max_abs_error</th>\n",
       "      <th>cs_median_max_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_spec_default</td>\n",
       "      <td>_dummy_default</td>\n",
       "      <td>{}</td>\n",
       "      <td>2979218856</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_spec_pred_params</td>\n",
       "      <td>_dummy_pred_params</td>\n",
       "      <td>{}</td>\n",
       "      <td>2767908549</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>BenchmarkSpecPrediction</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.559402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CustomTestExampleBench</td>\n",
       "      <td>dummy_core_data_v1</td>\n",
       "      <td>{}</td>\n",
       "      <td>3139743514</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.573960</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.993465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           benchmark_name          dataset_id hyperparameters        seed  \\\n",
       "0           _spec_default      _dummy_default              {}  2979218856   \n",
       "1       _spec_pred_params  _dummy_pred_params              {}  2767908549   \n",
       "2  CustomTestExampleBench  dummy_core_data_v1              {}  3139743514   \n",
       "\n",
       "   training_time_seconds  test_time_seconds           benchmark_type  \\\n",
       "0               0.000006           0.001325  BenchmarkSpecSimulation   \n",
       "1               0.000006           0.000844  BenchmarkSpecPrediction   \n",
       "2               0.000005           0.000521  BenchmarkSpecSimulation   \n",
       "\n",
       "  metric_name  metric_score  cs_avg_max_abs_error  cs_median_max_abs_error  \n",
       "0        rmse      0.564484                   NaN                      NaN  \n",
       "1        rmse      0.559402                   NaN                      NaN  \n",
       "2        rmse      0.573960              0.993465                 0.993465  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_results = run_benchmarks(\n",
    "    specs=[_spec_sim,_spec_pred,spec_with_custom_test], \n",
    "    build_model=_dummy_build_model,\n",
    "    return_dataframe=False\n",
    ")\n",
    "benchmark_results_to_dataframe(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting benchmark run for 3 specifications, repeating each 3 times ---\n",
      "\n",
      "-- Repetition 1/3 --\n",
      "\n",
      "[1/9] Running: _spec_default (Rep 1)\n",
      "Building model with spec: _spec_default, seed: 30935737\n",
      "  -> Success: _spec_default (Rep 1) completed.\n",
      "\n",
      "[2/9] Running: _spec_pred_params (Rep 1)\n",
      "Building model with spec: _spec_pred_params, seed: 2986847840\n",
      "  -> Success: _spec_pred_params (Rep 1) completed.\n",
      "\n",
      "[3/9] Running: CustomTestExampleBench (Rep 1)\n",
      "Building model with spec: CustomTestExampleBench, seed: 1147267216\n",
      "  -> Success: CustomTestExampleBench (Rep 1) completed.\n",
      "\n",
      "-- Repetition 2/3 --\n",
      "\n",
      "[4/9] Running: _spec_default (Rep 2)\n",
      "Building model with spec: _spec_default, seed: 3191904871\n",
      "  -> Success: _spec_default (Rep 2) completed.\n",
      "\n",
      "[5/9] Running: _spec_pred_params (Rep 2)\n",
      "Building model with spec: _spec_pred_params, seed: 1536587039\n",
      "  -> Success: _spec_pred_params (Rep 2) completed.\n",
      "\n",
      "[6/9] Running: CustomTestExampleBench (Rep 2)\n",
      "Building model with spec: CustomTestExampleBench, seed: 3900899545\n",
      "  -> Success: CustomTestExampleBench (Rep 2) completed.\n",
      "\n",
      "-- Repetition 3/3 --\n",
      "\n",
      "[7/9] Running: _spec_default (Rep 3)\n",
      "Building model with spec: _spec_default, seed: 3797015292\n",
      "  -> Success: _spec_default (Rep 3) completed.\n",
      "\n",
      "[8/9] Running: _spec_pred_params (Rep 3)\n",
      "Building model with spec: _spec_pred_params, seed: 3789263585\n",
      "  -> Success: _spec_pred_params (Rep 3) completed.\n",
      "\n",
      "[9/9] Running: CustomTestExampleBench (Rep 3)\n",
      "Building model with spec: CustomTestExampleBench, seed: 851966748\n",
      "  -> Success: CustomTestExampleBench (Rep 3) completed.\n",
      "\n",
      "--- Benchmark run finished. 9/9 individual runs completed successfully. ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark_name</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_time_seconds</th>\n",
       "      <th>test_time_seconds</th>\n",
       "      <th>benchmark_type</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_score</th>\n",
       "      <th>cs_avg_max_abs_error</th>\n",
       "      <th>cs_median_max_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_spec_default</td>\n",
       "      <td>_dummy_default</td>\n",
       "      <td>{}</td>\n",
       "      <td>30935737</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_spec_pred_params</td>\n",
       "      <td>_dummy_pred_params</td>\n",
       "      <td>{}</td>\n",
       "      <td>2986847840</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>BenchmarkSpecPrediction</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.559402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CustomTestExampleBench</td>\n",
       "      <td>dummy_core_data_v1</td>\n",
       "      <td>{}</td>\n",
       "      <td>1147267216</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.573960</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.993465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_spec_default</td>\n",
       "      <td>_dummy_default</td>\n",
       "      <td>{}</td>\n",
       "      <td>3191904871</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_spec_pred_params</td>\n",
       "      <td>_dummy_pred_params</td>\n",
       "      <td>{}</td>\n",
       "      <td>1536587039</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>BenchmarkSpecPrediction</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.559402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CustomTestExampleBench</td>\n",
       "      <td>dummy_core_data_v1</td>\n",
       "      <td>{}</td>\n",
       "      <td>3900899545</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.573960</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.993465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_spec_default</td>\n",
       "      <td>_dummy_default</td>\n",
       "      <td>{}</td>\n",
       "      <td>3797015292</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_spec_pred_params</td>\n",
       "      <td>_dummy_pred_params</td>\n",
       "      <td>{}</td>\n",
       "      <td>3789263585</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>BenchmarkSpecPrediction</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.559402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CustomTestExampleBench</td>\n",
       "      <td>dummy_core_data_v1</td>\n",
       "      <td>{}</td>\n",
       "      <td>851966748</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse</td>\n",
       "      <td>0.573960</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.993465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           benchmark_name          dataset_id hyperparameters        seed  \\\n",
       "0           _spec_default      _dummy_default              {}    30935737   \n",
       "1       _spec_pred_params  _dummy_pred_params              {}  2986847840   \n",
       "2  CustomTestExampleBench  dummy_core_data_v1              {}  1147267216   \n",
       "3           _spec_default      _dummy_default              {}  3191904871   \n",
       "4       _spec_pred_params  _dummy_pred_params              {}  1536587039   \n",
       "5  CustomTestExampleBench  dummy_core_data_v1              {}  3900899545   \n",
       "6           _spec_default      _dummy_default              {}  3797015292   \n",
       "7       _spec_pred_params  _dummy_pred_params              {}  3789263585   \n",
       "8  CustomTestExampleBench  dummy_core_data_v1              {}   851966748   \n",
       "\n",
       "   training_time_seconds  test_time_seconds           benchmark_type  \\\n",
       "0               0.000009           0.001040  BenchmarkSpecSimulation   \n",
       "1               0.000004           0.000537  BenchmarkSpecPrediction   \n",
       "2               0.000004           0.000385  BenchmarkSpecSimulation   \n",
       "3               0.000003           0.000280  BenchmarkSpecSimulation   \n",
       "4               0.000003           0.000285  BenchmarkSpecPrediction   \n",
       "5               0.000003           0.000330  BenchmarkSpecSimulation   \n",
       "6               0.000003           0.000264  BenchmarkSpecSimulation   \n",
       "7               0.000003           0.000278  BenchmarkSpecPrediction   \n",
       "8               0.000003           0.000531  BenchmarkSpecSimulation   \n",
       "\n",
       "  metric_name  metric_score  cs_avg_max_abs_error  cs_median_max_abs_error  \n",
       "0        rmse      0.564484                   NaN                      NaN  \n",
       "1        rmse      0.559402                   NaN                      NaN  \n",
       "2        rmse      0.573960              0.993465                 0.993465  \n",
       "3        rmse      0.564484                   NaN                      NaN  \n",
       "4        rmse      0.559402                   NaN                      NaN  \n",
       "5        rmse      0.573960              0.993465                 0.993465  \n",
       "6        rmse      0.564484                   NaN                      NaN  \n",
       "7        rmse      0.559402                   NaN                      NaN  \n",
       "8        rmse      0.573960              0.993465                 0.993465  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_multiple_runs = run_benchmarks(\n",
    "    specs=[_spec_sim,_spec_pred,spec_with_custom_test], \n",
    "    build_model=_dummy_build_model,\n",
    "    n_times=3\n",
    ")\n",
    "results_multiple_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def aggregate_benchmark_results(\n",
    "    results_df: pd.DataFrame, # DataFrame returned by run_benchmarks (with return_dataframe=True).\n",
    "    group_by_cols: str | list[str] = 'benchmark_name', # Column(s) to group by before aggregation.\n",
    "    agg_funcs: str | list[str] = 'mean' # Aggregation function(s) ('mean', 'median', 'std', etc.) or list thereof.\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates numeric results from a benchmark DataFrame, grouped by specified columns.\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        return pd.DataFrame() # Return empty if input is empty\n",
    "\n",
    "    # Identify numeric columns suitable for aggregation\n",
    "    numeric_cols = results_df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    # Exclude columns that are typically identifiers or settings, even if numeric\n",
    "    cols_to_exclude = ['seed', 'repetition']\n",
    "    agg_cols = [col for col in numeric_cols if col not in cols_to_exclude and col not in (group_by_cols if isinstance(group_by_cols, list) else [group_by_cols])]\n",
    "\n",
    "    if not agg_cols:\n",
    "        print(\"Warning: No numeric columns found to aggregate (excluding identifiers). Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Perform groupby and aggregation\n",
    "        aggregated_df = results_df.groupby(group_by_cols)[agg_cols].agg(agg_funcs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during aggregation: {e}\")\n",
    "        # Provide more context if grouping fails\n",
    "        if isinstance(group_by_cols, list):\n",
    "            missing_cols = [col for col in group_by_cols if col not in results_df.columns]\n",
    "        else:\n",
    "            missing_cols = [group_by_cols] if group_by_cols not in results_df.columns else []\n",
    "\n",
    "        if missing_cols:\n",
    "            print(f\"  -> Grouping columns not found in DataFrame: {missing_cols}\")\n",
    "        print(f\"  -> Available columns: {results_df.columns.tolist()}\")\n",
    "        print(f\"  -> Columns selected for aggregation: {agg_cols}\")\n",
    "        return pd.DataFrame() # Return empty on error\n",
    "\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">training_time_seconds</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_time_seconds</th>\n",
       "      <th colspan=\"2\" halign=\"left\">metric_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cs_avg_max_abs_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cs_median_max_abs_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CustomTestExampleBench</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.453506e-07</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.573960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.993465</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_spec_default</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>3.395723e-06</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.564484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_spec_pred_params</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.254011e-07</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.559402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       training_time_seconds               test_time_seconds  \\\n",
       "                                        mean           std              mean   \n",
       "benchmark_name                                                                 \n",
       "CustomTestExampleBench              0.000003  4.453506e-07          0.000415   \n",
       "_spec_default                       0.000005  3.395723e-06          0.000528   \n",
       "_spec_pred_params                   0.000003  3.254011e-07          0.000367   \n",
       "\n",
       "                                 metric_score      cs_avg_max_abs_error       \\\n",
       "                             std         mean  std                 mean  std   \n",
       "benchmark_name                                                                 \n",
       "CustomTestExampleBench  0.000104     0.573960  0.0             0.993465  0.0   \n",
       "_spec_default           0.000443     0.564484  0.0                  NaN  NaN   \n",
       "_spec_pred_params       0.000147     0.559402  0.0                  NaN  NaN   \n",
       "\n",
       "                       cs_median_max_abs_error       \n",
       "                                          mean  std  \n",
       "benchmark_name                                       \n",
       "CustomTestExampleBench                0.993465  0.0  \n",
       "_spec_default                              NaN  NaN  \n",
       "_spec_pred_params                          NaN  NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_benchmark_results(results_multiple_runs,agg_funcs=['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
