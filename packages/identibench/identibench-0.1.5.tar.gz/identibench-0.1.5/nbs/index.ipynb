{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/daniel-om-weber/identibench/main/assets/logo.svg\" width=\"200\" align=\"left\" alt=\"identibench logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdentiBench\n",
    "[![PyPI version](https://badge.fury.io/py/identibench.svg)](https://badge.fury.io/py/identibench)\n",
    "[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n",
    "[![Docs Status](https://img.shields.io/badge/docs-up_to_date-brightgreen.svg)](https://daniel-om-weber.github.io/identibench/)\n",
    "[![Python Versions](https://img.shields.io/pypi/pyversions/identibench)](https://pypi.org/project/identibench/)\n",
    "\n",
    "IdentiBench is a Python library designed to streamline and standardize the benchmarking of system identification models. Evaluating and comparing dynamic models often requires repetitive setup for data handling, evaluation protocols, and metrics implementation, making fair comparisons and reproducing results challenging. IdentiBench tackles this by offering a collection of pre-defined benchmark specifications for simulation and prediction tasks, built upon common datasets. It automates data downloading and processing into a consistent format and provides standard evaluation metrics via a simple interface (run_benchmark). This allows you to focus your efforts on developing innovative models, while relying on IdentiBench for robust and reproducible evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "* **Access Many Benchmarks from different systems:** Instantly utilize pre-configured benchmarks covering diverse domains like electronics (Silverbox), mechanics (Industrial Robot), process control (Cascaded Tanks), aerospace (Quadrotors), and more, available for both simulation and prediction tasks.\n",
    "* **Automate Data Management:** Forget manual downloading and processing; the library handles fetching data from various sources (web, Drive, Dataverse), extracting archives (ZIP, RAR, MAT, BAG), converting to a standard HDF5 format, and caching locally.\n",
    "* **Integrate Any Model to evaluate on all benchmarks:** Plug in your custom models, regardless of the Python framework used (NumPy, SciPy, PyTorch, TensorFlow, JAX, etc.), using a straightforward function interface (`build_model`) that receives all necessary context.\n",
    "* **Capture Comprehensive Results:** Obtain detailed evaluation reports including standard metrics (RMSE, NRMSE, FIT%, etc.), task-specific scores, execution timings, configuration parameters (hyperparameters, seed), and raw model predictions for thorough analysis.\n",
    "* **Easily Define New Benchmarks:** Go beyond the included datasets by creating your own benchmark specifications (`BenchmarkSpecSimulation`, `BenchmarkSpecPrediction`) for private data or unique tasks, leveraging the library's structure and transparent data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "You can install `identibench` using pip:\n",
    "```bash\n",
    "pip install identibench\n",
    "```\n",
    "To install the latest development version directly from GitHub, use:\n",
    "```bash\n",
    "pip install git+https://github.com/daniel-om-weber/identibench.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage\n",
    "import identibench as idb\n",
    "from pathlib import Path\n",
    "\n",
    "# Example: Download a single dataset\n",
    "# Note: Always use a Path object, not a string\n",
    "save_path = Path('./tmp/wh')\n",
    "idb.datasets.workshop.dl_wiener_hammerstein(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sysidentpy.model_structure_selection import FROLS\n",
    "from sysidentpy.parameter_estimation import LeastSquares\n",
    "def build_frols_model(context):\n",
    "    u_train, y_train, _ = next(context.get_train_sequences())\n",
    "    \n",
    "    ylag = context.hyperparameters.get('ylag', 5)\n",
    "    xlag = context.hyperparameters.get('xlag', 5)\n",
    "    n_terms = context.hyperparameters.get('n_terms', 10)\n",
    "    estimator = context.hyperparameters.get('estimator', LeastSquares())\n",
    "\n",
    "    _model = FROLS(xlag=xlag, ylag=ylag, n_terms=n_terms,estimator=estimator)\n",
    "    _model.fit(X=u_train, y=y_train)\n",
    "\n",
    "    def model(u_test, y_init):\n",
    "        nonlocal _model\n",
    "        yhat_full = _model.predict(X=u_test, y=y_init[:_model.max_lag])\n",
    "        y_pred = yhat_full[_model.max_lag:]\n",
    "        return y_pred\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ylag': 2,\n",
    "    'xlag': 2,\n",
    "    'n_terms': 10, # Number of terms for FROLS\n",
    "    'estimator': LeastSquares()\n",
    "}\n",
    "\n",
    "results = idb.run_benchmark(\n",
    "    spec=idb.BenchmarkWH_Simulation,\n",
    "    build_model=build_frols_model,\n",
    "    hyperparameters=hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Simulation Benchmarks\n",
      "\n",
      "| Key | Benchmark Name |\n",
      "|---|---|\n",
      "| `WH_Sim` | BenchmarkWH_Simulation |\n",
      "| `Silverbox_Sim` | BenchmarkSilverbox_Simulation |\n",
      "| `Tanks_Sim` | BenchmarkCascadedTanks_Simulation |\n",
      "| `CED_Sim` | BenchmarkCED_Simulation |\n",
      "| `EMPS_Sim` | BenchmarkEMPS_Simulation |\n",
      "| `NoisyWH_Sim` | BenchmarkNoisyWH_Simulation |\n",
      "| `RobotForward_Sim` | BenchmarkRobotForward_Simulation |\n",
      "| `RobotInverse_Sim` | BenchmarkRobotInverse_Simulation |\n",
      "| `Ship_Sim` | BenchmarkShip_Simulation |\n",
      "| `QuadPelican_Sim` | BenchmarkQuadPelican_Simulation |\n",
      "| `QuadPi_Sim` | BenchmarkQuadPi_Simulation |\n",
      "\n",
      "\n",
      "## Prediction Benchmarks\n",
      "\n",
      "| Key | Benchmark Name |\n",
      "|---|---|\n",
      "| `WH_Pred` | BenchmarkWH_Prediction |\n",
      "| `Silverbox_Pred` | BenchmarkSilverbox_Prediction |\n",
      "| `Tanks_Pred` | BenchmarkCascadedTanks_Prediction |\n",
      "| `CED_Pred` | BenchmarkCED_Prediction |\n",
      "| `EMPS_Pred` | BenchmarkEMPS_Prediction |\n",
      "| `NoisyWH_Pred` | BenchmarkNoisyWH_Prediction |\n",
      "| `RobotForward_Pred` | BenchmarkRobotForward_Prediction |\n",
      "| `RobotInverse_Pred` | BenchmarkRobotInverse_Prediction |\n",
      "| `Ship_Pred` | BenchmarkShip_Prediction |\n",
      "| `QuadPelican_Pred` | BenchmarkQuadPelican_Prediction |\n",
      "| `QuadPi_Pred` | BenchmarkQuadPi_Prediction |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| echo: false # Hide this code cell in the final documentation\n",
    "#| output: asis # Output the results directly as Markdown\n",
    "\n",
    "sim_md = \"## Simulation Benchmarks\\n\\n\"\n",
    "sim_md += \"| Key | Benchmark Name |\\n\"\n",
    "sim_md += \"|---|---|\\n\"\n",
    "for key, spec in idb.simulation_benchmarks.items(): # Iterates through the simulation benchmarks dictionary\n",
    "    # Accesses the 'name' attribute of each benchmark spec object\n",
    "    sim_md += f\"| `{key}` | {getattr(spec, 'name', 'N/A')} |\\n\"\n",
    "\n",
    "print(sim_md)\n",
    "\n",
    "# --- Generate Table for Prediction Benchmarks ---\n",
    "pred_md = \"\\n## Prediction Benchmarks\\n\\n\" # Adds a newline for spacing\n",
    "pred_md += \"| Key | Benchmark Name |\\n\"\n",
    "pred_md += \"|---|---|\\n\"\n",
    "for key, spec in idb.prediction_benchmarks.items(): # Iterates through the prediction benchmarks dictionary\n",
    "    # Accesses the 'name' attribute of each benchmark spec object\n",
    "    pred_md += f\"| `{key}` | {getattr(spec, 'name', 'N/A')} |\\n\"\n",
    "\n",
    "print(pred_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Details\n",
    "\n",
    "This section provides more detail on the core concepts and components of the `identibench` workflow.\n",
    "\n",
    "### Benchmark Types\n",
    "\n",
    "`identibench` defines two main types of benchmark tasks, specified using different classes:\n",
    "\n",
    "* **Simulation (`BenchmarkSpecSimulation`)**:\n",
    "    * **Goal:** Evaluate a model's ability to perform a free-run simulation, predicting the system's output over an extended period given the input sequence.\n",
    "    * **Typical Input to Predictor:** The full input sequence (`u_test`) and potentially an initial segment of the output sequence (`y_test[:init_window]`) for warm-up or state initialization.\n",
    "    * **Expected Output from Predictor:** The predicted output sequence (`y_pred`) corresponding to the input, usually excluding the warm-up period.\n",
    "    * **Use Case:** Assessing models intended for long-term prediction, control simulation, or understanding overall system dynamics.\n",
    "\n",
    "* **Prediction (`BenchmarkSpecPrediction`)**:\n",
    "    * **Goal:** Evaluate a model's ability to predict the system's output *k* steps into the future based on recent past data.\n",
    "    * **Typical Input to Predictor:** Often involves windows of past inputs and outputs (e.g., `u[t:t+H]`, `y[t:t+H]`).\n",
    "    * **Expected Output from Predictor:** The predicted output at a specific future time step (e.g., `y[t+H+k]`). The `pred_horizon` parameter defines 'k', and `pred_step` defines how frequently predictions are made.\n",
    "    * **Use Case:** Evaluating models focused on short-to-medium term forecasting, state estimation, or receding horizon control.\n",
    "\n",
    "* **`init_window`**: Both benchmark types often use an `init_window`. This specifies an initial number of time steps whose data might be provided to the model for initialization or warm-up. Importantly, data within this window is typically *excluded* from the final performance metric calculation to ensure a fair evaluation of the model's predictive capabilities beyond the initial transient.\n",
    "\n",
    "### Model Interface (`build_model`)\n",
    "\n",
    "The core of integrating your custom logic is the `build_model` function you provide to `run_benchmark`.\n",
    "\n",
    "* **Purpose:** This function is responsible for defining your model architecture, training it using the provided data, and returning a callable predictor function.\n",
    "* **Input (`context: TrainingContext`):** Your `build_model` function receives a single argument, `context`, which is a `TrainingContext` object. This object gives you access to:\n",
    "    * `context.spec`: The full specification of the current benchmark being run (including dataset paths, input/output columns, `init_window`, etc.).\n",
    "    * `context.hyperparameters`: A dictionary containing any hyperparameters you passed to `run_benchmark`. Use this to configure your model or training process.\n",
    "    * `context.seed`: A random seed for ensuring reproducibility.\n",
    "    * Data Access Methods: Functions like `context.get_train_sequences()` and `context.get_valid_sequences()` provide iterators over the raw, full-length training and validation data sequences (as tuples of NumPy arrays `(u, y, x)`). **Note:** You need to handle any batching or windowing required for your specific training algorithm *within* your `build_model` function.\n",
    "* **Output (Predictor `Callable`):** `build_model` *must* return a callable object (e.g., a function, an object's method) that represents your trained model ready for prediction/simulation. This returned callable will be used internally by `run_benchmark` on the test set. Its expected signature depends on the benchmark type, but typically it accepts NumPy arrays for test inputs (and potentially initial outputs) and returns a NumPy array containing the predictions.\n",
    "\n",
    "### Running Multiple Benchmarks\n",
    "\n",
    "To evaluate a model across several scenarios efficiently, use the `run_multiple_benchmarks` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting benchmark run for 2 specifications, repeating each 3 times ---\n",
      "\n",
      "-- Repetition 1/3 --\n",
      "\n",
      "[1/6] Running: BenchmarkWH_Simulation (Rep 1)\n",
      "  -> Success: BenchmarkWH_Simulation (Rep 1) completed.\n",
      "\n",
      "[2/6] Running: BenchmarkSilverbox_Simulation (Rep 1)\n",
      "  -> Success: BenchmarkSilverbox_Simulation (Rep 1) completed.\n",
      "\n",
      "-- Repetition 2/3 --\n",
      "\n",
      "[3/6] Running: BenchmarkWH_Simulation (Rep 2)\n",
      "  -> Success: BenchmarkWH_Simulation (Rep 2) completed.\n",
      "\n",
      "[4/6] Running: BenchmarkSilverbox_Simulation (Rep 2)\n",
      "  -> Success: BenchmarkSilverbox_Simulation (Rep 2) completed.\n",
      "\n",
      "-- Repetition 3/3 --\n",
      "\n",
      "[5/6] Running: BenchmarkWH_Simulation (Rep 3)\n",
      "  -> Success: BenchmarkWH_Simulation (Rep 3) completed.\n",
      "\n",
      "[6/6] Running: BenchmarkSilverbox_Simulation (Rep 3)\n",
      "  -> Success: BenchmarkSilverbox_Simulation (Rep 3) completed.\n",
      "\n",
      "--- Benchmark run finished. 6/6 individual runs completed successfully. ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>benchmark_name</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>seed</th>\n",
       "      <th>training_time_seconds</th>\n",
       "      <th>test_time_seconds</th>\n",
       "      <th>benchmark_type</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_score</th>\n",
       "      <th>cs_multisine_rmse</th>\n",
       "      <th>cs_arrow_full_rmse</th>\n",
       "      <th>cs_arrow_no_extrapolation_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BenchmarkWH_Simulation</td>\n",
       "      <td>wh</td>\n",
       "      <td>{}</td>\n",
       "      <td>2406651230</td>\n",
       "      <td>4.944649</td>\n",
       "      <td>1.012850</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse_mV</td>\n",
       "      <td>42.161572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BenchmarkSilverbox_Simulation</td>\n",
       "      <td>silverbox</td>\n",
       "      <td>{}</td>\n",
       "      <td>3813113752</td>\n",
       "      <td>2.839149</td>\n",
       "      <td>1.246224</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse_mV</td>\n",
       "      <td>10.732386</td>\n",
       "      <td>8.501941</td>\n",
       "      <td>16.154317</td>\n",
       "      <td>7.5409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BenchmarkWH_Simulation</td>\n",
       "      <td>wh</td>\n",
       "      <td>{}</td>\n",
       "      <td>1950649438</td>\n",
       "      <td>4.801520</td>\n",
       "      <td>1.034119</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse_mV</td>\n",
       "      <td>42.161572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BenchmarkSilverbox_Simulation</td>\n",
       "      <td>silverbox</td>\n",
       "      <td>{}</td>\n",
       "      <td>1560698088</td>\n",
       "      <td>2.880391</td>\n",
       "      <td>1.217932</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse_mV</td>\n",
       "      <td>10.732386</td>\n",
       "      <td>8.501941</td>\n",
       "      <td>16.154317</td>\n",
       "      <td>7.5409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BenchmarkWH_Simulation</td>\n",
       "      <td>wh</td>\n",
       "      <td>{}</td>\n",
       "      <td>3258007268</td>\n",
       "      <td>4.916941</td>\n",
       "      <td>1.021927</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse_mV</td>\n",
       "      <td>42.161572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BenchmarkSilverbox_Simulation</td>\n",
       "      <td>silverbox</td>\n",
       "      <td>{}</td>\n",
       "      <td>4194043971</td>\n",
       "      <td>2.937101</td>\n",
       "      <td>1.231710</td>\n",
       "      <td>BenchmarkSpecSimulation</td>\n",
       "      <td>rmse_mV</td>\n",
       "      <td>10.732386</td>\n",
       "      <td>8.501941</td>\n",
       "      <td>16.154317</td>\n",
       "      <td>7.5409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  benchmark_name dataset_id hyperparameters        seed  \\\n",
       "0         BenchmarkWH_Simulation         wh              {}  2406651230   \n",
       "1  BenchmarkSilverbox_Simulation  silverbox              {}  3813113752   \n",
       "2         BenchmarkWH_Simulation         wh              {}  1950649438   \n",
       "3  BenchmarkSilverbox_Simulation  silverbox              {}  1560698088   \n",
       "4         BenchmarkWH_Simulation         wh              {}  3258007268   \n",
       "5  BenchmarkSilverbox_Simulation  silverbox              {}  4194043971   \n",
       "\n",
       "   training_time_seconds  test_time_seconds           benchmark_type  \\\n",
       "0               4.944649           1.012850  BenchmarkSpecSimulation   \n",
       "1               2.839149           1.246224  BenchmarkSpecSimulation   \n",
       "2               4.801520           1.034119  BenchmarkSpecSimulation   \n",
       "3               2.880391           1.217932  BenchmarkSpecSimulation   \n",
       "4               4.916941           1.021927  BenchmarkSpecSimulation   \n",
       "5               2.937101           1.231710  BenchmarkSpecSimulation   \n",
       "\n",
       "  metric_name  metric_score  cs_multisine_rmse  cs_arrow_full_rmse  \\\n",
       "0     rmse_mV     42.161572                NaN                 NaN   \n",
       "1     rmse_mV     10.732386           8.501941           16.154317   \n",
       "2     rmse_mV     42.161572                NaN                 NaN   \n",
       "3     rmse_mV     10.732386           8.501941           16.154317   \n",
       "4     rmse_mV     42.161572                NaN                 NaN   \n",
       "5     rmse_mV     10.732386           8.501941           16.154317   \n",
       "\n",
       "   cs_arrow_no_extrapolation_rmse  \n",
       "0                             NaN  \n",
       "1                          7.5409  \n",
       "2                             NaN  \n",
       "3                          7.5409  \n",
       "4                             NaN  \n",
       "5                          7.5409  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Run on a subset of benchmarks\n",
    "specs_to_run = {\n",
    "    'WH_Sim': idb.simulation_benchmarks['WH_Sim'],\n",
    "    'Silverbox_Sim': idb.simulation_benchmarks['Silverbox_Sim']\n",
    "}\n",
    "\n",
    "# Assume 'my_build_model' is your defined build function\n",
    "all_results = idb.run_benchmarks(specs_to_run, build_model=build_frols_model,n_times=3)\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates through the provided list or dictionary of benchmark specifications, calling `run_benchmark` for each one using the same `build_model` function and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">training_time_seconds</th>\n",
       "      <th colspan=\"2\" halign=\"left\">test_time_seconds</th>\n",
       "      <th colspan=\"2\" halign=\"left\">metric_score</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cs_multisine_rmse</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cs_arrow_full_rmse</th>\n",
       "      <th colspan=\"2\" halign=\"left\">cs_arrow_no_extrapolation_rmse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benchmark_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BenchmarkSilverbox_Simulation</th>\n",
       "      <td>2.885547</td>\n",
       "      <td>0.049179</td>\n",
       "      <td>1.231955</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>10.732386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.501941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.154317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BenchmarkWH_Simulation</th>\n",
       "      <td>4.887703</td>\n",
       "      <td>0.075912</td>\n",
       "      <td>1.022966</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>42.161572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              training_time_seconds            \\\n",
       "                                               mean       std   \n",
       "benchmark_name                                                  \n",
       "BenchmarkSilverbox_Simulation              2.885547  0.049179   \n",
       "BenchmarkWH_Simulation                     4.887703  0.075912   \n",
       "\n",
       "                              test_time_seconds           metric_score       \\\n",
       "                                           mean       std         mean  std   \n",
       "benchmark_name                                                                \n",
       "BenchmarkSilverbox_Simulation          1.231955  0.014147    10.732386  0.0   \n",
       "BenchmarkWH_Simulation                 1.022966  0.010673    42.161572  0.0   \n",
       "\n",
       "                              cs_multisine_rmse      cs_arrow_full_rmse       \\\n",
       "                                           mean  std               mean  std   \n",
       "benchmark_name                                                                 \n",
       "BenchmarkSilverbox_Simulation          8.501941  0.0          16.154317  0.0   \n",
       "BenchmarkWH_Simulation                      NaN  NaN                NaN  NaN   \n",
       "\n",
       "                              cs_arrow_no_extrapolation_rmse       \n",
       "                                                        mean  std  \n",
       "benchmark_name                                                     \n",
       "BenchmarkSilverbox_Simulation                         7.5409  0.0  \n",
       "BenchmarkWH_Simulation                                   NaN  NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate mean and std of the results\n",
    "idb.aggregate_benchmark_results(all_results,agg_funcs=['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling & Format\n",
    "\n",
    "Understanding how `identibench` organizes and stores data is helpful for direct interaction or adding new datasets.\n",
    "\n",
    "* **Directory Structure:** Datasets are stored under a root directory (default: `~/.identibench_data`, configurable via the `IDENTIBENCH_DATA_ROOT` environment variable). The structure follows: `DATA_ROOT / [dataset_id] / [subset] / [experiment_file.hdf5]`.\n",
    "* **Subsets:** Standard subset names are `train`, `valid`, and `test`. An optional `train_valid` directory might contain combined data.\n",
    "* **Download & Cache:** Data is downloaded automatically when a benchmark requires it and cached locally to avoid re-downloads. The `identibench.datasets.download_all_datasets` function can fetch all datasets at once.\n",
    "* **File Format:** Processed time-series data is stored in the **HDF5 (`.hdf5`)** format.\n",
    "* **HDF5 Structure:**\n",
    "    * Each `.hdf5` file typically represents one experimental run.\n",
    "    * Signals (inputs, outputs, states) are stored as separate 1-dimensional datasets within the file, named conventionally as `u0`, `u1`, ..., `y0`, `y1`, ..., `x0`, ...\n",
    "    * Data is usually stored as `float32` NumPy arrays.\n",
    "    * Metadata like sampling frequency (`fs`) and suggested initialization window size (`init_sz`) are stored as attributes on the root group of the HDF5 file.\n",
    "    * *Example Structure:*\n",
    "        ```\n",
    "        my_dataset/\n",
    "        └── train/\n",
    "            └── train_run_1.hdf5\n",
    "                ├── u0 (Dataset: shape=(N,), dtype=float32)\n",
    "                ├── y0 (Dataset: shape=(N,), dtype=float32)\n",
    "                └── Attributes:\n",
    "                    └── fs (Attribute: float)\n",
    "        ```\n",
    "* **Extensibility:** Adhering to this HDF5 format ensures compatibility when adding new dataset loaders. Helper functions like `identibench.utils.write_array` facilitate creating files in the correct format.\n",
    "\n",
    "### Understanding Benchmark Results\n",
    "\n",
    "The `run_benchmark` function returns a dictionary containing detailed results of the experiment. Key entries include:\n",
    "\n",
    "* `benchmark_name` (`str`): The unique name of the benchmark specification used.\n",
    "* `dataset_id` (`str`): Identifier for the dataset source.\n",
    "* `hyperparameters` (`dict`): The hyperparameters dictionary passed to the run.\n",
    "* `seed` (`int`): The random seed used for the run.\n",
    "* `training_time_seconds` (`float`): Wall-clock time spent inside your `build_model` function.\n",
    "* `test_time_seconds` (`float`): Wall-clock time spent evaluating the returned predictor on the test set.\n",
    "* `benchmark_type` (`str`): The type of benchmark run (e.g., `'BenchmarkSpecSimulation'`).\n",
    "* `metric_name` (`str`): The name of the primary metric function defined in the spec.\n",
    "* `metric_score` (`float`): The calculated score for the primary metric on the test set (aggregated if multiple test files).\n",
    "* `custom_scores` (`dict`): Any additional scores calculated by custom evaluation logic specific to the benchmark.\n",
    "* `model_predictions` (`list`): A list containing the raw outputs. For simulation, it's typically `[(y_pred_test1, y_true_test1), (y_pred_test2, y_true_test2), ...]`. For prediction, the structure might be nested reflecting windowed predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
