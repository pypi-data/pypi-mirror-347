{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berlin Robust Orientation Estimation Assessment Dataset (BROAD)\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets.broad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from identibench.utils import write_array\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dl_broad(\n",
    "        save_path: Path, #directory the files are written to, created if it does not exist\n",
    "        force_download: bool = True, # force download the dataset\n",
    ") -> None:\n",
    "    save_path = Path(save_path)\n",
    "\n",
    "    idxs_valid = ['14_', '39_', '21_']\n",
    "    idxs_test = ['29_', '22_', '35_']\n",
    "    \n",
    "    url = \"https://api.github.com/repos/dlaidig/broad/contents/data_hdf5\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    for file in data:\n",
    "        # Check if it is a file in hdf5 format\n",
    "        if (file['type'] == 'file') and ('.hdf5' in file['name']):\n",
    "            download_url = file['download_url']\n",
    "            file_response = requests.get(download_url)\n",
    "            file_response.raise_for_status()\n",
    "\n",
    "            file_idx = file['name'][:3]\n",
    "            if file_idx in idxs_valid:\n",
    "                parent = 'valid'\n",
    "            elif file_idx in idxs_test:\n",
    "                parent = 'test'\n",
    "            else:\n",
    "                parent = 'train'\n",
    "            hdf_path = save_path / parent \n",
    "            os.makedirs(hdf_path, exist_ok=True)\n",
    "\n",
    "            #open loaded hdf5 file in ram\n",
    "            tmp_file = BytesIO(file_response.content)\n",
    "            # Write file to local system\n",
    "            with h5py.File(tmp_file) as f_read:\n",
    "                with h5py.File(hdf_path / file['name'],'w') as f_write:\n",
    "                    def transfer_dataset(ds_name: str) -> None:\n",
    "                        x = f_read[ds_name][:]\n",
    "                        if x.ndim == 2:\n",
    "                            write_array(f_write,ds_name,x,dtype='f4')\n",
    "\n",
    "                    #transfer each dataset in the source file\n",
    "                    f_read.visit(transfer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = Path('./tmp')\n",
    "dl_broad(tmp_dir / 'broad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import shutil\n",
    "#clean temporary hdf5 file\n",
    "# shutil.rmtree(tmp_dir)\n",
    "# shutil.rmtree(get_tmp_benchmark_directory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
