# Recitations Segmenter

<div align="center">
<strong>Ø¨ØªÙˆÙÙŠÙ‚ Ø§Ù„Ù„Ù‡: Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Ù Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©</strong>

[![Tests][tests-badge]][tests-url]
[![PyPI][pypi-badge]][pypi-url]
[![Python Versions][python-badge]][python-url]
[![Hugging Face][hf-badge]][hf-url]
[![Google Colab][colab-badge]][colab-url]
[![MIT License][mit-badge]][mit-url]

</div>

[tests-badge]: https://img.shields.io/github/actions/workflow/status/obadx/recitations-segmenter/tests.yml?branch=main&label=tests
[tests-url]: https://github.com/obadx/recitations-segmenter/actions/workflows/tests.yml
[pypi-badge]: https://img.shields.io/pypi/v/recitations-segmenter.svg
[pypi-url]: https://pypi.org/project/recitations-segmenter/
[mit-badge]: https://img.shields.io/github/license/obadx/recitations-segmenter.svg
[mit-url]: https://github.com/obadx/recitations-segmenter/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/recitations-segmenter.svg
[python-url]: https://pypi.org/project/recitations-segmenter/
[colab-badge]: https://img.shields.io/badge/Google%20Colab-Open%20in%20Colab-F9AB00?logo=google-colab&logoColor=white
[colab-url]: https://colab.research.google.com/drive/1-RuRQOj4l2MA_SG2p4m-afR7MAsT5I22?usp=sharing
[hf-badge]: https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue
[hf-url]: https://huggingface.co/obadx/recitation-segmenter-v2



Ø¨ÙØ¶Ù„ Ø§Ù„Ù„Ù‡ Ù†Ù‚Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø­Ø³Ù‘ÙÙ† Ù…Ù† [W2v2Bert](https://huggingface.co/docs/transformers/model_doc/wav2vec2-bert) Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ù…Ù‡Ù…Ø© Sequence Frame Level Classification Ø¨Ø¯Ù‚Ø© 20 Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ© (20 milliseconds)  ÙˆÙ…Ø¹Ù‡ Ø£ÙŠØ¶Ø§ Ù…ÙƒØªØ¨Ø© python ØªØ¹Ù…Ù„ Ø¨Ø£Ø¯Ø§Ø¡ Ø¹Ø§Ù„ÙŠ Ù„Ø£ÙŠ Ø¹Ø¯Ø¯ ÙˆØ£ÙŠ Ø·ÙˆÙ„ (Ù…Ù† Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†Ù Ù„Ø¹Ø¯Ø© Ø³Ø§Ø¹Ø§Øª) Ù…Ù† Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ©


![VAD Architecture](./assets/vad-arch.svg)

## Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª

* ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Ù

* Ù…Ø¯Ø±Ù‘ÙØ¨ Ø®ØµÙŠØµØ§ Ù„Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ©

* Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ØªØµÙ„ Ø¥Ù„Ù‰ 20 Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©

* ØªØ­ØªØ§Ø¬ ÙÙ‚Ø· Ø¥Ù„ÙŠ 3 GB Ù…Ù† Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ§Øª (GPU Memory)

* ÙŠØ³ØªØ·ÙŠØ¹ ØªÙ‚Ø·ÙŠØ¹  Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ù„Ø£ÙŠ Ù…Ø¯Ø© Ù…Ù† Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ù…Ù† Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†Ù  Ù„Ø¹Ø¯Ø© Ø³Ø§Ø¹Ø§Øª Ù…Ù† ØºÙŠØ± Ù†Ù‚Øµ ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡

## Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Hugging Face ğŸ¤—
* [Ø§Ù„Ù†Ù…ÙˆØ°Ø¬](https://huggingface.co/obadx/recitation-segmenter-v2)
* [Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨](https://huggingface.co/datasets/obadx/recitation-segmentation)
* [Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ Ø¥Ø¶Ø§ÙØ© augmentation](https://huggingface.co/datasets/obadx/recitation-segmentation-augmented)

<
## Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª

* [ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø©](#ØªØ«Ø¨ÙŠØª-Ø§Ù„Ù…ÙƒØªØ¨Ø©)

* [Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Python](#api-Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…-python)
* [ÙˆØ§Ø¬Ù‡Ø© Command Line Interface](#command-line-interface)
    * [ÙˆØµÙ Ù…ÙØµÙ„ Ù„Ù„ Command Line](#ÙˆØµÙ-Ù…ÙØµÙ„-Ù„Ù„-command-line)
* [ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù…ÙƒØªØ¨Ø© (API Refernece)](#ØªÙˆØ«ÙŠÙ‚-Ø§Ù„Ù…ÙƒØªØ¨Ø©-(api-refernece))
* [ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨](#ØªÙØ§ØµÙŠÙ„-Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
    * [Ø¯ÙˆØ§ÙØ¹ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ ÙˆØ¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ©](#Ø¯ÙˆØ§ÙØ¹-ØªØ¯Ø±ÙŠØ¨-Ù†Ù…ÙˆØ°Ø¬-Ø¬Ø¯ÙŠØ¯-ÙˆØ¹Ø¯Ù…-Ø§Ø³ØªØ®Ø¯Ø§Ù…-Ø§Ù„Ø·Ø±Ù‚-Ø§Ù„Ø­Ø§Ù„ÙŠØ©)
    * [Ø·Ø±ÙŠÙ‚Ø© Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©](#Ø·Ø±ÙŠÙ‚Ø©-Ø­Ù„-Ø§Ù„Ù…Ø´ÙƒÙ„Ø©)
    * [ØªÙ‡ÙŠØ¦Ø© Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±](#ØªÙ‡ÙŠØ¦Ø©-Ø¨ÙŠØ¦Ø©-Ø§Ù„ØªØ·ÙˆÙŠØ±)
    * [Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨](#Ø¨ÙŠØ§Ù†Ø§Øª-Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
    * [Ø·Ø±ÙŠÙ‚Ø© ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª](#Ø·Ø±ÙŠÙ‚Ø©-ØªØ¬Ù…ÙŠØ¹-Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
    * [ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª](#ØªÙ‡ÙŠØ¦Ø©-Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
    * [Ø§Ù„ØªØ¯Ø±ÙŠØ¨](#Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
    * [Ø§Ù„Ù†ØªØ§Ø¦Ø¬:](#Ø§Ù„Ù†ØªØ§Ø¦Ø¬)
* [Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©](#Ù…Ù„Ø§Ø­Ø¸Ø§Øª-Ù…Ù‡Ù…Ø©)



## ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø©

### Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ«Ø¨ÙŠØª

ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨ØªÙŠ:

* [ffmbeg](https://ffmpeg.org/download.html)
* [libsoundfile](https://github.com/libsndfile/libsndfile/releases)

#### Linux

```bash
sudo apt-get update
sudo apt-get install -y ffmpeg libsndfile1 portaudio19-dev
```

#### Winodws & Mac

ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© `anaconda` . ÙˆÙ…Ù† Ø«Ù… ØªÙ†Ø²ÙŠÙ„ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ù…ÙƒØªØ¨ØªÙŠÙ†

```bash
conda create -n segment python=3.12
conda activate segment
conda install -c conda-forge ffmpeg libsndfile
```


####  Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pip

```bash
pip install recitations-segmenter
```

####  Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… uv

```bash
uv add recitations-segmenter
```

## API  Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Python

Ù…ÙˆØ¶Ø­ Ø£Ø¯Ù†Ø§Ù‡ Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ø§ØªØ³Ø®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø¨Ø§Ù„ python ÙˆÙŠÙˆØ¬Ø¯ Ø£ÙŠØ¶Ø§ Ù…Ø«Ø§Ù„ Ø¯Ø§Ø®Ù„ Google Colab:

[![Google Colab][colab-badge]][colab-url]

```python
from pathlib import Path

from recitations_segmenter import segment_recitations, read_audio, clean_speech_intervals
from transformers import AutoFeatureExtractor, AutoModelForAudioFrameClassification
import torch

if __name__ == '__main__':
    device = torch.device('cuda')
    dtype = torch.bfloat16

    processor = AutoFeatureExtractor.from_pretrained(
        "obadx/recitation-segmenter-v2")
    model = AutoModelForAudioFrameClassification.from_pretrained(
        "obadx/recitation-segmenter-v2",
    )

    model.to(device, dtype=dtype)

    # Change this to the file pathes of Holy Quran recitations
    # File pathes with the Holy Quran Recitations
    file_pathes = [
        './assets/dussary_002282.mp3',
        './assets/hussary_053001.mp3',
    ]
    waves = [read_audio(p) for p in file_pathes]

    # Extracting speech inervals in samples according to 16000 Sample rate
    sampled_outputs = segment_recitations(
        waves,
        model,
        processor,
        device=device,
        dtype=dtype,
        batch_size=8,
    )

    for out, path in zip(sampled_outputs, file_pathes):
        # Clean The speech intervals by:
        # * merging small silence durations
        # * remove small speech durations
        # * add padding to each speech duration
        # Raises:
        # * NoSpeechIntervals: if the wav is complete silence
        # * TooHighMinSpeechDruation: if `min_speech_duration` is too high which
        # resuls for deleting all speech intervals
        clean_out = clean_speech_intervals(
            out.speech_intervals,
            out.is_complete,
            min_silence_duration_ms=30,
            min_speech_duration_ms=30,
            pad_duration_ms=30,
            return_seconds=True,
        )

        print(f'Speech Intervals of: {Path(path).name}: ')
        print(clean_out.clean_speech_intervals)
        print(f'Is Recitation Complete: {clean_out.is_complete}')
        print('-' * 40)
```



## Command Line Interface

ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø¨Ø§Ø´Ø±Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø© Ù…Ù† ÙˆØ¨Ø¯ÙˆÙ† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚:

```bash
uvx recitations-segmenter alfateha.mp3 
```

Ø£Ùˆ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: 

```bash
recitations-segmenter alfateha.mp3 
```

Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÙ‚ÙŠØªØ§Øª Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Ù Ø¹Ù„Ù‰ Ù‡ÙŠØ¦ØªÙŠÙ†: 

### ÙÙŠ Ø§Ù„ terminal
```text
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.04s/it]
Speech Intervals:
tensor([[ 0.7300,  5.2900],
        [ 6.5100, 10.9900],
        [12.4700, 17.2100],
        [18.1500, 21.6850],
        [22.6850, 26.2650],
        [27.4450, 33.2050],
        [34.2650, 38.6250],
        [39.8250, 53.3200]])

```
### ÙˆØ¹Ù„Ù‰ Ù‡ÙŠØ¦Ø© Ù…Ù„Ù JSON ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± : `output/speech_intervals_alfateha.json`

Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÙ‚ÙŠØªØ§Øª Ù„ÙƒÙ„ Ù…Ù‚Ø·Ø¹ Ù‚Ø±Ø¢Ù†ÙŠ Ø¨Ø¯Ø§Ø®Ù„ Ø³ÙˆØ±Ø© Ø§Ù„ÙØ§ØªØ© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± : `output` ÙˆÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„Ù `speech_intervals_alfateha.json`. ÙˆÙŠØ­ØªÙˆÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¢ØªÙŠ:

```json
{
    "clean_speech_intervals": [
        [
            0.7300000190734863,
            5.289999961853027
        ],
        [
            6.510000228881836,
            10.989999771118164
        ],
        [
            12.470000267028809,
            17.209999084472656
        ],
        [
            18.149999618530273,
            21.684999465942383
        ],
        [
            22.684999465942383,
            26.264999389648438
        ],
        [
            27.44499969482422,
            33.20500183105469
        ],
        [
            34.26499938964844,
            38.625
        ],
        [
            39.82500076293945,
            53.31999969482422
        ]
    ],
    "speech_intervals": [
        [
            0.7599999904632568,
            5.260000228881836
        ],
        [
            6.539999961853027,
            10.960000038146973
        ],
        [
            12.5,
            17.18000030517578
        ],
        [
            18.18000030517578,
            21.655000686645508
        ],
        [
            22.71500015258789,
            26.235000610351562
        ],
        [
            27.475000381469727,
            33.17499923706055
        ],
        [
            34.29499816894531,
            38.595001220703125
        ],
        [
            39.85499954223633,
            53.290000915527344
        ]
    ],
    "is_complete": true
}
```

ÙŠØªØ¶Ù…Ù† ÙƒÙ„ Ù…Ù„Ù JSON Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙØ§ØªÙŠØ­:

* `clean_speech_intervals`:     Ø§Ù„ØªÙˆÙ‚ÙŠØªØ§Øª Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆÙ†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ù…Ù‚Ø·Ø¹ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ù‚ÙŠØ­
* `speech_intervals`: Ø§Ù„ØªÙˆÙ‚ÙŠØªØ§Øª Ø¨Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆÙ†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ù…Ù‚Ø·Ø¹ 
*  `is_complete`: Ù‡Ù„ Ø§Ù„ØªÙ„Ø§ÙˆØ© Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© ØªØ§Ù…Ø© Ø£Ù… Ø£Ù† Ø¢Ø®Ø± Ø§Ù„Ù…Ù‚Ø·Ø¹ Ù„Ø§ ÙŠØªØ¶Ù…Ù† ÙˆÙ‚ÙØ§ Ù…Ø­Ø¶Ø§



### ÙˆØµÙ Ù…ÙØµÙ„ Ù„Ù„ Command Line




```text
usage: recitations-segmenter [-h] [-o OUTPUT] [--min-silence-duration-ms MIN_SILENCE_DURATION_MS] [--min-speech-duration-ms MIN_SPEECH_DURATION_MS] [--pad-duration-ms PAD_DURATION_MS]
                             [--return-samples] [--batch-size BATCH_SIZE] [--max-duration-ms MAX_DURATION_MS] [--device {cpu,cuda}] [--dtype {bfloat16,float16,float32}]
                             inputs [inputs ...]

Segment Holy Quran rectiations into speech intervals based on ÙˆÙ‚Ù using Wav2Vec2Bert model.

options:
  -h, --help            show this help message and exit

Input/Output Options:
  inputs                Input paths (files or directories) containing audio files
  -o OUTPUT, --output OUTPUT
                        Output directory for JSON results (default: ./output)

Segmentation Parameters:
  --min-silence-duration-ms MIN_SILENCE_DURATION_MS
                        Minimum silence duration (ms) between speech segments
                        - Silence shorter than this will be merged with speech
                        - Default: 30ms
  --min-speech-duration-ms MIN_SPEECH_DURATION_MS
                        Minimum valid speech duration (ms)
                        - Speech segments shorter than this will be removed
                        - Default: 30ms
  --pad-duration-ms PAD_DURATION_MS
                        Padding added around speech segments (ms)
                        Default: 30ms
  --return-samples      Return intervals in samples according to 16000 sampling rate.
                        - Default to return interval in seconds

Model Configuration:
  --batch-size BATCH_SIZE
                        Number of audio chunks processed simultaneously
                        - Higher values may increase speed but require more GPU memory.
                        - Default: 8 which occupies nearly 3GB of GPU memory.
  --max-duration-ms MAX_DURATION_MS
                        Maximum processing chunk duration (2-20000ms)
                        - Affects memory usage and processing granularity
                        - Do not Change it unless there exists a strong reason.
                        - Default: 19995ms
  --device {cpu,cuda}   Processing device selection
                        Default: cuda
  --dtype {bfloat16,float16,float32}
                        Numerical precision for model computation:
                        - bfloat16: Best performance (modern GPUs)
                        - float16: Legacy support
                        - float32: Maximum precision (CPU fallback)
                        Default: bfloat16

Examples:
  # Process single file with default settings
  recitations-segmenter input.mp3 -o results

  # Process multiple files file with default settings
  recitations-segmenter input1.mp3 input2.wav -o output

  # Process directory of audio files
  recitations-segmenter /path/to/recitations/ --output ./segmentation_results

  # Process: audio files and directory of audio files
  recitations-segmenter input.mp3 /path/to/recitations/ --output ./segmentation_results

  # Adjust segmentation parameters
  recitations-segmenter input.wav --min-silence-duration-ms 200 --min-speech-duration-ms 900 --pad-duration-ms 40

File Formats Supported:
  MP3, WAV, FLAC, OGG, AAC, M4A, OPUS

Output Format:
  Each input file generates a JSON file containing:
  - clean_speech_intervals: Final filtered speech segments
  - speech_intervals: Raw detected speech segments
  - is_complete: whether the recitaion is a complete ÙˆÙ‚Ù or the recitation is contining (has not stoped yet)

Error Handling:
  - Skips unsupported file types


```



## ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù…ÙƒØªØ¨Ø© (API Refernece)

### `segment_recitations`

```python
@torch.no_grad()
def segment_recitations(
    waves: list[torch.FloatTensor],
    model: Wav2Vec2BertForAudioFrameClassification,
    processor: Wav2Vec2BertProcessor,
    batch_size=64,
    device=torch.device('cpu'),
    dtype=torch.bfloat16,
    return_probabilities=False,
    sample_rate=16000,
    processor_window=400,
    processor_hop=160,
    processor_stride=2,
    max_duration_ms=19995,
    speech_label=1,
    silence_label=0,
    cache_dir: Optional[str | Path] = None,
    overwrite_cache: Optional[bool] = False,
) -> list[W2vBSegmentationOutput]:

```

Segment The Holy Quran rectiations into speech intervals based on ÙˆÙ‚Ù using Wav2Vec2Bert model.

**Arguments**:
- `waves` (`list[torch.FloatTensor]`): List of audio waveforms to process (each as FloatTensor)
- `model` (`Wav2Vec2BertForAudioFrameClassification`): Loaded Wav2Vec2BertForAudioFrameClassification model
- `processor` (`Wav2Vec2BertProcessor`): Wav2Vec2BertProcessor for feature extraction
- `batch_size` (`int`): Number of samples per batch
- `sample_rate` (`int`): Input audio sampling rate (must be 16000)
- `processor_window` (`int`): Processor window size (fixed at 400 samples)
- `processor_hop` (`int`): Processor hop length (fixed at 160 samples)
- `processor_stride` (`int`): Processor stride (fixed at 2)
- `max_duration_ms` (`int`): Maximum chunk duration in ms for processing (2-20000)
- `speech_label` (`int`): Class index for speech segments
- `silence_label` (`int`): Class index for silence segments
- `device` (`torch.device`): Torch device for inference
- `dtype` (`torch.dtype`): Data type for model computation only. Default it `torch.bfloat16` for post processing we use `torch.float32`
- `return_probabilities` (`bool`): Whether to return class probabilities
- `cache_dir` (`Optional[str | Path]`): Optional feature disabled by default: if it is not `None`. Saving speech intervals to the `cach_dir` so next time for inference with the sample input `waves` we did not have to recompute the speech_intervals
- `overwrite_cache` (`Optional[bool]`): if there exists a `cache_dir` overwrite it.

**Returns**:
- `list[W2vBSegmentationOutput]`:
-  Every `W2vBSegmentationOutput` is:
  - `clean_speech_intervals` (`torch.FloatTensor`):  `None`.
  - `speech_intervals` (`torch.FloatTensor`): Tensor of shape (N, 2) containing raw speech intervals before filtering. Format: `[[speech_start, speech_end], [speech_start, speech_end], ...]` in samples.
  - `probs` (`torch.FloatTensor | None`): Class probabilities (None if not requested)
  - `is_complete` (`bool`): Whether audio processing completed normally

**Note**:
- Processes audio in chunks of max_duration_ms for GPU memory efficiency
- Input waveforms are automatically padded and batched
- Final interval end is clamped to (audio_length + hop*stride) if not provided

---

### `clean_speech_intervals`

```python
def clean_speech_intervals(
    speech_intervals: torch.LongTensor,
    is_complete: bool,
    min_silence_duration_ms=30,
    min_speech_duration_ms=30,
    pad_duration_ms=30,
    sample_rate=16000,
    return_probabilities=False,
    return_seconds=False,
) -> W2vBSegmentationOutput:
```

Permores cleaning on raw speech intervals extracted by the model. Clean The speech intervals by:
* merging small silence durations.
* remove small speech durations.
* add padding to each speech duration.

**Arguments**:
- `speech_intervals` (`torch.LongTensor`): Tensor of shape (N, 2) containing raw speech intervals before filtering. Format: `[[speech_start, speech_end], [speech_start, speech_end], ...]` in samples.
- `is_complete` (`bool`): Whether audio processing completed normally
- `min_silence_duration_ms` (`int`): Minimum silence duration (ms) between speech segments. silence durations < `min_silence_duration_ms` will be merged into speech durations
- `min_speech_duration_ms` (`int`): Minimum duration (ms) for a valid speech segment. speech intervals durations < `min_speech_duration_ms` will be removed
- `pad_duration_ms` (`int`): Padding duration (ms) to add around speech segments
- `sample_rate` (`int`): Audio sample rate in Hz
- `return_probabilities` (`bool`): Whether to return class probabilities
- `return_seconds` (`bool`): Whether to return intervals in seconds instead of samples

**Returns**:
- `W2vBSegmentationOutput`:
  - `clean_speech_intervals` (`torch.FloatTensor`): Tensor of shape (N, 2) containing speech intervals after filtering. Format: `[[speech_start, speech_end], ...]` in samples if `return_seconds` is `false`. otherwise return the speech inervals in seconds.
  - `speech_intervals` (`torch.FloatTensor`): Tensor of shape (N, 2) containing raw speech intervals before filtering. Format: `[[speech_start, speech_end], ...]` in samples if `return_seconds` is `false`. otherwise return the speech inervals in seconds
  - `probs` (`torch.FloatTensor | None`): Class probabilities (None if not requested)
  - `is_complete` (`bool`): Whether audio processing completed normally

**Raises**:
- `NoSpeechIntervals`: If no speech segments are detected
- `TooHighMinSpeechDuration`: If filtering removes all speech segments

**Note**:
- Intervals are clamped to prevent negative starts or exceeding audio length
- Final interval end is clamped to (audio_length + hop*stride) if not provided

### Data Structures

### `W2vBSegmentationOutput`
Named tuple containing:
- `clean_speech_intervals`: `torch.Tensor` or `None`
- `speech_intervals`: `torch.Tensor`
- `probs`: `torch.Tensor` or `None`
- `is_complete`: `bool`

### Exceptions
- `NoSpeechIntervals`: Raised when input contains no speech
- `TooHighMinSpeechDuration`: Raised when filtering removes all segments

##  ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨

### Ø¯ÙˆØ§ÙØ¹ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ ÙˆØ¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
ÙƒØ§Ù† Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Ù Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ¯Ù‚Ø© ØªØµÙ„ Ù„ 50 Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ© Voice Activity Detectoin (VAD) ÙˆØªÙ… ØªØ¬Ø±Ø¨Ø©:

* Ù†Ù…ÙˆØ°Ø¬ [sliero-vad-v5](https://github.com/snakers4/silero-vad) ÙˆÙ„Ù„Ø£Ø³Ù ÙƒØ§Ù† Ø³ÙŠØ¦Ø§ Ø¬Ø¯Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ø¯Ù‚ØªÙ‡ ØªØµÙ„ 32 Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
* Ù†Ù…ÙˆØ°Ø¬ [sliero-vad-v4](https://github.com/snakers4/silero-vad/tree/v4.0stable) Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡Ø§ Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© ÙˆØ¯Ù‚ØªØ® ØªØ¨Ù„Øº 95 Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
* Ù†Ù…Ø°ÙˆØ¬ pyannotate ÙƒØ§Ù† Ø³ÙŠØ¦Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ 
ØªÙ… ØªØ¬Ø±Ø¨Ø© Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ø±Ù‰ ÙƒØ§Ù†Øª Ø³ÙŠØ¦Ø© Ø¬Ø¯Ø§

### Ø·Ø±ÙŠÙ‚Ø© Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©

Ø§Ù„Ù‡Ø¯Ù ÙÙŠ Ù‡Ùˆ ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ø§Ù„ÙˆÙ‚Ù Ù„Ø¨Ù†Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø±Ø¢Ù†ÙŠØ©. ÙØ§Ù„Ù‡Ø¯Ù Ù„ÙŠØ³ Ø§Ù„ streaming Ø¨Ù„ Ù‡Ùˆ Ø¨Ù†Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ©. ÙˆÙ…Ù† Ø«Ù… ÙØ­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠÙ„Ø²Ù…Ù‡ Ø§Ù„ÙƒØ¨Ø± Ø£Ùˆ Ø§Ù„ØµØºØ± Ø¨Ø§Ù„ Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø¬ÙˆØ¯Ø© Ø£Ù‡Ù… Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙˆØ¹ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù„Ù‰ [w2v2Bert](https://huggingface.co/docs/transformers/model_doc/wav2vec2-bert) Ù„Ø£Ù†Ù‡:

* Ù…Ø¯Ø±Ø¨ Ø¹Ù„Ù‰ 4.5 Ù…Ù„ÙŠÙˆÙ† Ø³Ø§Ø¹Ø© Ù…ØªØ¹Ø¯Ø© Ø§Ù„Ù„Ù‡Ø¬Ø§Øª ÙˆØ§Ù„Ù„ØºØ§Øª
* Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ù‘ÙØ±Ø¨ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† 100 Ø£Ù„Ù Ø³Ø§Ø¹Ø© Ù…Ù† Ø§Ù„Ø£ØµÙˆØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
* ØµØºØ± Ù†Ø§ÙØ°ØªÙ‡ Ø­ÙŠØ« ÙƒÙ„ Ù†Ø§ÙØ°Ø© Ù…Ù† Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª (feature extractor) ØªØ¨Ù„Øº 20 Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©

ÙˆÙ…Ù† Ù‡Ø§Ù‡Ù†Ø§ ØªÙ… ØªØ¯Ø±ÙŠØ¨ [w2v2Bert](https://huggingface.co/docs/transformers/model_doc/wav2vec2-bert) Ùƒ sequence labeling Ù„ÙƒÙ„ Ù†Ø§ÙØ°Ø© Ø¹Ù„Ù‰ ØªÙ„Ø§ÙˆØ§Øª Ù‚Ø±Ø¢Ù†ÙŠØ© Ø´Ø¨Ù‡ Ù…Ø¹Ù„Ù‘Ù…Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§ Ø¨Ø³Ø§ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ VAD ØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„ÙŠÙ‡.


### ØªÙ‡ÙŠØ¦Ø© Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±

#### ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª

ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨ØªÙŠ:

* [ffmbeg](https://ffmpeg.org/download.html)
* [libsoundfile](https://github.com/libsndfile/libsndfile/releases)

##### Linux
```bash
sudo apt-get update
sudo apt-get install -y ffmpeg libsndfile1 portaudio19-dev
```

##### Winodws & Mac

ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© `anaconda` . ÙˆÙ…Ù† Ø«Ù… ØªÙ†Ø²ÙŠÙ„ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ù…ÙƒØªØ¨ØªÙŠÙ†

####  ØªØ«Ø¨ÙŠØª Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±

First of all glone the repo

```bash
git clone https://github.com/obadx/recitations-segmenter.git
```

```bash
cd recitations-segmenter
```

Create conda environment with python 3.12

```bash
conda create -n segment12 python=3.12
conda activate segment12
```


> Note: for data builindg we worked on python 3.13 and for augmentations we worked to python 3.12 due to audiomentatiosn depends on scipy

Install `ffmbeg` and `scipy` using conda

```bash
conda install -c conda-forge ffmpeg scipy=1.15.2

```

Install our package

```bash
pip install -e ./[agument]
```


### Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨

### Ø·Ø±ÙŠÙ‚Ø© ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

* ÙƒØ§Ù† Ø£ÙØ¶Ù„ Ø§Ù„ VAD Ø£Ø¯Ø§Ø¡Ø§ Ù‡Ùˆ [sliero-vad-v4](https://github.com/snakers4/silero-vad/tree/v4.0stable) ÙØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØµØ§Ø­Ù Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ù…Ù† [everyayh](everyayah.com)
* ÙˆØ¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªÙ… Ø¹Ù…Ù„ Ø¯Ø§Ù„Ø© ØªÙ‚ÙˆÙ… Ø¨ØªØ¹ÙˆÙŠØ¶ Ø¹ÙŠÙˆØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¶Ø§ÙØ©:
  - `min_silence_duration_ms`: ØªÙ‚ÙˆÙ… Ø¨Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙ…Øª Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙˆØª
  - `min_speech_duration_ms`: ØªÙ‚ÙˆÙ… Ø¨Ø­Ø°Ù Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙˆØª
  ÙˆÙ…Ø¹ Ø£ÙŠØ¶Ø§ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù†Ø¸Ø± [Ù‡Ù†Ø§](./src/recitations_segmenter/train/vad_utils.py)
  * ÙˆØ¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªÙ… ØªØ­Ø¯ÙŠØ¯ ØªÙ„Ùƒ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙŠØ¯ÙˆÙŠØ§ Ù„ØªÙ‚Ø³Ù… Ø§Ù„ØªÙ„Ø§ÙˆØª Ø¨Ø¯Ù‚Ø© ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙØ´Ù„ sliro-vad-v4 ÙÙŠÙ‡Ø§
  * ÙˆÙ…Ù† Ø«Ù… Ø§Ø³ØªÙ‚Ø± Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø¹Ù„Ù‰ ØªÙ„Ùƒ [Ø§Ù„Ù…ØµØ§Ø­Ù](./recitations.yml)
  
  ```yml
  recitations:
    - reciter_name: Ù…Ø­Ù…ÙˆØ¯ Ø®Ù„ÙŠÙ„ Ø§Ù„Ø­ØµØ±ÙŠ
      id: 0
      url: https://everyayah.com/data/Husary_128kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 500
      min_speech_duration_ms: 1000
      pad_duration_ms: 40
  
    - reciter_name: Ù…Ø­Ù…Ø¯ ØµØ¯ÙŠÙ‚ Ø§Ù„Ù…Ù†Ø´Ø§ÙˆÙŠ
      id: 1
      url: https://everyayah.com/data/Minshawy_Murattal_128kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 400
      min_speech_duration_ms: 1000
      pad_duration_ms: 20
  
    - reciter_name: Ø¹Ø¨Ø¯ Ø§Ù„Ø¨Ø§Ø³Ø· Ø¹Ø¨Ø¯ Ø§Ù„ØµÙ…Ø¯
      id: 2
      url: https://everyayah.com/data/Abdul_Basit_Murattal_192kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 400
      min_speech_duration_ms: 700
      pad_duration_ms: 20
  
  
    - reciter_name: Ù…Ø­Ù…ÙˆØ¯ Ø¹Ù„ÙŠ Ø§Ù„Ø¨Ù†Ø§
      id: 3
      url: https://everyayah.com/data/mahmoud_ali_al_banna_32kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 400
      min_speech_duration_ms: 700
      pad_duration_ms: 20
      
    - reciter_name: Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø°ÙŠÙÙŠ
      id: 5
      url: https://everyayah.com/data/Hudhaify_128kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 350
      min_speech_duration_ms: 700
      pad_duration_ms: 5
  
    - reciter_name: Ø£ÙŠÙ…Ù† Ø±Ø´Ø¯ÙŠ Ø³ÙˆÙŠØ¯
      id: 6
      url: https://everyayah.com/data/Ayman_Sowaid_64kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 500
      min_speech_duration_ms: 1000
      pad_duration_ms: 10
  
    - reciter_name: Ù…Ø­Ù…Ø¯ Ø£ÙŠÙˆØ¨
      id: 7
      url: https://everyayah.com/data/Muhammad_Ayyoub_128kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 400
      min_speech_duration_ms: 1000
      pad_duration_ms: 10
  
  
    - reciter_name: Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø®Ø¶Ø±
      id: 8
      url: https://everyayah.com/data/Ibrahim_Akhdar_32kbps/000_versebyverse.zip
      window_size_samples: 1536
      threshold: 0.3
      min_silence_duration_ms: 390
      min_speech_duration_ms: 700
      pad_duration_ms: 30
  ```

### ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØµØ§Ø­Ù Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ù…ØµÙÙˆÙØ§Øª array Ø¨ØµÙŠØºØ© Hugging Face Audio Dataset Ø¨Ù…Ø¹Ù…Ø¯Ù„ (sample rate)   16000 HZ 
2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¢ÙŠØ§Øª ØªØ¨Ø¹Ø§ Ù„Ù„ÙˆÙ‚Ù Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… sliro-vad-v4
3. ØªØ·Ø¨ÙŠÙ‚ ØªØ³Ø±ÙŠØ¹ ÙˆØ¥Ø¨Ø·Ø§Ø¡ Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ØªÙ„Ø§ÙˆØª Ø¹Ù„Ù‰ 40 % Ù…Ù† Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ù„Ù…ÙˆØ§ÙƒØ¨Ø© Ø³Ø±Ø¹Ø§Øª Ø§Ù„ØªÙ„Ø§ÙˆØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
4. ØªØ·ÙŠØ¨Ù‚ data augmentations Ø¨Ø§ØªØ³Ø®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© [audumentations](https://github.com/iver56/audiomentations) Ù…ØªØ¨Ø¹ÙŠÙ† Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© sliro-vad ÙˆØ¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù„ augmentattions Ù…ÙˆØ¬ÙˆØ¯Ø© [Ù‡Ù†Ø§](./augment_config.yml)

5. ÙˆÙ…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ… Ø£Ù† w2v2Bert ØªØ¯Ø¹Ù… Ø·ÙˆÙ„ ÙŠØµÙ„ Ø¥Ù„Ù‰ 100 Ø«Ø§Ù†ÙŠØ©. ÙÙ‚Ø¯ ÙˆÙ‚Ø¹ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù„Ù‰ 20 Ø«Ø§Ù†ÙŠØ©.
6. ÙˆØ¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø£Ø·ÙˆÙ„ Ù…Ù† 20 Ø«Ø§Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…ØªØ­Ø±ÙƒØ© sliding window Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙ‡Ø°Ù‡ ØµÙˆØ±Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰: 
![durations-fig](./assets/durations_histogram.png)

Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ augmentations: 


```yml
# Audio processing parameters
min_size_samples: 32000       # Minimum audio length (2 seconds at 16kHz)
max_size_samples: 320000      # Maximum audio length (20 seconds at 16kHz)
truncate_window_overlap_length: 16000  # Overlap when splitting long audio

# Spectrogram feature extraction
window_length_samples: 400    # Window length for STFT
hop_length_samples: 160       # Hop length for STFT
sampling_rate: 16000          # Audio sample rate
stride: 2                     # Convolution stride for feature extraction

# Label configuration
speech_label: 1               # Label for speech segments
silence_label: 0              # Label for silence segments
ignored_idx: -100             # Index to ignore in loss calculations

# Model and processing
model_id: facebook/w2v-bert-2.0  # Pre-trained model identifier
batch-size: 32                # Batch size for processing
samples-per-shard: 1024       # Samples per Parquet shard

# Augmentation parameters
seed: 1                       # Random seed for reproducibility
min-stretch-ratio: 0.8        # Minimum time stretch ratio
max-stretch-ratio: 1.5        # Maximum time stretch ratio
augment-prob: 0.4             # Probability of applying augmentation

```

Ø§Ù„ augmentations Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© [Ù‡Ù†Ø§](./src/recitations_segmenter/train/augment.py):

```python
def build_audiomentations_augs(p=0.4, seed=42, all=False):
    """taken form: https://github.com/snakers4/silero-vad/blob/master/tuning/utils.py#L37
    """
    # audiomentations usesd python random for its calculations
    random.seed(seed)
    np.random.seed(seed)

    from audiomentations import (
        SomeOf,
        AirAbsorption,
        BandPassFilter,
        BandStopFilter,
        ClippingDistortion,
        HighPassFilter,
        HighShelfFilter,
        LowPassFilter,
        LowShelfFilter,
        Mp3Compression,
        PeakingFilter,
        PitchShift,
        RoomSimulator,
        SevenBandParametricEQ,
        Aliasing,
        AddGaussianNoise,
        GainTransition,
        Compose,
    )
    transforms = [
        Aliasing(p=1),
        AddGaussianNoise(p=1),
        AirAbsorption(p=1),
        BandPassFilter(p=1),
        BandStopFilter(p=1),
        ClippingDistortion(p=1),
        HighPassFilter(p=1),
        HighShelfFilter(p=1),
        LowPassFilter(p=1),
        LowShelfFilter(p=1),
        Mp3Compression(p=1),
        PeakingFilter(p=1),
        PitchShift(p=1),
        RoomSimulator(p=1, leave_length_unchanged=True),
        SevenBandParametricEQ(p=1),
        GainTransition(p=1, min_gain_db=-17),
    ]
    if all:
        return Compose(transforms, p=p)
    return SomeOf((1, 3), transforms=transforms, p=p)

```
### Ø§Ù„ØªØ¯Ø±ÙŠØ¨

* ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨ØªÙˆÙÙŠÙ‚ Ø§Ù„Ù„Ù‡ Ø³Ø¨Ø­Ø§Ù†Ù‡ ÙˆØªØ¹Ø§Ù„ÙŠ Ø¹Ù„Ù‰ Ù…Ù†ØµØ© [Lightning Studio](https://lightning.ai) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬ Ø±Ø³ÙˆÙ…ÙŠØ§Øª ÙˆØ§Ø­Ø¯ Ù…Ù† Ù†ÙˆØ¹ Nvidia L40 (48GB) ÙˆÙ„Ù…Ø¯Ø© Ø³Ø§Ø¹Ø§ØªØ§Ù† ØªÙ‚Ø±ÙŠØ¨Ø§
* ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬: `Wav2Vec2BertForAudioFrameClassification` .ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯](./train.py)

Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```bash
pip install -r train_requirements.txt
```

ÙˆØ¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø¨ØªÙ‡ÙŠØ¦Ø© accelerate

```bash
accelereate config
```

ÙˆÙ…Ù† Ø«Ù… Ø§Ø¨ØªØ¯Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:

```bash
accelerate launch train.py
```

### Ø§Ù„Ù†ØªØ§Ø¦Ø¬

Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ Ù…ØµØ­Ù Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„ÙŠÙ‡:

| Metric     | Value  |
|------------|--------|
| Accuracy   | 0.9958 |
| F1         | 0.9964 |
| Loss       | 0.0132 |
| Precision  | 0.9976 |
| Recall     | 0.9951 |

## Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©

* ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø§Ù„ÙØ§Ø¦Ù‚ Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£Ø³ÙƒÙ†Ø¯Ø±ÙŠØ© [Bibliotheca Alexandrina (BA), HPC](https://hpc.bibalex.org/about) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© [slurm](https://slurm.schedmd.com/overview.html)
* ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© [submitit](https://pypi.org/project/submitit/) Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø£ÙƒØ«Ø± Ù…Ù† Ø¹Ù„Ù…ÙŠØ© Ø­Ø³Ø§Ø¨ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ø³Ø¨ Ø§Ù„ÙØ§ÙŠÙ‚ ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
* Ø¬Ù…ÙŠØ¹ Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø­Ø§Ø³Ø¨ Ø§Ù„ÙØ§Ø¦Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© [Ù‡Ù†Ø§](./hpc_scripts/)
* Ø§Ù„Ù…Ù‚Ø³Ù… ÙŠØ¹ØªØ¨Ø± Ø§Ù„Ø³ÙƒØª ÙˆÙ‚ÙØ§ ÙˆÙ„Ø§ ÙŠØ¹ØªØ¨Ø±Ù‡ Ø³ÙƒØªØ§ ÙˆÙ‡Ø°Ø§ ÙŠØ¹ØªØ¨Ø± Ø¹ÙŠØ¨.


## TODO

* [x] Test the model on notebook.
* [x] Add CI/CD checking python versions.
* [x] Add commdnad line tool to API.
* [x] Add pytest for the cli.
* [x] Whether to raise execption or not if no speech found
* [x] Add caching mechanism.
* [x] Project Description
* [x] API docs
* [ ] train docs
* [ ] datasets docs (create and description)
* [x] Add lock file for reproudcing training
* [ ] Steps to reprouduce Dev environment [see](https://chat.qwen.ai/s/75280423-a193-4f1b-a35b-93a5f8e03ff8?fev=0.0.87)
* [x] Add libsoundfile and ffmbeg as backend for reading mp3 files
* [ ] publish to pypip

* [ ] update colab docs
